\documentclass{article} % For LaTeX2e

\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{graphicx}
\usepackage{array,multirow,graphicx}
\usepackage{subcaption}
% \documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\usepackage{booktabs}
\usepackage{makecell}

% \usepackage[obeyspaces]{url}

\title{Text2Scene Generation \\ Generator}


\author{
%David S.~Hippocampus\thanks{ Use footnote for providing further information
%about author (webpage, alternative address)---\emph{not} for acknowledging
%funding agencies.} \\
%Department of Computer Science\\
%Cranberry-Lemon University\\
%Pittsburgh, PA 15213 \\
%\texttt{hippo@cs.cranberry-lemon.edu} \\
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email} \\
%\AND
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email} \\
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email} \\
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email} \\
%(if needed)\\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
\end{abstract}

%\section{Task}
%Generate a consistent picture given a query text, using learned discriminators as score functions
\section{Dataset}

\begin{table}[htbp]
	\caption{Keyword occurrence}
	\centering
	% \resizebox{\columnwidth}{!}{
	\begin{tabular}{rccc}
		\toprule
		Occurrence & $1$ & $2$ & $>2$\\
		\#~keyword & $114$ & $22$ & $23$\\
		\midrule
		\#~subject & $0$ & $2$ & $10$\\
		\#~object & $88$ & $9$ & $7$\\
		\#~action & $26$ & $11$ & $6$\\
		\bottomrule
	\end{tabular}%}
	\label{tab: feat}
\end{table}

\begin{table}[htbp]
	\caption{Layer occurrence}
	\centering
	% \resizebox{\columnwidth}{!}{
	\begin{tabular}{rccc}
		\toprule
		Occurrence & $1$ & $2$ & $>2$\\
		\#~layer & $151$ & $14$ & $9$\\
		\midrule
		\#~background & $0$ & $0$ & $1$\\
		\#~character & $74$ & $9$ & $3$\\
		\#~surrounding & $66$ & $5$ & $3$\\
		\#~accessory & $11$ & $0$ & $2$\\
		
		\bottomrule
	\end{tabular}%}
	\label{tab: feat}
\end{table}

Suppose under each category there is only one layer or none, the total number of possible combinations of layers is (1+1)*(74+1)*(86+1)*(13+1) = 182,700.

Things to do:
\begin{itemize}
	\item Slice new pictures
	\item Use sliced layers in current pictures to compose new pictures
	\item paraphrase the description
\end{itemize}

\section{Metric}
\subsection{Layer-layer similarity}
We use a mean IoU to measure the similarity between two layers. A layer is typically composed of several objects and verbs. These two types will be treated differently. For object keywords, we use the original IoU. Specifically, suppose the object sets in two layers are $\mathcal{O}_1=\{o_i\}$ and $\mathcal{O}_2=\{o_j\}$ respectively. The similarity is
$$
\mathrm{sim}_o = \frac{|\mathcal{O}_1\cap\mathcal{O}_2|}{|\mathcal{O}_1\cup \mathcal{O}_2|}.
$$
Duplicate keywords will not treated individually when performing intersection and union. For instance, if the object set in layer 1 is $\{\mathrm{``man"},\mathrm{``man"}\}$ and the object set in layer 2 is $\{\mathrm{``man"}\}$, then the intersection set will be $\{\mathrm{``man"}\}$ and the union set will be $\{\mathrm{``man"},\mathrm{``man"}\}$, thus $\mathrm{sim}_o = 1/2$.

For verbs, we use a soft IoU, considering that verbs are often flexible. Suppose the verb sets in two layers are $\mathcal{A}_1=\{a_i\}$ and $\mathcal{A}_2=\{a_j\}$ respectively. The similarity is
$$
\mathrm{sim}_a = \frac{\sum\mathcal{A}^*_\cap}{|\mathcal{A}_1|+|\mathcal{A}_2| - |\mathcal{A}^*_\cap|},
$$
where
$$
\mathcal{A}^*_\cap = \{r(a_i, a_j)\}_{i,j<\min(|\mathcal{A}_1|,|\mathcal{A}_2|)}.
$$
Here $r(a_1,a_2)$ is the relatedness between two keywords. The intersection set is build greedily, such that the most related keywords in the two sets are first selected, then the less related keywords, until one set is exhausted. For instance, if two verb sets are $\{\mathrm{``walk"},\mathrm{``talk"}\}$ and $\{\mathrm{``run"}\}$, and suppose the relatedness between ``walk" and ``run'' is 0.5, then $\mathrm{sim}_a = 1/4$. This similarity is appropriate for sets of different but similar keywords.

The overall similarity between two layers is the weighted sum of three types of keywords. Here, three types refer to verbs, objects and subjects. Subjects will fit the metric for objects. Therefore, we can write
$$
\mathrm{sim}(l_1,l_2) = \alpha_s \mathrm{sim}_o(\mathcal{S}_1, \mathcal{S}_2) + \alpha_a \mathrm{sim}_a(\mathcal{A}_1, \mathcal{A}_2) + \alpha_o \mathrm{sim}_o(\mathcal{O}_1, \mathcal{O}_2),
$$
where $\mathcal{S}$, $\mathcal{A}$ and $\mathcal{O}$ refer to the sets of subjects, verbs and objects in a layer. Currently, the weights are selected such that $\alpha_s = 0.5$, $\alpha_a = 0.2$, $\alpha_o = 0.3$.

\subsection{Picture-picture similarity}
To quantitatively evaluate the generation, we needs a metric to measure the similarity between the generated picture and the true picture. A picture is composed of several layers. And the generated picture may not have an equal number of layers against the true picture. One simple metric is the averaged pairwise similarities between layers. But this metric will discourage the number of generated layers in a picture because layers in different categories are distinct. Here we borrow the idea of soft IoU in layer-layer similarity. Specifically,
%% maybe similarity in different categories separately.
$$
\mathrm{sim}(p_1,p_2) = \frac{\sum\mathcal{L}^*_\cap}{|\mathcal{L}_1|+|\mathcal{L}_2| - |\mathcal{L}^*_\cap|},
$$
and
$$
\mathcal{L}^*_\cap = \{\mathrm{sim}(l_i, l_j)\}_{i,j<\min(|\mathcal{L}_1|,|\mathcal{L}_2|)}.
$$
The intuition here is to choose the most similar layers in two pictures as intersections. 
 
 % reasonability metric, just the knowledges based similarity between different categories. see below
 
\section{Evaluation}
Currently we have 90 pairs of pictures and descriptions. Let us randomly choose 70 as the training set, and the rest as test set. We do not have a learning process in our pipeline. The difference between training and test set is that the layers of pictures in training set are extracted as material. Therefore the true layers in training set are seen by the model, while the true layers in test set are unobservable. In other words the model needs to select consistent layers in the training set to compose the pictures in the test set.

Using the above metric to measure the similarity between generated pictures and true pictures, the overall average performance on the training set is 0.496. In contrast, the performance on the test set is 0.333. The prominent gap means the model needs to improve the ability to utilize existing materials.

%\begin{table}[htbp]
%	\caption{Generation score}
%	\centering
%	\begin{tabular}{rcccc}
%		\toprule
%				 & \multicolumn{2}{c}{Random picture} & \multicolumn{2}{c}{Model}\\
%		\cmidrule{2-3}\cmidrule{4-5}
%				 & train &  test & train & test\\
%		\midrule
%		mIoU & 0.22 & 0.20 & 0.39 & 0.24  \\   
%		\bottomrule
%	\end{tabular}%}
%	\label{tab: feat}
%\end{table}

\begin{table}[htbp]
	\caption{Generation score (ConceptNet relatedness)}
	\centering
	\begin{tabular}{rccccccccc}
		\toprule
				 & \multicolumn{8}{c}{mIoU} & \multirow{3}{*}{Runtime(sec)}\\
		\cmidrule{2-9}
				 & \multicolumn{2}{c}{overall} & \multicolumn{2}{c}{subject} & \multicolumn{2}{c}{action} & \multicolumn{2}{c}{object} & \\
		\cmidrule{2-3}\cmidrule{4-5}\cmidrule{6-7}\cmidrule{8-9}
				 & train &  test &  train &  test &  train &  test &  train &  test & \\
		\midrule
		Random picture
				& 0.22 & 0.20 & 	0.34		& 	0.28	  & 	0.28	  & 	0.23	 &  0.03		  & 	0.02	&  - \\
		Graph-based generator
				& 0.39 & 0.24 & 	0.46 & 	 0.30 & 	0.42	  & 	0.27  & 	0.22	  & 0.10 	&  0.021  \\
				\makecell{Exhaustive search\\(Beam=2*20*20*2)}
				% & 0.28 & 0.25 &  			& 		  & 		 & 		& 			&		& 12.9\\
				& 0.26 & 0.23 & 0.34 & 0.28 & 0.34 & 0.37 & 0.05 & 0.04 & 12.9\\
		\bottomrule
	\end{tabular}%}
	\label{tab: feat}
\end{table}

\begin{table}[htbp]
	\caption{Generation score w/o background (ConceptNet relatedness)}
	\centering
	\begin{tabular}{rccccccccc}
		\toprule
				 & \multicolumn{8}{c}{mIoU} & \multirow{3}{*}{Runtime(sec)}\\
		\cmidrule{2-9}
				 & \multicolumn{2}{c}{overall} & \multicolumn{2}{c}{subject} & \multicolumn{2}{c}{action} & \multicolumn{2}{c}{object} & \\
		\cmidrule{2-3}\cmidrule{4-5}\cmidrule{6-7}\cmidrule{8-9}
				 & train &  test &  train &  test &  train &  test &  train &  test & \\
		\midrule
		Random picture
				& 0.23 & 0.17 & 0.28 & 0.19 & 0.34 & 0.32 & 0.07 & 0.04 &   - \\
		Graph-based generator
				& 0.50 & 0.30 & 0.60 & 0.38 & 0.54 & 0.35 & 0.30 & 0.13 & 0.021  \\
				\makecell{Exhaustive search\\(Beam=2*20*20*2)}
				& 0.26 & 0.24 & 0.35 & 0.31 & 0.36 & 0.35 & 0.06 & 0.03 & 12.9\\
				% & 0.28 & 0.25 &  			& 		  & 		 & 		& 			&		& 12.9\\
		\bottomrule
	\end{tabular}%}
	\label{tab: feat}
\end{table}


% use conceptNet relatedness improve performance on training set.
% but not on test set
% because on test set, the major problem is the material does not exist, rather than deviation in locating the material.
\begin{table}[htbp]
	\caption{Generation score (GloVe cosine similarity; metric stays the same)}
	\centering
	\begin{tabular}{rccc}
		\toprule
				 & \multicolumn{2}{c}{mIoU} & \multirow{2}{*}{Runtime(sec)}\\
		\cmidrule{2-3}
				 & train &  test & \\
		\midrule
		Random picture & 0.22 & 0.20 &  - \\   
		Graph-based generator & 0.29 & 0.23 & 0.021\\
		Exhaustive search (Beam=2*20*20*2) & 0.28 & 0.24 & 12.9\\
		\bottomrule
	\end{tabular}%}
	\label{tab: feat}
\end{table}

The following lists some specific issues of the model reflected from the test results. 
\begin{enumerate}
	\item Entities in the description are not included in the layer base
	\item Entities in the picture are not mentioned in the description. Background and most accessories will never be explicitly mentioned. 
	\item Token in the description and keyword in the description referring to the same object mismatch
	\item Improper partition
	\item Measure word not considered
\end{enumerate}

Let us show some test pairs with poor performance.
\begin{enumerate}
	
	\item \verb|Frozen| - Issue 1
	\begin{enumerate}
		\item \bf{Description}: \verb|There is a snowman.|
		\item \bf{Picture}: \verb|#alien(have[snowman])|
		\item \bf{Generated}: \verb|#home(have[bookcase,apple,wall_clock])|
	\end{enumerate}
	
	\item \verb|prototyping_process| - Issue 2 \& 1
	\begin{enumerate}
		\item \bf{Description}: \verb|A woman is standing in front of the drawing board with a pen.|
		\item \bf{Picture}: 
		\begin{itemize}
			\item \verb|#background; #accessory(have[leaf])|
			\item \verb|#chart(have[pad,diagram]); #other(have[drawing_board])|
			\item \verb|#woman(stand,hold[pen])|
		\end{itemize}
		\item \bf{Generated}: \verb|#woman(hold[pen],point_to)|
	\end{enumerate}

	\item \verb|team_work| - Issue 4 \& 2
	\begin{enumerate}
		\item \bf{Description}: \verb|A man is sitting in front of the leaves, and a woman is standing.|
		\item \bf{Picture}: 
		\begin{itemize}
			\item \verb|#background|
			\item \verb|#wild(have[leaf,sun,ground])|
			\item \verb|#man(sit); #woman(stand)|
		\end{itemize}
		\item \bf{Generated}: \verb|#group(have[woman(stand),man(stand)])|
	\end{enumerate}

	\item \verb|Game_day| - Issue 2 \& 5
	\begin{enumerate}
		\item \bf{Description}: \verb|Two man sat on the sofa celebrating a game.|
		\item \bf{Picture}: 
		\begin{itemize}
			\item \verb|#background; #accessory|
			\item \verb|#home(have[sofa,toy,chips])|
			\item \verb|#group(have[man(drink[beer],sit,raise[arm]),man(sit,raise[arm],eat[chips])])|
		\end{itemize}
		\item \bf{Generated}: \verb|#man(lie_on[sofa],watch[movie])|
	\end{enumerate}
	
	\item \verb|work_chat| - Issue 5 \& 4
	\begin{enumerate}
		\item \bf{Description}: \verb|Two men and a woman are standing by the message.|
		\item \bf{Picture}: 
		\begin{itemize}
			\item \verb|#background|
			\item \verb|#other(have[circle,message])|
			\item \verb|#woman(stand); #man(stand); #man(stand,put_up)|
		\end{itemize}
		\item \bf{Generated}: 
		\begin{itemize}
			\item \verb|#chart(have[message,leaf])|
			\item \verb|#group(have[woman(stand),man(stand)])|
		\end{itemize}
		
	\end{enumerate}

	\item \verb|weather| - Issue 3 \& 1
	\begin{enumerate}
		\item \bf{Description}: \verb|A man is standing to give the weather report.|
		\item \bf{Picture}: 
		\begin{itemize}
			\item \verb|#background; #accessory(have[plant]);|
			\item \verb|#other(have[bulletin])|
			\item \verb|#man(stand,point_to)|
		\end{itemize}
		\item \bf{Generated}: 
		\begin{itemize}
			\item \verb|#wild(have[rock,sign])|
			\item \verb|#man(collect[data])|
		\end{itemize}
		
	\end{enumerate}
			
\end{enumerate}

\section{Runtime}
Most of the time is spent on querying the similarity between keywords. Each time we query a similarity, the dictionary will be opened and closed, which consumes much time in file I/O.

{\color{red}{now 0.021}}
\begin{table}[htbp]
	\caption{Runtime profile in a hierarchy. The number below each module indicates the cumulative time spent on it, on seconds.}
	\centering
	% \resizebox{\columnwidth}{!}{
	\begin{tabular}{cccccc}
		\toprule
		\multicolumn{6}{c}{Generator}\\
		\multicolumn{6}{c}{1.133}\\
		\midrule
		 \multicolumn{3}{c}{\makecell{Grounding\\(layer)}} & \multicolumn{2}{c}{\makecell{Grounding\\(keyword)}} &... \\
		 \multicolumn{3}{c}{0.714} & \multicolumn{2}{c}{0.418} & \\
		 \cmidrule(r){1-6}
		 \multicolumn{2}{c}{\makecell{Layer similarity}}  & ...& \multicolumn{3}{c}{}\\
		  \multicolumn{2}{c}{0.714} & & \multicolumn{3}{c}{} \\		  
		  \cmidrule{1-2} \cmidrule{4-5}
		  Keyword similarity & ... &  & Keyword similarity & ... &  \\
		  0.711 					   &     &  & 0.312                     & & \\
		  
		 \bottomrule
	\end{tabular}%}
	\label{tab: feat}
\end{table}



\end{document}










