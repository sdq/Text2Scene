{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det(man-2, A:DT-1)\n",
      "nsubj(playing-7, man:NN-2)\n",
      "cc(man-2, and:CC-3)\n",
      "det(boy-5, a:DT-4)\n",
      "conj(man-2, boy:NN-5)\n",
      "aux(playing-7, are:VBP-6)\n",
      "ROOT(playing-7, playing:VBG-7)\n",
      "dobj(playing-7, computer:NN-8)\n",
      "cc(playing-7, and:CC-9)\n",
      "conj(playing-7, sitting:VBG-10)\n",
      "prep(sitting-10, on:IN-11)\n",
      "det(ground-13, the:DT-12)\n",
      "pobj(on-11, ground:NN-13)\n",
      "prep(sitting-10, in:IN-14)\n",
      "det(wild-16, the:DT-15)\n",
      "pobj(in-14, wild:NN-16)\n",
      "punct(playing-7, .:.-17)\n",
      "det(woman-19, A:DT-18)\n",
      "nsubj(waving-21, woman:NN-19)\n",
      "aux(waving-21, is:VBZ-20)\n",
      "ROOT(waving-21, waving:VBG-21)\n",
      "prep(waving-21, under:IN-22)\n",
      "det(tree-24, the:DT-23)\n",
      "pobj(under-22, tree:NN-24)\n",
      "punct(waving-21, .:.-25)\n",
      "det(girl-27, A:DT-26)\n",
      "nsubj(sitting-29, girl:NN-27)\n",
      "aux(sitting-29, is:VBZ-28)\n",
      "ROOT(sitting-29, sitting:VBG-29)\n",
      "prep(sitting-29, on:IN-30)\n",
      "det(chair-32, the:DT-31)\n",
      "pobj(on-30, chair:NN-32)\n",
      "punct(sitting-29, .:.-33)\n",
      "det(man-35, A:DT-34)\n",
      "nsubj(lying-37, man:NN-35)\n",
      "aux(lying-37, is:VBZ-36)\n",
      "ROOT(lying-37, lying:VBG-37)\n",
      "prep(lying-37, on:IN-38)\n",
      "det(games-42, the:DT-39)\n",
      "compound(games-42, sofa:NN-40)\n",
      "compound(games-42, playing:NN-41)\n",
      "pobj(on-38, games:NNS-42)\n",
      "punct(lying-37, .:.-43)\n",
      "expl(are-45, There:EX-44)\n",
      "ROOT(are-45, are:VBP-45)\n",
      "det(trees-47, some:DT-46)\n",
      "attr(are-45, trees:NNS-47)\n",
      "cc(trees-47, and:CC-48)\n",
      "conj(trees-47, plants:NNS-49)\n",
      "cc(plants-49, and:CC-50)\n",
      "conj(plants-49, flowers:NNS-51)\n",
      "prep(trees-47, in:IN-52)\n",
      "det(park-54, the:DT-53)\n",
      "pobj(in-52, park:NN-54)\n",
      "punct(are-45, .:.-55)\n",
      "det(alien-57, An:DT-56)\n",
      "nsubj(lying-59, alien:NN-57)\n",
      "aux(lying-59, is:VBZ-58)\n",
      "ROOT(lying-59, lying:VBG-59)\n",
      "xcomp(lying-59, crying:VBG-60)\n",
      "punct(lying-59, .:.-61)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'A man and a boy are playing computer and sitting on the ground in the wild. A woman is waving under the tree. A girl is sitting on the chair. A man is lying on the sofa playing games. There are some trees and plants and flowers in the park. An alien is lying crying.')\n",
    "# parse(doc)\n",
    "# 'he is '\n",
    "# 'mike is'\n",
    "for token in doc:\n",
    "    print(\"{2}({3}-{6}, {0}:{1}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rules.labels import subjects\n",
    "from tools.knowledge import LayerBase, TextBase\n",
    "layerbase = LayerBase()\n",
    "# prs_subjects = set([subj for subj in layerbase.entities_['subj'] if subj in subjects['character']])\n",
    "# prs_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similairties between subjects: the largest common subset\n",
    "#     E.g. 1. text: man man and base: man woman\n",
    "#              man - man: 1 + man - woman: 0.5\n",
    "#          2. TEXT: man man and BASE: man\n",
    "#              man - man: 1\n",
    "#              so choose man and woman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from rules.labels import subjects\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from collections import defaultdict\n",
    "from tools.common import ddict2dict\n",
    "from tools.instance import Node\n",
    "from tools.knowledge import LayerBase\n",
    "from query_relatedness import query_simi\n",
    "import warnings\n",
    "\n",
    "\n",
    "def get_element_from_set(set_, e_):\n",
    "    assert(isinstance(set_, dict) or isinstance(set_, set))\n",
    "    for e in set_:\n",
    "        if e == e_:\n",
    "            return e\n",
    "        \n",
    "class CombToken:\n",
    "    def __init__(self, token, keyword):\n",
    "        if token:\n",
    "            assert(isinstance(token, spacy.tokens.token.Token))\n",
    "        if keyword:\n",
    "            assert(isinstance(keyword, Node))\n",
    "        self.token = token\n",
    "        self.keyword = keyword\n",
    "        \n",
    "        self.tup = (self.token, self.keyword)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.token is None:\n",
    "            return '->%s' % (self.keyword.t)\n",
    "        if self.keyword is None:\n",
    "            return '%s->' % (self.token.lemma_)\n",
    "        return '%s->%s' % (self.token.lemma_, self.keyword.t)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.tup == other.tup\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.tup)\n",
    "\n",
    "    \n",
    "class Ground:\n",
    "    def __init__(self):\n",
    "        self.layerbase = LayerBase()\n",
    "        self.collocations_ = self.layerbase.collocations_\n",
    "        self.entities_ = self.layerbase.entities_\n",
    "        self.have = Node('have', 'act')\n",
    "        \n",
    "    def __call__(self, filename):\n",
    "        \n",
    "        with open(filename) as f:\n",
    "            text = f.read()\n",
    "        print(text.strip('\\n'))\n",
    "        doc = nlp(u'%s' % text.strip('\\n'))\n",
    "    \n",
    "        nested = defaultdict(lambda: defaultdict(set))\n",
    "        tokens = self.get_tokens(doc)\n",
    "        print(tokens)\n",
    "        \n",
    "        self.ground_subj(tokens, nested)\n",
    "        \n",
    "        self.ground_act(tokens, nested)\n",
    "        if not all([t.pos_ != 'VERB' for t in tokens]):\n",
    "            warnings.warn('Verbs not exhausted!')\n",
    "            print([t for t in tokens if t.pos_ == 'VERB'])\n",
    "\n",
    "        self.ground_obj(tokens, nested)\n",
    "        if tokens:\n",
    "            warnings.warn('Tokens not exhausted!')\n",
    "            print('-> left ungrounded tokens:', tokens)\n",
    "            \n",
    "        self.dup_combine(nested)\n",
    "        \n",
    "        self.conj_copy(nested)\n",
    "        \n",
    "        layers = self.slice_into_layers(nested)\n",
    "        \n",
    "        return layers\n",
    "        \n",
    "    def get_tokens(self, doc):\n",
    "        return [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "  \n",
    "    def ground_subj(self, tokens_, nested_):\n",
    "\n",
    "        ## query the base to identify a subject\n",
    "        tokens_copy = tokens_.copy()\n",
    "        subjs = [subj for subj in self.collocations_]\n",
    "        for token in tokens_copy:\n",
    "            if token.pos_ == 'NOUN':\n",
    "                tup = self.get_simi_keyword(token, subjs, thresh=0.5)\n",
    "                if tup:\n",
    "                    subj_, _ = tup\n",
    "                    tokens_.remove(token)\n",
    "                    nested_[CombToken(token, subj_)] = defaultdict(set)\n",
    "\n",
    "    def get_simi_keyword(self, token, keywords, thresh=0.5):\n",
    "        tups = [(k, query_simi(token.lemma_, k.t)) for k in keywords]\n",
    "        simi_key, simi_ = sorted(tups,\n",
    "                                 key=lambda x: (x[1], x[0].count))[-1]\n",
    "        if simi_ >= thresh:\n",
    "            return simi_key, simi_\n",
    "        return None\n",
    "    \n",
    "    def bind_keyword(self, token, attr='obj',\n",
    "                     subj=None, act=None, thresh=0.2):\n",
    "        if attr == 'act':\n",
    "            if subj:\n",
    "                keywords = [k for k in self.collocations_[subj]]\n",
    "            else:\n",
    "                keywords = [k for k in self.entities_['act']]\n",
    "        elif attr == 'obj':\n",
    "            if subj and act:\n",
    "                keywords = [k for k in self.collocations_[subj][act]] \n",
    "            elif subj:\n",
    "                keywords = [k for a in self.collocations_[subj] for k in self.collocations_[subj][a]]\n",
    "            else:        \n",
    "                keywords = [k for k in self.entities_['obj']]\n",
    "        else:\n",
    "            raise KeyError\n",
    "\n",
    "        tup = self.get_simi_keyword(token, keywords, thresh=thresh)\n",
    "        if tup:\n",
    "            k_, _ = tup\n",
    "        else:\n",
    "            k_ = None # Node('', attr=attr)\n",
    "        return CombToken(token, k_)\n",
    "\n",
    "    def wrap_multiple_actions(self, token, subj, acts, thresh=0.2):\n",
    "        tups = []\n",
    "        for act in acts:\n",
    "            if not self.collocations_[subj][act]: continue\n",
    "            tup = self.get_simi_keyword(token,\n",
    "                                   self.collocations_[subj][act],\n",
    "                                   thresh=thresh)\n",
    "            if tup:\n",
    "                tups.append((act,) + tup)\n",
    "        if tups:\n",
    "            return sorted(tups, key=lambda x: (x[-1], x[0].count))[-1]\n",
    "        return None\n",
    "    \n",
    "    def find_most_simi_subj(self, subjs, token, \n",
    "                            attr='obj', fix_have=True):\n",
    "        assert(isinstance(token, spacy.tokens.token.Token))\n",
    "        assert(all([isinstance(subj, Node) for subj in subjs]))\n",
    "        \n",
    "        if attr == 'obj':\n",
    "            most_simi_ = []\n",
    "            for subj in subjs:\n",
    "                if fix_have:\n",
    "                    assert(all([self.have in self.collocations_[s] for s in subjs]))\n",
    "                    if not self.collocations_[subj][self.have]:\n",
    "                        continue\n",
    "                    tup = self.get_simi_keyword(token,\n",
    "                                           self.collocations_[subj][self.have],\n",
    "                                       thresh=0.2)\n",
    "                else:\n",
    "                    if not self.collocations_[subj]: continue\n",
    "                    tup = self.wrap_multiple_actions(token, subj,\n",
    "                                        self.collocations_[subj],\n",
    "                                        thresh=0.2)               \n",
    "                if tup:\n",
    "                    most_simi_.append((subj,) + tup)\n",
    "\n",
    "            if most_simi_:\n",
    "                if fix_have:\n",
    "                    sort_f = lambda x: (x[-1], get_element_from_set(self.collocations_[x[0]][self.have], x[1]).count)\n",
    "                else:\n",
    "                    sort_f = lambda x: (x[-1], get_element_from_set(self.collocations_[x[0]][x[1]], x[2]).count)\n",
    "                return sorted(most_simi_, key=sort_f)[-1]\n",
    "\n",
    "        elif attr == 'act':\n",
    "            most_simi_ = []\n",
    "            for subj in subjs:\n",
    "                # if no action under this suject, skip it\n",
    "                if not self.collocations_[subj]: continue\n",
    "                tup = self.get_simi_keyword(token,\n",
    "                                       self.collocations_[subj], \n",
    "                                       thresh=0.2)\n",
    "                if tup:\n",
    "                    most_simi_.append((subj,) + tup)\n",
    "            if most_simi_:\n",
    "                sort_f = lambda x: (x[-1], get_element_from_set(self.collocations_[x[0]], x[1]).count)\n",
    "                return sorted(most_simi_, key=sort_f)[-1]\n",
    "        else:\n",
    "            raise KeyError\n",
    "\n",
    "        return None\n",
    "\n",
    "    def ground_act(self, tokens_, nested_):\n",
    "\n",
    "        ## ---- syntactical parency\n",
    "        tokens_copy = tokens_.copy()\n",
    "        for token in tokens_copy:\n",
    "            for key in nested_:\n",
    "                if token == key.token.head and key.token.dep_ == 'nsubj':\n",
    "                    tokens_.remove(token)\n",
    "                    nested_[key][self.bind_keyword(token, attr='act', subj=None)] = set()\n",
    "\n",
    "        ## ---- conjuncted verbs\n",
    "        tokens_copy = tokens_.copy()\n",
    "        saved_tups = []\n",
    "        for token in tokens_copy:\n",
    "            for subj in nested_:\n",
    "                for act in nested_[subj]:\n",
    "                    if token.head == act.token and token.pos_ == 'VERB':\n",
    "                        assert(token.dep_ in ['conj', 'xcomp', 'advcl']), token.dep_\n",
    "                        tokens_.remove(token)\n",
    "                        saved_tups.append((subj, token))\n",
    "        for subj, token in saved_tups:   \n",
    "            nested_[subj][self.bind_keyword(token, subj=None, attr='act')] = set()\n",
    "\n",
    "        ## ---- common root verbs\n",
    "        ### if the verb has the common root with any of the subjects, bind it. This cause confusion when a sentence contains two or more subjects\n",
    "        tokens_copy = tokens_.copy()\n",
    "        saved_tups = []\n",
    "        for token in tokens_copy:\n",
    "            if token.pos_ == 'VERB':\n",
    "                for subj in nested_:\n",
    "                    if token.sent.root == subj.sent.root:\n",
    "                        tokens_.remove(token)\n",
    "                        saved_tups.append(subj, token)\n",
    "        for subj, token in saved_tups:\n",
    "            nested_[subj][self.bind_keyword(token, subj=None, attr='act')] = set() \n",
    "\n",
    "        ### finally if still some verbs left, use knowledge base\n",
    "        ## subject-object collocation knowledge ground, subjects in the layerbase\n",
    "        tokens_copy = tokens_.copy()\n",
    "        saved_tups = []\n",
    "        subjs = [subj for subj in self.collocations_]\n",
    "        for token in tokens_copy:\n",
    "            if token.pos_ == 'VERB':\n",
    "                tup = self.find_most_simi_subj(subjs, token, attr='act')\n",
    "                if tup:\n",
    "                    subj_, act_, s_ = tup\n",
    "                    tokens_.remove(token)\n",
    "                    saved_tups.append((subj_, act_, token))\n",
    "        for subj, act, token in saved_tups:\n",
    "            nested_[CombToken(None, subj)][CombToken(token, act)] = set()\n",
    "\n",
    "        \n",
    "    def ground_obj(self, tokens_, nested_):\n",
    "\n",
    "        ## syntactically ground\n",
    "        tokens_copy = tokens_.copy()\n",
    "        saved_tups = []\n",
    "        for token in tokens_copy:\n",
    "            for subj in nested_:\n",
    "                for act in nested_[subj]:\n",
    "                    if token.head == act.token and token.pos_ == 'NOUN':\n",
    "                        if token.dep_ in ['dobj']:\n",
    "                            mapped = self.bind_keyword(token,\n",
    "                                                  attr='obj', \n",
    "                                                  subj=subj.keyword)\n",
    "                            if mapped.keyword:\n",
    "                                tokens_.remove(token)\n",
    "                                saved_tups.append((subj, act,\n",
    "                                                   token, mapped))\n",
    "        for subj, act, token, mapped in saved_tups:\n",
    "            nested_[subj][act].add(mapped)\n",
    "\n",
    "        # ground to current surrounding subjects based on knowledge\n",
    "        srd_subjects = [(subj.keyword, subj) for subj in nested_]\n",
    "        if srd_subjects:\n",
    "            subjs, mapped = zip(*srd_subjects)\n",
    "            tokens_copy = tokens_.copy()\n",
    "            saved_tups = []\n",
    "\n",
    "            fix_have=False\n",
    "            for token in tokens_copy:\n",
    "                tup = self.find_most_simi_subj(subjs, token,\n",
    "                                              fix_have=fix_have)\n",
    "                if tup:\n",
    "                    tokens_.remove(token)\n",
    "                    if fix_have:\n",
    "                        subj_, obj_, s_ = tup\n",
    "                        saved_tups.append((mapped[subjs.index(subj_)], obj_, token))\n",
    "                    else:\n",
    "                        subj_, act_, obj_, s_ = tup\n",
    "                        saved_tups.append((mapped[subjs.index(subj_)], act_, obj_, token))\n",
    "            if fix_have:\n",
    "                for subj, obj, token in saved_tups:\n",
    "                    if CombToken(None, self.have) not in nested_[subj]:\n",
    "                        nested_[subj][CombToken(None, self.have)] = set()\n",
    "                    nested_[subj][CombToken(None, self.have)].add(CombToken(token, obj))\n",
    "            else:\n",
    "                for subj, act, obj, token in saved_tups:\n",
    "                    if CombToken(None, act) not in nested_[subj]:\n",
    "                        nested_[subj][CombToken(None, act)] = set()\n",
    "                    nested_[subj][CombToken(None, act)].add(CombToken(token, obj))\n",
    "\n",
    "    #     ## other objects, just query the surroundings in knowledge and see which it belongs to\n",
    "        tokens_copy = tokens_.copy()\n",
    "        saved_tups = []\n",
    "        subjs = [subj for subj in self.collocations_ if self.have in self.collocations_[subj]]\n",
    "        for token in tokens_copy:\n",
    "            tup = self.find_most_simi_subj(subjs, token)\n",
    "            if tup:\n",
    "                subj_, obj_, s_ = tup\n",
    "                tokens_.remove(token)\n",
    "                saved_tups.append((subj_, obj_, token))\n",
    "        for subj, obj, token in saved_tups:\n",
    "            if CombToken(None, self.have) not in nested_[CombToken(None, subj)]:\n",
    "                nested_[CombToken(None, subj)][CombToken(None, self.have)] = set()\n",
    "            nested_[CombToken(None, subj)][CombToken(None, self.have)].add(CombToken(token, obj))\n",
    "        \n",
    "    def conj_copy(self, nested_):\n",
    "        # subjs = []\n",
    "        saved_tups = []\n",
    "        for subj in nested_:\n",
    "            if subj.token:\n",
    "                if subj.token.dep_ == 'conj':\n",
    "                    assert(not nested_[subj])\n",
    "                    for subj_ in nested:\n",
    "                        if subj.token.head == subj_.token and nested_[subj_]:\n",
    "                            saved_tups.append((subj, subj_))\n",
    "                            break\n",
    "\n",
    "        for subj, subj_ in saved_tups:\n",
    "            nested_[subj] = nested_[subj_]\n",
    "    \n",
    "    def dup_combine(self, nested_):\n",
    "        saved_tups = []\n",
    "        for subj in nested_:\n",
    "            assert(subj.keyword)\n",
    "            act_keys = [a.keyword for a in nested_[subj]]\n",
    "            if len(act_keys) == len(set(act_keys)):\n",
    "                continue\n",
    "            dup_keys = [a for a in nested_[subj] if act_keys.count(a.keyword) > 1]\n",
    "            for k in dup_keys:\n",
    "                # only combine those with token not specified\n",
    "                if k.token is None:\n",
    "                    k_dup = [k_ for k_ in dup_keys if k_.keyword == k.keyword and k_.token][0]\n",
    "                    dup_keys.remove(k)\n",
    "                    saved_tups.append((subj, k_dup, k))\n",
    "        for subj, k_des, k in saved_tups:\n",
    "            nested_[subj][k_des] |= nested_[subj][k]\n",
    "            del nested_[subj][k]\n",
    "    \n",
    "    def slice_into_layers(self, nested_):\n",
    "    \n",
    "        from rules.labels import subjects\n",
    "        # solidify first\n",
    "        nested_ = ddict2dict(nested_)\n",
    "\n",
    "        subjs = list(nested_)\n",
    "        group_list = [[subjs[0]]]\n",
    "        subjs = subjs[1:]\n",
    "        while subjs:\n",
    "            for subj in subjs:\n",
    "                # for group in group_list:\n",
    "                matched = [g for g in group_list if subj.token and g[0].token and g[0].token.sent.root==subj.token.sent.root and subj.keyword.t in subjects['character']]\n",
    "                if matched:\n",
    "                    assert(len(matched) == 1)\n",
    "                    matched[0].append(subj)\n",
    "                    subjs.remove(subj)\n",
    "                else:\n",
    "                    group_list.append([subj])\n",
    "                    subjs.remove(subj)\n",
    "\n",
    "        layers = []\n",
    "        for group in group_list:\n",
    "            layer = {}\n",
    "            for subj in group:\n",
    "                layer[subj] = nested_[subj]\n",
    "            layers.append(layer)\n",
    "        return tuple(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have 0.253\n",
      "stand 0.021\n",
      "point_to -0.067\n",
      "sit 0.038\n",
      "hold 0.076\n",
      "fishing 0.062\n",
      "drink 0.057\n",
      "raise -0.024\n",
      "eat 0.122\n",
      "touch 0.025\n",
      "play 0.09\n",
      "think 0.099\n",
      "walk 0.043\n",
      "talk -0.024\n",
      "lie_on 0.068\n",
      "collect 0.045\n",
      "exercise 0.113\n",
      "step_on 0.051\n",
      "work 0.153\n",
      "lie 0.013\n",
      "watch 0.02\n",
      "run 0.142\n",
      "sport -0.007\n",
      "write 0.07\n",
      "bend 0.091\n",
      "present 0.083\n",
      "back_on 0.041\n",
      "show -0.03\n",
      "take 0.172\n",
      "sit_on 0.08\n",
      "run_over 0.059\n"
     ]
    }
   ],
   "source": [
    "layerbase_ = LayerBase()\n",
    "for a in layerbase_.entities_['act']:\n",
    "    print(a, query_simi(a.t, 'use'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man touches a gamepad.\n",
      "[man, touches, gamepad]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({man->man: {touch->touch: set()}}, {->other: {->have: {gamepad->gamepad}}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground = Ground()\n",
    "ground('text/%s.txt' % 'gaming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two men\n",
    "# freelancer: use computer shoud be play computer; each action must bound, if not, skip\n",
    "# Fitness_tracker: -> play: phone -> phone, but should phone -> other, this is dependent on how strict when bind\n",
    "# videographer: ->hold: {camera->phone} but should other: camera. Somehow don't fix the priority but use the similarity to select between all methods (syntactical, free-ground, subject-specified ground, action-specified ground)\n",
    "# Firmware: raise: {robot->arm} but should alien -> robot. Use similarity to select methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really needs to set threshold, let's take maximum anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------------------------- \n",
      "road_sign\n",
      "(#wild(have[tree,sign,sun]), #man(stand))\n",
      "A man is standing in front of trees and signs.\n",
      "[man, standing, trees, signs]\n",
      "({man->man: {stand->stand: set()}}, {->wild: {->have: {sign->sign, tree->tree}}})\n",
      " -------------------------- \n",
      "to_do_list\n",
      "(#background, #accessory(have[leaf]), #accessory(have[leaf]), #chart(have[list,paper]), #woman(bend,write))\n",
      "A woman is writing paper with a pen.\n",
      "[woman, writing, paper, pen]\n",
      "({woman->woman: {write->write: {paper->pencil}, ->hold: {pen->pencil}}},)\n",
      " -------------------------- \n",
      "Gaming\n",
      "(#background, #wild(have[plant]), #other(have[gamepad]), #man(touch,stand))\n",
      "A man touches a gamepad.\n",
      "[man, touches, gamepad]\n",
      "({man->man: {touch->touch: set()}}, {->other: {->have: {gamepad->gamepad}}})\n",
      " -------------------------- \n",
      "Fishing\n",
      "(#background, #wild(have[lake,hill,cloud]), #wild(have[boat]), #man(fishing,stand,hold[fishing_rod]))\n",
      "A man is fishing on the lake.\n",
      "[man, fishing, lake]\n",
      "({man->man: {fish->fishing: set(), ->hold: {lake->fishing_rod}}},)\n",
      " -------------------------- \n",
      "Game_day\n",
      "(#background, #accessory, #home(have[sofa,toy,chips]), #group(have[man1(drink[beer],sit,raise[arm]),man2(sit,raise[arm],eat[chips])]))\n",
      "Two man sat on the sofa celebrating a game.\n",
      "[man, sat, sofa, celebrating, game]\n",
      "({man->man: {sit->sit: set(), celebrate->drink: set()}}, {->home: {->have: {sofa->sofa}}}, {->other: {->have: {game->gamepad}}})\n",
      " -------------------------- \n",
      "Freelancer\n",
      "(#background, #home(have[bookcase,apple,wall_clock]), #woman(play[computer],work,sit))\n",
      "A woman sit in the room and uses her computer.\n",
      "[woman, sit, room, uses, computer]\n",
      "({woman->woman: {sit->sit: set(), use->have: {computer->computer}}}, {->home: {->have: {room->house}}})\n",
      " -------------------------- \n",
      "weather\n",
      "(#background, #accessory(have[plant]), #other(have[bulletin]), #man(stand,point_to))\n",
      "A man is standing to give the weather report.\n",
      "[man, standing, weather, report]\n",
      "({man->man: {stand->stand: set(), ->collect: {report->data}}}, {->wild: {->have: {weather->wind}}})\n",
      " -------------------------- \n",
      "wandering_mind\n",
      "(#background, #other(have[plant,circle]), #man(sit))\n",
      "A man is sitting to rest.\n",
      "[man, sitting, rest]\n",
      "({man->man: {sit->sit: set(), rest->lie_on: set()}},)\n",
      " -------------------------- \n",
      "Fitness_tracker\n",
      "(#office(have[phone]), #wild(have[plant,wind]), #woman(sport,run))\n",
      "A woman runs next to her phone.\n",
      "[woman, runs, phone]\n",
      "({woman->woman: {run->run: set(), ->play: {phone->phone}}},)\n",
      " -------------------------- \n",
      "transfer_money\n",
      "(#background, #woman(sit,play[computer]), #man(stand,hold[phone]), #chart(have[money,arrow]))\n",
      "A man is standing and a woman is sitting with a computer to transfer money.\n",
      "[man, standing, woman, sitting, computer, transfer, money]\n",
      "({man->man: {stand->stand: set()}, woman->woman: {sit->sit: set(), transfer->: set(), ->play: {computer->computer}}}, {->chart: {->have: {money->money}}})\n",
      " -------------------------- \n",
      "wall_post\n",
      "(#background, #other(have[website,plant]), #woman(stand,hold[pencil]))\n",
      "A woman is writing a web page with a pen.\n",
      "[woman, writing, web, page, pen]\n",
      "({woman->woman: {write->write: set(), ->hold: {pen->pencil}}}, {->other: {->have: {page->webpage, web->website}}})\n",
      " -------------------------- \n",
      "Follow_me_drone\n",
      "(#background, #other(have[drone]), #woman(watch,have[head_shot]))\n",
      "A woman watches the drone.\n",
      "[woman, watches, drone]\n",
      "({woman->woman: {watch->watch: set()}}, {->other: {->have: {drone->drone}}})\n",
      " -------------------------- \n",
      "web_development\n",
      "(#background, #other(have[website]), #wild(have[leaf]), #woman(sit,watch,play[computer]))\n",
      "A woman is playing with her computer next to a web page.\n",
      "[woman, playing, computer, web, page]\n",
      "({woman->woman: {play->play: {computer->computer}}}, {->other: {->have: {page->webpage, web->website}}})\n",
      " -------------------------- \n",
      "weather_app\n",
      "(#other(have[sun,cloud]), #woman(sit))\n",
      "A woman is sitting on the clouds and the sun.\n",
      "[woman, sitting, clouds, sun]\n",
      "({woman->woman: {sit->sit: set()}}, {->wild: {->have: {cloud->cloud, sun->sun}}})\n",
      " -------------------------- \n",
      "Focus\n",
      "(#background, #wild(have[tree,plant]), #woman(sit,take[pictures]))\n",
      "A woman takes pictures under the tree.\n",
      "[woman, takes, pictures, tree]\n",
      "({woman->woman: {take->take: {picture->pictures}}}, {->wild: {->have: {tree->tree}}})\n",
      " -------------------------- \n",
      "Fireworks\n",
      "(#background, #woman(stand,talk), #woman(point_to,show,stand), #wild(have[firework,plant]))\n",
      "Two women enjoyed fireworks in the wild.\n",
      "[women, enjoyed, fireworks, wild]\n",
      "({woman->woman: {enjoy->take: set()}}, {wild->wild: {->have: {firework->firework}}})\n",
      " -------------------------- \n",
      "web_developer\n",
      "(#other(have[webpage]), #man(sit,play[computer]))\n",
      "A man is playing with his computer next to a web page.\n",
      "[man, playing, computer, web, page]\n",
      "({man->man: {play->play: {computer->computer}, ->hold: {page->paper}}}, {->other: {->have: {web->website}}})\n",
      " -------------------------- \n",
      "Forgot_password\n",
      "(#background, #man(think))\n",
      "A man is thinking.\n",
      "[man, thinking]\n",
      "({man->man: {think->think: set()}},)\n",
      " -------------------------- \n",
      "Following\n",
      "(#background, #chart(have[list]), #woman(stand,present))\n",
      "A woman is showing a chart.\n",
      "[woman, showing, chart]\n",
      "({woman->woman: {show->show: set()}}, {chart->chart: {}})\n",
      " -------------------------- \n",
      "walking_around\n",
      "(#background, #wild(have[guideboard,plant]), #woman(walk))\n",
      "A woman is walking.\n",
      "[woman, walking]\n",
      "({woman->woman: {walk->walk: set()}},)\n",
      " -------------------------- \n",
      "Gardening\n",
      "(#background, #wild(have[sunflower]), #woman(touch,stand,back_on))\n",
      "A woman touches the sunflowers.\n",
      "[woman, touches, sunflowers]\n",
      "({woman->woman: {touch->touch: {sunflower->flower}}},)\n",
      " -------------------------- \n",
      "videographer\n",
      "(#background, #other(have[camera]), #wild(have[plant]), #man(stand))\n",
      "A man is standing against the camera.\n",
      "[man, standing, camera]\n",
      "({man->man: {stand->stand: set(), ->hold: {camera->phone}}},)\n",
      " -------------------------- \n",
      "warning_cyit\n",
      "(#background, #wild(have[rock,sign]), #woman(stand,hold[flag]))\n",
      "A woman is standing with a flag beside some stones and a sign.\n",
      "[woman, standing, flag, stones, sign]\n",
      "({woman->woman: {stand->stand: set(), ->hold: {flag->flag, sign->flag}}}, {->wild: {->have: {stone->rock}}})\n",
      " -------------------------- \n",
      "Flowers\n",
      "(#wild(have[plant]), #woman(sit,hold[flower]))\n",
      "A woman holds a flower and sits on the grass.\n",
      "[woman, holds, flower, sits, grass]\n",
      "({woman->woman: {hold->hold: {flower->flower}, sit->sit: set()}}, {->accessory: {->have: {grass->leaf}}})\n",
      " -------------------------- \n",
      "Fish_bowl\n",
      "(#wild(have[fish,plant,water,fish_tank,cloud]), #woman(walk,play[phone]))\n",
      "A woman walk past a fish tank.\n",
      "[woman, walk, past, fish, tank]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjustin/miniconda2/envs/text2scene/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Tokens not exhausted!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> left ungrounded tokens: [past]\n",
      "({woman->woman: {walk->walk: set()}}, {->wild: {->have: {fish->fish, tank->fish_tank}}})\n",
      " -------------------------- \n",
      "Firmware\n",
      "(#background, #alien(have[robot],stand), #man(exercise,step_on[stone]), #man(work,play[computer],sit))\n",
      "A man played the computer. A man stepped on the stone and exercised. They had a robot.\n",
      "[man, played, computer, man, stepped, stone, exercised, robot]\n",
      "({man->man: {play->play: {computer->computer}, ->step_on: {stone->stone}, ->raise: {robot->arm}}}, {man->man: {step->step_on: set(), exercise->exercise: set()}})\n",
      " -------------------------- \n",
      "visual_data\n",
      "(#background, #wild(have[tree]), #man(collect[data]))\n",
      "A man is standing to collect data.\n",
      "[man, standing, collect, data]\n",
      "({man->man: {stand->stand: set(), collect->collect: {datum->data}}},)\n",
      " -------------------------- \n",
      "viral_tweet\n",
      "(#wild(have[hill,cloud]), #accessory(have[leaf]), #wild(have[balloon]), #woman(sit,hold[phone]))\n",
      "A woman is sitting in a hot air balloon.\n",
      "[woman, sitting, hot, air, balloon]\n",
      "({woman->woman: {sit->sit: set()}}, {->wild: {->have: {balloon->balloon, air->balloon, hot->sun}}})\n",
      " -------------------------- \n",
      "Forming_ideas\n",
      "(#background, #man(hold[cube],walk), #other(have[triangle,circle,square,tree]))\n",
      "A man walks past some objects.\n",
      "[man, walks, past, objects]\n",
      "-> left ungrounded tokens: [past, objects]\n",
      "({man->man: {walk->walk: set()}},)\n",
      " -------------------------- \n",
      "voice_control\n",
      "(#background, #woman(sit_on[chair]))\n",
      "A woman is sitting at her desk.\n",
      "[woman, sitting, desk]\n",
      "({woman->woman: {sit->sit: set()}}, {desk->office: {}})\n",
      " -------------------------- \n",
      "Friendship\n",
      "(#background, #wild(have[tree,plant]), #man(lie_on[stone],talk), #man(sit,talk))\n",
      "A man talks with his friend under the tree.\n",
      "[man, talks, friend, tree]\n",
      "-> left ungrounded tokens: [friend]\n",
      "({man->man: {talk->talk: set()}}, {->wild: {->have: {tree->tree}}})\n",
      " -------------------------- \n",
      "Forever\n",
      "(#background, #wild(have[plant,beach,sun]), #group(have[man(sit,hold),woman(lie,talk)]))\n",
      "A couple were lying on the beach sunning themselves.\n",
      "[couple, lying, beach, sunning]\n",
      "[sunning]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjustin/miniconda2/envs/text2scene/lib/python3.7/site-packages/ipykernel_launcher.py:65: UserWarning: Verbs not exhausted!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> left ungrounded tokens: [couple]\n",
      "({->woman: {lie->lie: set()}}, {->wild: {->have: {beach->beach, sun->sun}}})\n",
      " -------------------------- \n",
      "wind_turbine\n",
      "(#background, #other(have[windmill]), #woman(walk,watch))\n",
      "A woman is walking in front of three windmills.\n",
      "[woman, walking, windmills]\n",
      "({woman->woman: {walk->walk: set()}}, {->other: {->have: {windmill->windmill}}})\n",
      " -------------------------- \n",
      "Folder\n",
      "(#chart(have[folder,paper]), #man(stand))\n",
      "A man stands next to the folder.\n",
      "[man, stands, folder]\n",
      "({man->man: {stand->stand: set()}}, {->chart: {->have: {folder->folder}}})\n",
      " -------------------------- \n",
      "track_and_field\n",
      "(#background, #woman(exercise,run_over[hurdle]))\n",
      "A woman is running over hurdles.\n",
      "[woman, running, hurdles]\n",
      "({woman->woman: {run->run: set(), ->run_over: {hurdle->hurdle}}},)\n",
      " -------------------------- \n",
      "virtual_reality\n",
      "(#background, #man(lie,watch[movie]))\n",
      "A man is lying watching a movie.\n",
      "[man, lying, watching, movie]\n",
      "({man->man: {lie->lie: set(), watch->watch: {movie->movie}}},)\n",
      " -------------------------- \n",
      "For_sale\n",
      "(#home(have[house,sun,tree]),)\n",
      "A house with some tree.\n",
      "[house, tree]\n",
      "({house->home: {->have: {tree->tree}}},)\n",
      " -------------------------- \n",
      "timeline\n",
      "(#wild(have[plant]), #man(walk,hold[paper]), #chart(have[paper,arrow]))\n",
      "A man is walking with some paper in his hand.\n",
      "[man, walking, paper, hand]\n",
      "({man->man: {walk->walk: set(), ->hold: {paper->paper}, ->raise: {hand->arm}}},)\n",
      " -------------------------- \n",
      "voice_interface\n",
      "(#background, #park(have[plant]), #woman(sit,play[computer]))\n",
      "A woman sits using a computer.\n",
      "[woman, sits, computer]\n",
      "({woman->woman: {sit->sit: set(), ->play: {computer->computer}}},)\n",
      " -------------------------- \n",
      "Frozen\n",
      "(#alien(have[snowman]),)\n",
      "There is a snowman.\n",
      "[snowman]\n",
      "({->alien: {->have: {snowman->snowman}}},)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from tools.containers import Picture\n",
    "ground = Ground()\n",
    "for txt_name in glob.glob('text/*.txt'):\n",
    "    print(' -------------------------- ')\n",
    "    name = re.findall(r'text/(\\w+).txt', txt_name)[0]\n",
    "    print(name)\n",
    "    # name = 'wind_turbine'\n",
    "    print(Picture('images/%s.svg' % name).layers_)\n",
    "    print(ground('text/%s.txt' % name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  web development\n",
    "#  Text: A woman is playing with her computer next to a web page.\n",
    "#  Trouble: two plays: first with woman, second proposed by computer\n",
    "\n",
    "#  weather\n",
    "#  Text: A man is standing to give the weather report.\n",
    "#  Trouble: weather -> wind but should \"weathe report\" -> \"bulletin\"\n",
    "\n",
    "#  videographer\n",
    "#  Text: A man is standing against the camera.\n",
    "#  Trouble: \"camera\" is bound to \"phone\" under \"man\", because they are similar. No way it can be bound to \"camera\" in \"other\" because bindind to subjects in the list is prior\n",
    "\n",
    "#  transfer_money\n",
    "#  Text: A man is standing and a woman is sitting with a computer to transfer money.\n",
    "#  Trouble: \"money\" is syntactically connected to transfer, thus can't be further grounded into charts\n",
    "\n",
    "#  track and field: A woman is running over hurdles.\n",
    "#  hurdle isn't captured by woman. Extend the object capture to any subject\n",
    "\n",
    "#  A man talks with his friend under the tree.\n",
    "#   \"friend\" is not bound but is important. similar cases include couple.\n",
    "\n",
    "# For_sale.txt [resolved]\n",
    "#    syntactically bound surrounding subjects have no action\n",
    "#    thus add default action \n",
    "\n",
    "# Follow_me_drone\n",
    "#    \"woman watch drone\" rather than \"woman watch\" and \"other drone\"\n",
    "#    this is a syntactical grounding case, thus nothing can be done\n",
    "#    may require the difference between strong interaction and weak interaction. E.g. play and watch\n",
    "#    need special care when compare the similarity. keywords in a bag may be suitable in this case. Previously we propose layerwise retrieval, now picture level? The slicing is already done anyway. But then we cannot ensure, e.g. two men in a group is correctly bound, duplicate subjects information is lost after bagging.\n",
    "\n",
    "\n",
    "# text/Focus.txt [resolved]\n",
    "#    tree -> home rather than tree -> wild\n",
    "#    add a new ordering key in find_most_simi_subjs, the frequency of this object under this subject \n",
    "#    similarly, add the frequency key in get_simi_key\n",
    "#        in fact, this is the frequency prior\n",
    "\n",
    "# text/Flowers.txt\n",
    "#    grass -> leaf but should grass -> plant\n",
    "\n",
    "# 'text/Firmware.txt [resolved]\n",
    "#    robot can not be grounded to alien, because it's alien have robot, which is an oject\n",
    "#    -> ground to any subjects which have a \"have\" action, not just surrounding subjects\n",
    "\n",
    "# wild should be separate from man and boy? [resolved]\n",
    "#    generally if both are characters then in a layer group? surrounding cannot be in a group technically\n",
    "\n",
    "# chair should be with sit on?\n",
    "# but sofa should not be with lie on\n",
    "# what to do\n",
    "\n",
    "# in alien, lie is not bound to any keyword\n",
    "# because lie isn't in the collocations of alien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seemly that we don't need a second step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{man1(subj): {sit(act): set(),\n",
       "  drink(act): {beer(obj)},\n",
       "  raise(act): {arm(obj)}},\n",
       " man2(subj): {sit(act): set(), raise(act): {arm(obj)}, eat(act): {chips(obj)}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### group layer exclusion check\n",
    "sorted(list(layerbase.layer_vocab_))[12].nested_entities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subj': {man(subj): 6}, 'act': {sit(act): 6, drink(act): 3, raise(act): 6, eat(act): 3}, 'obj': {beer(obj): 3, arm(obj): 6, chips(obj): 3}}\n",
      "{'subj': {man(subj): 6}, 'act': {sit(act): 6, drink(act): 3, raise(act): 6, eat(act): 3}, 'obj': {beer(obj): 3, arm(obj): 6, chips(obj): 3}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x96 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### absorption check\n",
    "from tools.containers import LayerName\n",
    "from tools.image_process import getLayerNames\n",
    "layer = LayerName(getLayerNames('images/Game_day.svg')[-1])#\n",
    "layer_c = LayerName()\n",
    "# layer_c.triples_\n",
    "# print(layer_c.entities_['subj'])\n",
    "layer_c.absorb(layer)\n",
    "layer_c.absorb(layer)\n",
    "layer_c.absorb(layer)\n",
    "layer_c.triples_\n",
    "# print(layer)\n",
    "# print(layer.entities_)\n",
    "# layer.collapse_subj()\n",
    "# print(layer.entities_)\n",
    "# layer.print_()\n",
    "# layer_c.plot()\n",
    "print(layer_c.entities_) # plot()\n",
    "print(layer_c._get_entities())\n",
    "layer_c.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2scene",
   "language": "python",
   "name": "text2scene"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
