{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det(man-2, A:DT-1)\n",
      "nsubj(playing-7, man:NN-2)\n",
      "cc(man-2, and:CC-3)\n",
      "det(boy-5, a:DT-4)\n",
      "conj(man-2, boy:NN-5)\n",
      "aux(playing-7, are:VBP-6)\n",
      "ROOT(playing-7, playing:VBG-7)\n",
      "dobj(playing-7, computer:NN-8)\n",
      "cc(playing-7, and:CC-9)\n",
      "conj(playing-7, sitting:VBG-10)\n",
      "prep(sitting-10, on:IN-11)\n",
      "det(ground-13, the:DT-12)\n",
      "pobj(on-11, ground:NN-13)\n",
      "prep(sitting-10, in:IN-14)\n",
      "det(wild-16, the:DT-15)\n",
      "pobj(in-14, wild:NN-16)\n",
      "punct(playing-7, .:.-17)\n",
      "det(woman-19, A:DT-18)\n",
      "nsubj(waving-21, woman:NN-19)\n",
      "aux(waving-21, is:VBZ-20)\n",
      "ROOT(waving-21, waving:VBG-21)\n",
      "prep(waving-21, under:IN-22)\n",
      "det(tree-24, the:DT-23)\n",
      "pobj(under-22, tree:NN-24)\n",
      "punct(waving-21, .:.-25)\n",
      "det(girl-27, A:DT-26)\n",
      "nsubj(sitting-29, girl:NN-27)\n",
      "aux(sitting-29, is:VBZ-28)\n",
      "ROOT(sitting-29, sitting:VBG-29)\n",
      "prep(sitting-29, on:IN-30)\n",
      "det(chair-32, the:DT-31)\n",
      "pobj(on-30, chair:NN-32)\n",
      "punct(sitting-29, .:.-33)\n",
      "det(man-35, A:DT-34)\n",
      "nsubj(lying-37, man:NN-35)\n",
      "aux(lying-37, is:VBZ-36)\n",
      "ROOT(lying-37, lying:VBG-37)\n",
      "prep(lying-37, on:IN-38)\n",
      "det(games-42, the:DT-39)\n",
      "compound(games-42, sofa:NN-40)\n",
      "compound(games-42, playing:NN-41)\n",
      "pobj(on-38, games:NNS-42)\n",
      "punct(lying-37, .:.-43)\n",
      "expl(are-45, There:EX-44)\n",
      "ROOT(are-45, are:VBP-45)\n",
      "det(trees-47, some:DT-46)\n",
      "attr(are-45, trees:NNS-47)\n",
      "cc(trees-47, and:CC-48)\n",
      "conj(trees-47, plants:NNS-49)\n",
      "cc(plants-49, and:CC-50)\n",
      "conj(plants-49, flowers:NNS-51)\n",
      "prep(trees-47, in:IN-52)\n",
      "det(park-54, the:DT-53)\n",
      "pobj(in-52, park:NN-54)\n",
      "punct(are-45, .:.-55)\n",
      "det(alien-57, An:DT-56)\n",
      "nsubj(lying-59, alien:NN-57)\n",
      "aux(lying-59, is:VBZ-58)\n",
      "ROOT(lying-59, lying:VBG-59)\n",
      "xcomp(lying-59, crying:VBG-60)\n",
      "punct(lying-59, .:.-61)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'A man and a boy are playing computer and sitting on the ground in the wild. A woman is waving under the tree. A girl is sitting on the chair. A man is lying on the sofa playing games. There are some trees and plants and flowers in the park. An alien is lying crying.')\n",
    "# parse(doc)\n",
    "# 'he is '\n",
    "# 'mike is'\n",
    "for token in doc:\n",
    "    print(\"{2}({3}-{6}, {0}:{1}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rules.labels import subjects\n",
    "from tools.knowledge import LayerBase, TextBase\n",
    "layerbase = LayerBase()\n",
    "# prs_subjects = set([subj for subj in layerbase.entities_['subj'] if subj in subjects['character']])\n",
    "# prs_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similairties between subjects: the largest common subset\n",
    "#     E.g. 1. text: man man and base: man woman\n",
    "#              man - man: 1 + man - woman: 0.5\n",
    "#          2. TEXT: man man and BASE: man\n",
    "#              man - man: 1\n",
    "#              so choose man and woman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from rules.labels import subjects\n",
    "from collections import defaultdict\n",
    "from tools.common import ddict2dict\n",
    "from tools.instance import Node\n",
    "from query_relatedness import query_simi\n",
    "import warnings\n",
    "\n",
    "# class Ground:\n",
    "#     def __init__(self):\n",
    "#         self.layerbase = LayerBase()\n",
    "#         self.prs_subjects = set([subj for subj in layerbase.entities_['subj'] if subj in subjects['character']])\n",
    "#         # self.srd_subjects = \n",
    "\n",
    "def incre_name(s, dic):\n",
    "    count = 0\n",
    "    for k in dic:\n",
    "        # remove any tail digits\n",
    "        sub = re.sub(r'(?<=\\w)\\d+$','', k)\n",
    "        if re.match(r'%s\\d*' % sub, s):\n",
    "            # print(count, r'%s\\d*' % sub, s)\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        return '%s%i' % (s, count)\n",
    "    return s\n",
    "\n",
    "def get_tokens(doc):\n",
    "    return [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "\n",
    "def get_simi_keyword(token, keywords, thresh=0.5):\n",
    "    \n",
    "    tups = [(k, query_simi(token.lemma_, k.t)) for k in keywords]\n",
    "    simi_key, simi_ = sorted(tups, key=lambda x: (x[1], x[0].count))[-1]\n",
    "    if simi_ >= thresh:\n",
    "        return simi_key, simi_\n",
    "#     return None\n",
    "#     for keyword in keywords:\n",
    "#         if  > thresh:\n",
    "#             return keyword\n",
    "    return None\n",
    "\n",
    "class MappedToken:\n",
    "    def __init__(self, token, keyword):\n",
    "        if token:\n",
    "            assert(isinstance(token, spacy.tokens.token.Token))\n",
    "        assert(isinstance(keyword, Node))\n",
    "        self.token = token\n",
    "        self.keyword = keyword\n",
    "        \n",
    "        self.tup = (self.token, self.keyword)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.token is None:\n",
    "            return '->%s' % (self.keyword.t)\n",
    "        return '%s->%s' % (self.token.lemma_, self.keyword.t)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.tup == other.tup\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.tup)\n",
    "    \n",
    "def ground_subj(tokens_, nested_):\n",
    "    \n",
    "    ## query the base to identify a subject\n",
    "    tokens_copy = tokens_.copy()\n",
    "    subjs = [subj for subj in layerbase.collocations_]\n",
    "    for token in tokens_copy:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            tup = get_simi_keyword(token, subjs, thresh=0.5)\n",
    "            if tup:\n",
    "                subj_, _ = tup\n",
    "                tokens_.remove(token)\n",
    "                nested_[MappedToken(token, subj_)] = defaultdict(set)\n",
    "#         for type_ in subjects:\n",
    "#             if token.lemma_ in subjects[type_]:\n",
    "#                 tokens_.remove(token)\n",
    "#                 nested_[token] = defaultdict(set)\n",
    "#                 if token.lemma_ not in nested_:\n",
    "#                     map_dict[token] = token.lemma_\n",
    "#                     nested_[token.lemma_] = defaultdict(set)\n",
    "#                 else:\n",
    "#                     lemma_i = incre_name(token.lemma_, nested_)\n",
    "#                     map_dict[token] = lemma_i\n",
    "#                     nested_[lemma_i] = defaultdict(set)\n",
    "\n",
    "def bind_keyword(token, dest='obj', subj=None, act=None, thresh=0.3):\n",
    "    # bind keyword\n",
    "    # if act is None:\n",
    "    if dest == 'act':\n",
    "        # used to bind actions\n",
    "        if subj:\n",
    "            keywords = [k for k in layerbase.collocations_[subj]]\n",
    "        else:\n",
    "            keywords = [k for k in layerbase.entities_['act']]\n",
    "        attr = 'act'\n",
    "    elif dest == 'obj':\n",
    "        # used to bind objects\n",
    "        # maybe, search all objects regardless of subj and act\n",
    "        # error case: \"woman transfer money\" should be woman transfer and \"money\"\n",
    "        if subj and act:\n",
    "            keywords = [k for k in layerbase.collocations_[subj][act]]      \n",
    "        elif subj:\n",
    "            keywords = [k for a in layerbase.collocations_[subj] for k in layerbase.collocations_[subj][a]]\n",
    "        else:        \n",
    "            keywords = [k for k in layerbase.entities_['obj']]\n",
    "        attr = 'obj'\n",
    "    else:\n",
    "        raise KeyError\n",
    "        \n",
    "    tup = get_simi_keyword(token, keywords, thresh=thresh)\n",
    "    if tup:\n",
    "        k_, _ = tup\n",
    "    else:\n",
    "        k_ = Node('', attr=attr)\n",
    "    return MappedToken(token, k_)\n",
    "\n",
    "def wrap_multiple_actions(token, subj, acts, thresh=0.2):\n",
    "    tups = []\n",
    "    for act in acts:\n",
    "        if not layerbase.collocations_[subj][act]: continue\n",
    "        tup = get_simi_keyword(token,\n",
    "                               layerbase.collocations_[subj][act],\n",
    "                               thresh=thresh)\n",
    "        if tup:\n",
    "            tups.append((act,) + tup)\n",
    "    if tups:\n",
    "        return sorted(tups, key=lambda x: (x[-1], x[0].count))[-1]\n",
    "    return None\n",
    "    \n",
    "def find_most_simi_subj(subjs, token, dest='obj', fix_have=True):\n",
    "    assert(isinstance(token, spacy.tokens.token.Token))\n",
    "    assert(all([isinstance(subj, Node) for subj in subjs]))\n",
    "    # assert(all([s.t in subjects['surrounding'] for s in subjs]))\n",
    "    # search all subjects, not just surrounding subjects. E.g. Alien have robot\n",
    "    if dest == 'obj':\n",
    "        most_simi_ = []\n",
    "        for subj in subjs:\n",
    "            if fix_have:\n",
    "                assert(all([Node('have', attr='act') in layerbase.collocations_[s] for s in subjs]))\n",
    "                if not layerbase.collocations_[subj][Node('have', attr='act')]:\n",
    "                    continue\n",
    "                tup = get_simi_keyword(token,\n",
    "                                       layerbase.collocations_[subj][Node('have', attr='act')],\n",
    "                                   thresh=0.2)\n",
    "            else:\n",
    "                if not layerbase.collocations_[subj]: continue\n",
    "                tup = wrap_multiple_actions(token, subj,\n",
    "                                    layerbase.collocations_[subj],\n",
    "                                    thresh=0.2)               \n",
    "            if tup:\n",
    "                most_simi_.append((subj,) + tup)\n",
    "\n",
    "        if most_simi_:\n",
    "            if fix_have:\n",
    "                sort_f = lambda x: (x[-1], get_element_from_set(layerbase.collocations_[x[0]][Node('have', attr='act')], x[1]).count)\n",
    "            else:\n",
    "                sort_f = lambda x: (x[-1], get_element_from_set(layerbase.collocations_[x[0]][x[1]], x[2]).count)\n",
    "            return sorted(most_simi_, key=sort_f)[-1]\n",
    "        \n",
    "    elif dest == 'act':\n",
    "        most_simi_ = []\n",
    "        for subj in subjs:\n",
    "            # if no action under this suject, skip it\n",
    "            if not layerbase.collocations_[subj]: continue\n",
    "            tup = get_simi_keyword(token,\n",
    "                                   layerbase.collocations_[subj], \n",
    "                                   thresh=0.2)\n",
    "            if tup:\n",
    "                most_simi_.append((subj,) + tup)\n",
    "        if most_simi_:\n",
    "            sort_f = lambda x: (x[-1], get_element_from_set(layerbase.collocations_[x[0]], x[1]).count)\n",
    "            return sorted(most_simi_, key=sort_f)[-1]\n",
    "    else:\n",
    "        raise KeyError\n",
    "        \n",
    "    return None\n",
    "\n",
    "def ground_act(tokens_, nested_):\n",
    "    \n",
    "    ## ---- syntactical parency\n",
    "    tokens_copy = tokens_.copy()\n",
    "    for token in tokens_copy:\n",
    "        for key in nested_:\n",
    "            if token == key.token.head and key.token.dep_ == 'nsubj':\n",
    "                tokens_.remove(token)\n",
    "                \n",
    "                # bind keyword\n",
    "#                 acts = [a for a in layerbase.collocations_[key.keyword]]\n",
    "#                 tup = get_simi_keyword(token, acts, thresh=0.3)\n",
    "#                 if tup:\n",
    "#                     act_, _ = tup\n",
    "#                 else:\n",
    "#                     act_ = Node('', attr='act')\n",
    "                # print(token, key.keyword)\n",
    "                nested_[key][bind_keyword(token, dest='act', subj=None)] = set()\n",
    "    \n",
    "    ## ---- conjuncted verbs\n",
    "    tokens_copy = tokens_.copy()\n",
    "    # cannot pickle a spacy.token\n",
    "    ## nested_copy = deepcopy(nested_)\n",
    "    # cannnot modify a dict during iteration, thus save keys\n",
    "    saved_tups = []\n",
    "    for token in tokens_copy:\n",
    "        for subj in nested_:\n",
    "            for act in nested_[subj]:\n",
    "                if token.head == act.token and token.pos_ == 'VERB':\n",
    "                    assert(token.dep_ in ['conj', 'xcomp', 'advcl']), token.dep_\n",
    "                    tokens_.remove(token)\n",
    "                    saved_tups.append((subj, token))\n",
    "    for subj, token in saved_tups:   \n",
    "        nested_[subj][bind_keyword(token, subj=None, dest='act')] = set()\n",
    "    \n",
    "    ## ---- other verbs\n",
    "    ### if the verb has the common root with any of the subjects, bind it. This cause confusion when a sentence contains two or more subjects\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    for token in tokens_copy:\n",
    "        if token.pos_ == 'VERB':\n",
    "            for subj in nested_:\n",
    "                if token.sent.root == subj.sent.root:\n",
    "                    tokens_.remove(token)\n",
    "                    saved_tups.append(subj, token)\n",
    "    for subj, token in saved_tups:\n",
    "        nested_[subj][bind_keyword(token, subj=None, dest='act')] = set() \n",
    "        \n",
    "    ### finally if still some verbs left, use knowledge base\n",
    "    ## subject-object collocation knowledge ground, subjects in the layerbase\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    subjs = [subj for subj in layerbase.collocations_]\n",
    "    for token in tokens_copy:\n",
    "        if token.pos_ == 'VERB':\n",
    "            tup = find_most_simi_subj(subjs, token, dest='act')\n",
    "            if tup:\n",
    "                subj_, act_, s_ = tup\n",
    "                tokens_.remove(token)\n",
    "                saved_tups.append((subj_, act_, token))\n",
    "    for subj, act, token in saved_tups:\n",
    "        nested_[MappedToken(None, subj)][MappedToken(token, act)] = set()\n",
    "    \n",
    "# static var stuff\n",
    "# related dict needs to transformed to dict\n",
    "# def query_related(t, k):\n",
    "#     import dill\n",
    "#     with open('relateDict.pkl', 'rb') as f:\n",
    "#         relateDict = dill.load(f) \n",
    "#     if k in relateDict and t in relateDict[k]:\n",
    "#         return relateDict[k][t]\n",
    "    \n",
    "#     import requests\n",
    "#     return requests.get('http://api.conceptnet.io/relatedness?node1=/c/en/%s&node2=/c/en/%s' % (k, t)).json()['value']\n",
    "\n",
    "def get_element_from_set(set_, e_):\n",
    "    for e in set_:\n",
    "        if e == e_:\n",
    "            return e\n",
    "\n",
    "\n",
    "        \n",
    "def ground_obj(tokens_, nested_):\n",
    "    \n",
    "    ## syntactically ground\n",
    "    ### E.g. man plays computer: computer -> play -> man\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    for token in tokens_copy:\n",
    "        for subj in nested_:\n",
    "            for act in nested_[subj]:\n",
    "                if token.head == act.token and token.pos_ == 'NOUN':\n",
    "                    if token.dep_ in ['dobj']:\n",
    "                        # unbound subjects are neglected, such as \"couple\"\n",
    "                        tokens_.remove(token)\n",
    "                        saved_tups.append((subj, act, token))\n",
    "    for subj, act, token in saved_tups:\n",
    "        # nested_[subj][act].add(token)\n",
    "        nested_[subj][act].add(bind_keyword(token,\n",
    "                                            dest='obj', \n",
    "                                            subj=None))\n",
    "        \n",
    "    # ground to current surrounding subjects based on knowledge\n",
    "    ## subject-object collocation knowledge ground, current subjects only\n",
    "    ### surrounding only. because obj in character must be syntactically grounded\n",
    "    ### not necessarily, obj can be knowledgely grounded to characters if th action is have. E.g. Alien have robot\n",
    "    \n",
    "    ## srd_subjects = [(subj.keyword, subj) for subj in nested_ if subj.keyword.t in subjects['surrounding']]\n",
    "    # srd_subjects = [(subj.keyword, subj) for subj in nested_ if Node('have', attr='act') in layerbase.collocations_[subj.keyword]]\n",
    "    srd_subjects = [(subj.keyword, subj) for subj in nested_]\n",
    "    ## if no surrounding subjects are captured now, skip this step\n",
    "    if srd_subjects:\n",
    "        subjs, mapped = zip(*srd_subjects)\n",
    "        tokens_copy = tokens_.copy()\n",
    "        saved_tups = []\n",
    "\n",
    "        fix_have=False\n",
    "        for token in tokens_copy:\n",
    "            tup = find_most_simi_subj(subjs, token,\n",
    "                                      fix_have=fix_have)\n",
    "            if tup:\n",
    "                tokens_.remove(token)\n",
    "                if fix_have:\n",
    "                    subj_, obj_, s_ = tup\n",
    "                    saved_tups.append((mapped[subjs.index(subj_)], obj_, token))\n",
    "                else:\n",
    "                    subj_, act_, obj_, s_ = tup\n",
    "                    saved_tups.append((mapped[subjs.index(subj_)], act_, obj_, token))\n",
    "                ## use original text in the description as key\n",
    "                ## or use grounding keyword as key\n",
    "        if fix_have:\n",
    "            for subj, obj, token in saved_tups:\n",
    "                if MappedToken(None, Node('have', attr='act')) not in nested_[subj]:\n",
    "                    nested_[subj][MappedToken(None, Node('have', attr='act'))] = set()\n",
    "                nested_[subj][MappedToken(None, Node('have', attr='act'))].add(MappedToken(token, obj))\n",
    "        else:\n",
    "            for subj, act, obj, token in saved_tups:\n",
    "                if MappedToken(None, act) not in nested_[subj]:\n",
    "                    nested_[subj][MappedToken(None, act)] = set()\n",
    "                nested_[subj][MappedToken(None, act)].add(MappedToken(token, obj))\n",
    "        \n",
    "#         for subj in nested_:\n",
    "#             if subj.lemma_ in subjects['surrounding']:\n",
    "#                 assert(subj.lemma_ in layerbase.layer_merge_.nested_entities_)\n",
    "# #                 if token.lemma_ in layerbase.layer_merge_.nested_entities_[subj.lemma_]['have']:            \n",
    "# #                     print(subj, token)\n",
    "#                 # get the most similar obj in the vocabulary under this subject\n",
    "#                 obj_ = get_simi_obj(token.lemma_,\n",
    "#                                     layerbase.layer_merge_.nested_entities_[subj.lemma_]['have'],\n",
    "#                                     thresh=0.2)\n",
    "#                 print(token.lemma_, obj_)\n",
    "#                 if obj_:\n",
    "#                     tokens_.remove(token)\n",
    "#                     saved_tups.append((subj, token))\n",
    "#                     # prevent other subjs containing the same object emerging\n",
    "#                     # first come, first serve\n",
    "#                     break \n",
    "#     for subj, token in saved_tups:\n",
    "#         nested_[subj]['have'].add(token)\n",
    "        \n",
    "#     ## other objects, just query the surroundings in knowledge and see which it belongs to\n",
    "    ## subject-object collocation knowledge ground, subjects in the layerbase\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    # subjs = [subj for subj in layerbase.collocations_ if subj.t in subjects['surrounding']]\n",
    "    subjs = [subj for subj in layerbase.collocations_ if Node('have', attr='act') in layerbase.collocations_[subj]]\n",
    "    for token in tokens_copy:\n",
    "        tup = find_most_simi_subj(subjs, token)\n",
    "        if tup:\n",
    "            subj_, obj_, s_ = tup\n",
    "            # print(token, obj_, s_, subj_)\n",
    "            tokens_.remove(token)\n",
    "            ## use original text in the description as key\n",
    "            ## or use grounding keyword as key\n",
    "            # saved_tups.append((subj, token))\n",
    "            saved_tups.append((subj_, obj_, token))\n",
    "    for subj, obj, token in saved_tups:\n",
    "        if MappedToken(None, Node('have', attr='act')) not in nested_[MappedToken(None, subj)]:\n",
    "            nested_[MappedToken(None, subj)][MappedToken(None, Node('have', attr='act'))] = set()\n",
    "        nested_[MappedToken(None, subj)][MappedToken(None, Node('have', attr='act'))].add(MappedToken(token, obj))\n",
    "        \n",
    "#         for subj in layerbase.layer_merge_.nested_entities_:\n",
    "#             if subj not in subjects['surrounding']:\n",
    "#                 continue\n",
    "#             for act in layerbase.layer_merge_.nested_entities_[subj]:\n",
    "#                 obj_ = get_simi_obj(token.lemma_,\n",
    "#                                     layerbase.layer_merge_.nested_entities_[subj][act],\n",
    "#                                     thresh=0.3)\n",
    "#                 if obj_:\n",
    "#                     assert(act == 'have'), (act, token)\n",
    "#                     # assert(subj not in [s.lemma_ for s in nested_])\n",
    "#                     ## Attention! here the key type is string now\n",
    "#                     print(token, subj, obj_)\n",
    "#                     tokens_.remove(token)\n",
    "#                     nested_[subj][act].add(token)\n",
    "#                     break\n",
    "                    \n",
    "\n",
    "# def comb_obj(tokens_, nested_):\n",
    "    \n",
    "#     ## layers built from clusters\n",
    "#     tokens_copy = tokens_.copy()\n",
    "#     for token in tokens_copy:\n",
    "\n",
    "def conj_copy(nested_):\n",
    "    # subjs = []\n",
    "    saved_tups = []\n",
    "    for subj in nested_:\n",
    "        if subj.token:\n",
    "            if subj.token.dep_ == 'conj':\n",
    "                assert(not nested_[subj])\n",
    "                for subj_ in nested:\n",
    "                    if subj.token.head == subj_.token and nested_[subj_]:\n",
    "                        saved_tups.append((subj, subj_))\n",
    "                        break\n",
    "                        \n",
    "    for subj, subj_ in saved_tups:\n",
    "        nested_[subj] = nested_[subj_]\n",
    "    # if conj, copy syntactic children    \n",
    "    \n",
    "def slice_into_layers(nested_):\n",
    "    \n",
    "    # solidify first\n",
    "    nested_ = ddict2dict(nested_)\n",
    "    \n",
    "    subjs = list(nested_)\n",
    "    group_list = [[subjs[0]]]\n",
    "    subjs = subjs[1:]\n",
    "    while subjs:\n",
    "        for subj in subjs:\n",
    "            # for group in group_list:\n",
    "            matched = [g for g in group_list if subj.token and g[0].token and g[0].token.sent.root==subj.token.sent.root and subj.keyword.t in subjects['character']]\n",
    "#                 if subj.token.sent.root == group[0].token.sent.root:\n",
    "#                     group.append(subj)\n",
    "#                     break\n",
    "            if matched:\n",
    "                assert(len(matched) == 1)\n",
    "                matched[0].append(subj)\n",
    "                subjs.remove(subj)\n",
    "            else:\n",
    "                group_list.append([subj])\n",
    "                subjs.remove(subj)\n",
    "                \n",
    "    layers = []\n",
    "    for group in group_list:\n",
    "        layer = {}\n",
    "        for subj in group:\n",
    "            layer[subj] = nested_[subj]\n",
    "        layers.append(layer)\n",
    "    return tuple(layers)\n",
    "        \n",
    "\n",
    "####\n",
    "# technically all entities can be grounded based on similarity, no need to exactly same\n",
    "####\n",
    "\n",
    "def ground(filename):\n",
    "    with open(filename) as f:\n",
    "        text = f.read()\n",
    "    print(text)\n",
    "    doc = nlp(u'%s' % text.strip('\\n'))\n",
    "    \n",
    "    nested = defaultdict(lambda: defaultdict(set))\n",
    "    tokens = get_tokens(doc)\n",
    "    print(tokens)\n",
    "    # map_dict = defaultdict(str)\n",
    "    ground_subj(tokens, nested)\n",
    "    print('-> ground subjects:', ddict2dict(nested))\n",
    "    # print(tokens)\n",
    "    ground_act(tokens, nested)\n",
    "    print('-> ground actions:', ddict2dict(nested))\n",
    "    # print(tokens)\n",
    "    # no verbs will be left\n",
    "    if not all([t.pos_ != 'VERB' for t in tokens]):\n",
    "        warnings.warn('Verbs not exhausted!')\n",
    "        print([t for t in tokens if t.pos_ == 'VERB'])\n",
    "\n",
    "    # print(tokens)\n",
    "    ground_obj(tokens, nested)\n",
    "    print('-> left ungrounded tokens:', tokens)\n",
    "    conj_copy(nested)\n",
    "    layers = slice_into_layers(nested)\n",
    "    return layers\n",
    "# nested\n",
    "# # print(map_dict)\n",
    "# nested\n",
    "\n",
    "## why don't we use scapy token as dict keys, such that no overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------------------------- \n",
      "road_sign\n",
      "(#wild(have[tree,sign,sun]), #man(stand))\n",
      "A man is standing in front of trees and signs.\n",
      "\n",
      "[man, standing, trees, signs]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {stand->stand: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "to_do_list\n",
      "(#background, #accessory(have[leaf]), #accessory(have[leaf]), #chart(have[list,paper]), #woman(bend,write))\n",
      "A woman is writing paper with a pen.\n",
      "\n",
      "[woman, writing, paper, pen]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {write->write: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Gaming\n",
      "(#background, #wild(have[plant]), #other(have[gamepad]), #man(touch,stand))\n",
      "A man touches a gamepad.\n",
      "\n",
      "[man, touches, gamepad]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {touch->touch: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Fishing\n",
      "(#background, #wild(have[lake,hill,cloud]), #wild(have[boat]), #man(fishing,stand,hold[fishing_rod]))\n",
      "A man is fishing on the lake.\n",
      "\n",
      "[man, fishing, lake]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {fish->fishing: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Game_day\n",
      "(#background, #accessory, #home(have[sofa,toy,chips]), #group(have[man1(drink[beer],sit,raise[arm]),man2(sit,raise[arm],eat[chips])]))\n",
      "Two man sat on the sofa celebrating a game.\n",
      "\n",
      "[man, sat, sofa, celebrating, game]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {sit->sit: set(), celebrate->: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Freelancer\n",
      "(#background, #home(have[bookcase,apple,wall_clock]), #woman(play[computer],work,sit))\n",
      "A woman sit in the room and uses her computer.\n",
      "\n",
      "[woman, sit, room, uses, computer]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {sit->sit: set(), use->: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "weather\n",
      "(#background, #accessory(have[plant]), #other(have[bulletin]), #man(stand,point_to))\n",
      "A man is standing to give the weather report.\n",
      "\n",
      "[man, standing, weather, report]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {stand->stand: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "wandering_mind\n",
      "(#background, #other(have[plant,circle]), #man(sit))\n",
      "A man is sitting to rest.\n",
      "\n",
      "[man, sitting, rest]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {sit->sit: set(), rest->lie_on: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Fitness_tracker\n",
      "(#office(have[phone]), #wild(have[plant,wind]), #woman(sport,run))\n",
      "A woman runs next to her phone.\n",
      "\n",
      "[woman, runs, phone]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {run->run: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "transfer_money\n",
      "(#background, #woman(sit,play[computer]), #man(stand,hold[phone]), #chart(have[money,arrow]))\n",
      "A man is standing and a woman is sitting with a computer to transfer money.\n",
      "\n",
      "[man, standing, woman, sitting, computer, transfer, money]\n",
      "-> ground subjects: {man->man: {}, woman->woman: {}}\n",
      "-> ground actions: {man->man: {stand->stand: set()}, woman->woman: {sit->sit: set(), transfer->: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "wall_post\n",
      "(#background, #other(have[website,plant]), #woman(stand,hold[pencil]))\n",
      "A woman is writing a web page with a pen.\n",
      "\n",
      "[woman, writing, web, page, pen]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {write->write: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Follow_me_drone\n",
      "(#background, #other(have[drone]), #woman(watch,have[head_shot]))\n",
      "A woman watches the drone.\n",
      "\n",
      "[woman, watches, drone]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {watch->watch: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "web_development\n",
      "(#background, #other(have[website]), #wild(have[leaf]), #woman(sit,watch,play[computer]))\n",
      "A woman is playing with her computer next to a web page.\n",
      "\n",
      "[woman, playing, computer, web, page]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {play->play: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "weather_app\n",
      "(#other(have[sun,cloud]), #woman(sit))\n",
      "A woman is sitting on the clouds and the sun.\n",
      "\n",
      "[woman, sitting, clouds, sun]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {sit->sit: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Focus\n",
      "(#background, #wild(have[tree,plant]), #woman(sit,take[pictures]))\n",
      "A woman takes pictures under the tree.\n",
      "\n",
      "[woman, takes, pictures, tree]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {take->take: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Fireworks\n",
      "(#background, #woman(stand,talk), #woman(point_to,show,stand), #wild(have[firework,plant]))\n",
      "Two women enjoyed fireworks in the wild.\n",
      "\n",
      "[women, enjoyed, fireworks, wild]\n",
      "-> ground subjects: {woman->woman: {}, wild->wild: {}}\n",
      "-> ground actions: {woman->woman: {enjoy->: set()}, wild->wild: {}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "web_developer\n",
      "(#other(have[webpage]), #man(sit,play[computer]))\n",
      "A man is playing with his computer next to a web page.\n",
      "\n",
      "[man, playing, computer, web, page]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {play->play: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Forgot_password\n",
      "(#background, #man(think))\n",
      "A man is thinking.\n",
      "\n",
      "[man, thinking]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {think->think: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Following\n",
      "(#background, #chart(have[list]), #woman(stand,present))\n",
      "A woman is showing a chart.\n",
      "\n",
      "[woman, showing, chart]\n",
      "-> ground subjects: {woman->woman: {}, chart->chart: {}}\n",
      "-> ground actions: {woman->woman: {show->show: set()}, chart->chart: {}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "walking_around\n",
      "(#background, #wild(have[guideboard,plant]), #woman(walk))\n",
      "A woman is walking.\n",
      "\n",
      "[woman, walking]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {walk->walk: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Gardening\n",
      "(#background, #wild(have[sunflower]), #woman(touch,stand,back_on))\n",
      "A woman touches the sunflowers.\n",
      "\n",
      "[woman, touches, sunflowers]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {touch->touch: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "videographer\n",
      "(#background, #other(have[camera]), #wild(have[plant]), #man(stand))\n",
      "A man is standing against the camera.\n",
      "\n",
      "[man, standing, camera]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {stand->stand: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "warning_cyit\n",
      "(#background, #wild(have[rock,sign]), #woman(stand,hold[flag]))\n",
      "A woman is standing with a flag beside some stones and a sign.\n",
      "\n",
      "[woman, standing, flag, stones, sign]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {stand->stand: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Flowers\n",
      "(#wild(have[plant]), #woman(sit,hold[flower]))\n",
      "A woman holds a flower and sits on the grass.\n",
      "\n",
      "[woman, holds, flower, sits, grass]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {hold->hold: set(), sit->sit: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Fish_bowl\n",
      "(#wild(have[fish,plant,water,fish_tank,cloud]), #woman(walk,play[phone]))\n",
      "A woman walk past a fish tank.\n",
      "\n",
      "[woman, walk, past, fish, tank]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {walk->walk: set()}}\n",
      "-> left ungrounded tokens: [past]\n",
      " -------------------------- \n",
      "Firmware\n",
      "(#background, #alien(have[robot],stand), #man(exercise,step_on[stone]), #man(work,play[computer],sit))\n",
      "A man played the computer. A man stepped on the stone and exercised. They had a robot.\n",
      "\n",
      "[man, played, computer, man, stepped, stone, exercised, robot]\n",
      "-> ground subjects: {man->man: {}, man->man: {}}\n",
      "-> ground actions: {man->man: {play->play: set()}, man->man: {step->step_on: set(), exercise->exercise: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "visual_data\n",
      "(#background, #wild(have[tree]), #man(collect[data]))\n",
      "A man is standing to collect data.\n",
      "\n",
      "[man, standing, collect, data]\n",
      "-> ground subjects: {man->man: {}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> ground actions: {man->man: {stand->stand: set(), collect->collect: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "viral_tweet\n",
      "(#wild(have[hill,cloud]), #accessory(have[leaf]), #wild(have[balloon]), #woman(sit,hold[phone]))\n",
      "A woman is sitting in a hot air balloon.\n",
      "\n",
      "[woman, sitting, hot, air, balloon]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {sit->sit: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Forming_ideas\n",
      "(#background, #man(hold[cube],walk), #other(have[triangle,circle,square,tree]))\n",
      "A man walks past some objects.\n",
      "\n",
      "[man, walks, past, objects]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {walk->walk: set()}}\n",
      "-> left ungrounded tokens: [past, objects]\n",
      " -------------------------- \n",
      "voice_control\n",
      "(#background, #woman(sit_on[chair]))\n",
      "A woman is sitting at her desk.\n",
      "\n",
      "[woman, sitting, desk]\n",
      "-> ground subjects: {woman->woman: {}, desk->office: {}}\n",
      "-> ground actions: {woman->woman: {sit->sit: set()}, desk->office: {}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Friendship\n",
      "(#background, #wild(have[tree,plant]), #man(lie_on[stone],talk), #man(sit,talk))\n",
      "A man talks with his friend under the tree.\n",
      "\n",
      "[man, talks, friend, tree]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {talk->talk: set()}}\n",
      "-> left ungrounded tokens: [friend]\n",
      " -------------------------- \n",
      "Forever\n",
      "(#background, #wild(have[plant,beach,sun]), #group(have[man(sit,hold),woman(lie,talk)]))\n",
      "A couple were lying on the beach sunning themselves.\n",
      "\n",
      "[couple, lying, beach, sunning]\n",
      "-> ground subjects: {}\n",
      "-> ground actions: {->woman: {lie->lie: set()}}\n",
      "[sunning]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjustin/miniconda2/envs/text2scene/lib/python3.7/site-packages/ipykernel_launcher.py:453: UserWarning: Verbs not exhausted!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> left ungrounded tokens: [couple]\n",
      " -------------------------- \n",
      "wind_turbine\n",
      "(#background, #other(have[windmill]), #woman(walk,watch))\n",
      "A woman is walking in front of three windmills.\n",
      "\n",
      "[woman, walking, windmills]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {walk->walk: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Folder\n",
      "(#chart(have[folder,paper]), #man(stand))\n",
      "A man stands next to the folder.\n",
      "\n",
      "[man, stands, folder]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {stand->stand: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "track_and_field\n",
      "(#background, #woman(exercise,run_over[hurdle]))\n",
      "A woman is running over hurdles.\n",
      "\n",
      "[woman, running, hurdles]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {run->run: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "virtual_reality\n",
      "(#background, #man(lie,watch[movie]))\n",
      "A man is lying watching a movie.\n",
      "\n",
      "[man, lying, watching, movie]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {lie->lie: set(), watch->watch: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "For_sale\n",
      "(#home(have[house,sun,tree]),)\n",
      "A house with some tree.\n",
      "\n",
      "[house, tree]\n",
      "-> ground subjects: {house->home: {}}\n",
      "-> ground actions: {house->home: {}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "timeline\n",
      "(#wild(have[plant]), #man(walk,hold[paper]), #chart(have[paper,arrow]))\n",
      "A man is walking with some paper in his hand.\n",
      "\n",
      "[man, walking, paper, hand]\n",
      "-> ground subjects: {man->man: {}}\n",
      "-> ground actions: {man->man: {walk->walk: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "voice_interface\n",
      "(#background, #park(have[plant]), #woman(sit,play[computer]))\n",
      "A woman sits using a computer.\n",
      "\n",
      "[woman, sits, computer]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {sit->sit: set()}}\n",
      "-> left ungrounded tokens: []\n",
      " -------------------------- \n",
      "Frozen\n",
      "(#alien(have[snowman]),)\n",
      "There is a snowman.\n",
      "\n",
      "[snowman]\n",
      "-> ground subjects: {}\n",
      "-> ground actions: {}\n",
      "-> left ungrounded tokens: []\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "for txt_name in glob.glob('text/*.txt'):\n",
    "    print(' -------------------------- ')\n",
    "    name = re.findall(r'text/(\\w+).txt', txt_name)[0]\n",
    "    print(name)\n",
    "    # name = 'wind_turbine'\n",
    "    print(Picture('images/%s.svg' % name).layers_)\n",
    "    ground('text/%s.txt' % name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#background, #woman(sit_on[chair]))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools.containers import Picture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{run(act): set(),\n",
       " sport(act): set(),\n",
       " watch(act): set(),\n",
       " have(act): {head_shot(obj)},\n",
       " stand(act): set(),\n",
       " hold(act): {flag(obj), flower(obj), pencil(obj), phone(obj)},\n",
       " sit(act): set(),\n",
       " play(act): {computer(obj), phone(obj)},\n",
       " work(act): set(),\n",
       " write(act): set(),\n",
       " bend(act): set(),\n",
       " present(act): set(),\n",
       " back_on(act): set(),\n",
       " touch(act): set(),\n",
       " walk(act): set(),\n",
       " talk(act): set(),\n",
       " show(act): set(),\n",
       " point_to(act): set(),\n",
       " take(act): {pictures(obj)},\n",
       " lie(act): set(),\n",
       " sit_on(act): {chair(obj)},\n",
       " exercise(act): set(),\n",
       " run_over(act): {hurdle(obj)}}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layerbase.collocations_[Node('woman', 'subj')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(layerbase.collocations_[Node('wild', attr='subj')][Node('have', attr='act')])[-3].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{man(subj): {sit(act): set(), hold(act): set()},\n",
       " woman(subj): {lie(act): set(), talk(act): set()}}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layerbase.pic_vocab_)[11].layers_[-1].nested_entities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  weather\n",
    "#  Text: A man is standing to give the weather report.\n",
    "#  Trouble: weather -> wind but should \"weathe report\" -> \"bulletin\"\n",
    "\n",
    "#  videographer\n",
    "#  Text: A man is standing against the camera.\n",
    "#  Trouble: \"camera\" is bound to phone, because they are similar. No way it can be bound to \"camera\" in other because bindind to subjects in the list is prior\n",
    "\n",
    "#  transfer_money\n",
    "#  Text: A man is standing and a woman is sitting with a computer to transfer money.\n",
    "#  Trouble: \"money\" is syntactically connected to transfer, thus can't be further grounded into charts\n",
    "\n",
    "#  track and field: A woman is running over hurdles.\n",
    "#  hurdle isn't captured by woman. Extend the object capture to any subject\n",
    "\n",
    "#  A man talks with his friend under the tree.\n",
    "#   \"friend\" is not bound but is important. similar cases include couple.\n",
    "\n",
    "# For_sale.txt [resolved]\n",
    "#    syntactically bound surrounding subjects have no action\n",
    "#    thus add default action \n",
    "\n",
    "# Follow_me_drone\n",
    "#    \"woman watch drone\" rather than \"woman watch\" and \"other drone\"\n",
    "#    this is a syntactical grounding case, thus nothing can be done\n",
    "#    may require the difference between strong interaction and weak interaction. E.g. play and watch\n",
    "#    need special care when compare the similarity. keywords in a bag may be suitable in this case. Previously we propose layerwise retrieval, now picture level? The slicing is already done anyway. But then we cannot ensure, e.g. two men in a group is correctly bound, duplicate subjects information is lost after bagging.\n",
    "\n",
    "\n",
    "# text/Focus.txt [resolved]\n",
    "#    tree -> home rather than tree -> wild\n",
    "#    add a new ordering key in find_most_simi_subjs, the frequency of this object under this subject \n",
    "#    similarly, add the frequency key in get_simi_key\n",
    "#        in fact, this is the frequency prior\n",
    "\n",
    "# text/Flowers.txt\n",
    "#    grass -> leaf but should grass -> plant\n",
    "\n",
    "# 'text/Firmware.txt [resolved]\n",
    "#    robot can not be grounded to alien, because it's alien have robot, which is an oject\n",
    "#    -> ground to any subjects which have a \"have\" action, not just surrounding subjects\n",
    "\n",
    "# wild should be separate from man and boy? [resolved]\n",
    "#    generally if both are characters then in a layer group? surrounding cannot be in a group technically\n",
    "\n",
    "# chair should be with sit on?\n",
    "# but sofa should not be with lie on\n",
    "# what to do\n",
    "\n",
    "# in alien, lie is not bound to any keyword\n",
    "# because lie isn't in the collocations of alien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'A man and a boy are playing computer and sitting on the ground in the wild. A woman is waving under the tree. A girl is sitting on the chair. A man is lying on the sofa playing games. There are some trees and plants and flowers in the park. An alien is lying crying.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seemly that we don't need a second step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.instance import Node\n",
    "from tools.containers import Picture, Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function tools.image_process.LayerName.collapse_subj.<locals>.<lambda>()>,\n",
       "            {'background': {},\n",
       "             'accessory': {'have': {'leaf', 'plant'}},\n",
       "             'other': {'have': {'bulletin',\n",
       "               'camera',\n",
       "               'circle',\n",
       "               'cloud',\n",
       "               'drone',\n",
       "               'gamepad',\n",
       "               'plant',\n",
       "               'square',\n",
       "               'sun',\n",
       "               'tree',\n",
       "               'triangle',\n",
       "               'webpage',\n",
       "               'website',\n",
       "               'windmill'}},\n",
       "             'man': {'point_to': set(),\n",
       "              'stand': set(),\n",
       "              'sit': set(),\n",
       "              'hold': {'cube', 'fishing_rod', 'paper', 'phone'},\n",
       "              'fishing': set(),\n",
       "              'drink': {'beer'},\n",
       "              'raise': {'arm'},\n",
       "              'eat': {'chips'},\n",
       "              'touch': set(),\n",
       "              'play': {'computer'},\n",
       "              'think': set(),\n",
       "              'walk': set(),\n",
       "              'talk': set(),\n",
       "              'lie_on': {'stone'},\n",
       "              'collect': {'data'},\n",
       "              'step_on': {'stone'},\n",
       "              'exercise': set(),\n",
       "              'work': set(),\n",
       "              'watch': {'movie'},\n",
       "              'lie': set()},\n",
       "             'office': {'have': {'phone'}},\n",
       "             'wild': {'have': {'balloon',\n",
       "               'beach',\n",
       "               'boat',\n",
       "               'cloud',\n",
       "               'firework',\n",
       "               'fish',\n",
       "               'fish_tank',\n",
       "               'guideboard',\n",
       "               'hill',\n",
       "               'lake',\n",
       "               'leaf',\n",
       "               'plant',\n",
       "               'rock',\n",
       "               'sign',\n",
       "               'sun',\n",
       "               'sunflower',\n",
       "               'tree',\n",
       "               'water',\n",
       "               'wind'}},\n",
       "             'woman': {'sport': set(),\n",
       "              'run': set(),\n",
       "              'watch': set(),\n",
       "              'have': {'head_shot'},\n",
       "              'stand': set(),\n",
       "              'hold': {'flag', 'flower', 'pencil', 'phone'},\n",
       "              'play': {'computer', 'phone'},\n",
       "              'sit': set(),\n",
       "              'work': set(),\n",
       "              'write': set(),\n",
       "              'bend': set(),\n",
       "              'present': set(),\n",
       "              'back_on': set(),\n",
       "              'touch': set(),\n",
       "              'walk': set(),\n",
       "              'talk': set(),\n",
       "              'point_to': set(),\n",
       "              'show': set(),\n",
       "              'take': {'pictures'},\n",
       "              'lie': set(),\n",
       "              'sit_on': {'chair'},\n",
       "              'run_over': {'hurdle'},\n",
       "              'exercise': set()},\n",
       "             'chart': {'have': {'arrow', 'folder', 'list', 'money', 'paper'}},\n",
       "             'home': {'have': {'apple',\n",
       "               'bookcase',\n",
       "               'chips',\n",
       "               'house',\n",
       "               'sofa',\n",
       "               'sun',\n",
       "               'toy',\n",
       "               'tree',\n",
       "               'wall_clock'}},\n",
       "             'alien': {'stand': set(), 'have': {'robot', 'snowman'}},\n",
       "             'park': {'have': {'plant'}}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layerbase.layer_merge_.nested_entities_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{man1(subj): {sit(act): set(),\n",
       "  drink(act): {beer(obj)},\n",
       "  raise(act): {arm(obj)}},\n",
       " man2(subj): {sit(act): set(), raise(act): {arm(obj)}, eat(act): {chips(obj)}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### group layer exclusion check\n",
    "sorted(list(layerbase.layer_vocab_))[12].nested_entities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subj': {man(subj): 6}, 'act': {sit(act): 6, drink(act): 3, raise(act): 6, eat(act): 3}, 'obj': {beer(obj): 3, arm(obj): 6, chips(obj): 3}}\n",
      "{'subj': {man(subj): 6}, 'act': {sit(act): 6, drink(act): 3, raise(act): 6, eat(act): 3}, 'obj': {beer(obj): 3, arm(obj): 6, chips(obj): 3}}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAABbCAYAAAAC9lxGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAbI0lEQVR4nO2deZgcVbmH32+6e2aSzEwmIQsR4QJKFLIACYsBIktAQFHksiiFF7mIuLAKXh4UpFJwFRU3RC8QBFGxRAjLjWBYZVVBMCwhoCiyXkI2ksxktvTMfPePc2q6ptOz9KR7pqf7vM+TJ9PV1bV0V/3qO+d85/uJquJwOByVSNVIH4DD4XCMFE4AHQ5HxeIE0OFwVCxOAB0OR8XiBNDhcFQsTgAdDkfF4gTQ4XBULE4AHUVFAvmZBLKb/fvrWe+NkUAekUAS9vUOEsh9EshLEsiLEsiOdvnNEsguw33sjvLHCaCjqKivp6mvL9qXX896+1TgdvW1y77+JXCF+rorsA+w2i6/Grig6AfrqDiSI30AjvJBAhkH3AK8F0gAlwFfAr4KHAeMkUCeBVaorycBJwGe/exuQFJ9vR9Afd0U2/RjwI0SSFJ97Ryu83GUPy4CdBSSI4C31dfd1deZwD3RG+rrhUCb+rqH+nqSBFIN7Ky+vmZXmQ5skEBul0CekUCuiJrG6ms38E9g92E9G0fZ4wTQUUiWA4dKIN+RQOarrxv7WXcSsCH2OgnMx0SLewM7A6fE3l8NvKewh+uodJwAOgqCXCgH0M2xwBcwQni5BHJJPx9pA2pjr98CnlFf/2WbuXcCc2Lv19rPOBwFwwmgY6sQkYR8Tg4jzf2kuQwjXK8A36O3gAGkJZAUgPq6HkhIIJEIPgVMkEAm29eHAC/GPjsdWFGs83BUJk4AHUNCRKpEZDwwhWoOppoUKRLAGOA24CLgv7M+tgh4XgL5tX19H3AAgB0J/irwoASyHBDgOgAJZCqm/3Blsc/LUVmIqwfoyAcRqQLqgHEYkWrla8yghvuBFJAGFqivfx5wW4HsCZynvv7HAOt9BWhSX6/f6hNwOGK4CNAxKMRQD0zFCGA7sFpVN+i39I/AAuASBil+AOrrM8BD0WhvP2wAfjH0o3c4cuMiQEe/iIhgor06zAOzHWhSdfl4jtGPS4R25MQK31iM8CWADozwpUf0wByOAuIiQMcWiMhYoB4jfJsxwrd5ZI/K4Sg8LgJ09CAiYzDCl8QMZmxQ1Y6RPSqHo3i4CNCBiNRihC8FdGIivvaRPSqHo/g4AaxgRKQGI3zVGOFrVlU328JRMTgBrEBEpBojfDVAF9AMtKm7GBwVhusDrCBEJIURvlqgG9gItDrhc1QqTgArABFJYoRvDEb4moAWJ3yOSscJYBkjIgmM8I0FFNPUbVHV7hE9MIejRHACWIZY4avDCB/AJmCTEz6HozduLnAZYSu0NABTMNPXWjHzdZuc+DmGnVBuJJTjcix/D6Es3ortLiaUne3f9xDKc4SyglCuIbTzykP5HqEcMtCmnACWAVb44oUK2oBVqrpRtcdwqKSIu8UN4bPnSiAnx16fJYH8XQJZIYF81y6bJYHcWKDDdRQST9/G0y2FcTCEMgNI4Om/7JIT8HR3YCYwGTjeLr8KuHCgzbkm8CgmR6GCNkwuX0kUKpBABBDr6dEL9fW0IW4ziXGTm2NfHwwcDcxWXzskkCl2+8slkPdKIDuor28M+SQcgyeUkzE1HRV4HpNi9WFCOQ/YFrgATxcTyo7AXXg6k1BOAY7BpGTtBIR4GhDmMNjy9LcYI63/7dmnp032ryQmn1Xt8tcJZRtC2RZP3+nrkJ0AjkJihQrqyVRoaS6FQgXWy3cp8BAwD3hWApmFGYFerL76dr2HMTfLM8D1wF6Yi/cG9fWHEsj7gJ9inuqtwOfV179hKkUvi7nDfQn4tvpmyp76GllpAvwO+DTw3WKdr8NiIrOLgP3xdC2hTAR+AEzDFL39ILAEyNX03QcTwbUCTxHK3cC/AW/j6cfs9sfbdfcHfpO173vtNpZmbX+ZXf+2vg7bNYFHGbZQwRRgPGa+7lpVfbcUxC/GB4Bfqq97Auerr3sBs4EDJZDZWevuAWynvs5UX2cBP7fLFwFnqa9zMUL5P3b5/sBfY5+fDsyXQJ60Jut7x957GmO05Cg+hwCL8XQtAJ6+a5ffiafdePoiposmF/fj6To8bQNuxwjmcuBQQvkOoczH6zHYmgas6fVpTw+3y2vscUQMaKTlBHCUICJjRGQK0IhpWqxT1XUlWqXldfX1Cfv3CRLIMkykNwPI7vf7F7CzBHKVBHIE0CSB1AH7AbdaH+FrMRc4bHkDJIEJwIeA/wJusU1vcE5yw4kQNT9705G1Ti6yP6d4+jIwF2uwRdhjsJVtpmXwtB0TYR4dWzqgkZZrApc4WYUK0sC7pVioQM6Ug2hkP1K8BLQASCA7YW0u1df1dlCi18Vrl+8OHA6cAZwAnAtsUF/3yLGrXG5yt6uvCvxFAunGWG6uwTnJFR0RkdVXs//EOnZKVHEsofwQT9fZJvBgOcyu3wZ8EjiVUN4DvIunNxHKJjIWqS8B7wdeI5Q6oB5PVxJKEvgo8Fhsu9OBW/vbsYsASxQRqRGRycBEzJNzvaquKSXxE5GEiNTJifJR0iyliUvpIER78g8bMGK40RobHbnFNgKZBFSpr7cB3wDmqK9NwKsSyPF2HbEiCZkbIOJObLNHApmO6Qhfa9+bDrxQyHOuZEQkKSK1IlIvIhNEZMpvz+KozZ3cn+7iLEwT9y+E8hym/2+wPA78CngWuA1PnwZm2W09S2+DrbuBg+zf44AlhPI88Bwm4r8GgFBSmOvk6f527CLAEsMWKmjA3MhdmJp8rSN7VBmsKdIY+68agG2Yx1hSQII0SpJ6EZkKvMolPEsVKzBN3T/m2OR2wM8lkOhh/DX7/0nA1RLIxZjo92bMRb4Uc7NE3ADcIIG8gCne+lkbDQIcjLlhHHlgE+mTmO89/n+8CdsJdB7wAeY0jiNVnSCBacr+DE8vz7lhT+vs/69hBj0iVuPpmVnr3gvcm2Mri4GHCMXH01XA3jnWATgK0yfZb0aEqwZTIthCBQ2YjtxuzLS1kihUYEWvFiN6NXZxJ6bJ0sZC9gYeBFIoad7lSK5iuV1X4usWIkVHArkDuEB9/Uc/69QAjwAHxEaMHTHs75pL6OItwy7M75eO/99zXYYyj+i3t46AeIMzxbKfPwXYawsB7P8zhwMv4fWT3hTK8ZjBlQ39bcoJ4AhjCxU0kKnQsokSKFRgU23ioieYm6ENI8y9REUCmYdpmjwcucL1I5ytGDEcUpK2BPIBYKr6+mg/6+yCGV1+eCj7KCfsb5lL6OJufN3kFrqBZxCFmd8+L/ErAZwAjhC2mdGAEQclI3wjNmXN3ig19phqyYheO0awhjzinLPpbJqsbUB7qc5YGW3YB2q20MW7upTcQleR378TwGEmVqhgHOZibGGECxXYytCR6FVhooFI9AruCWK/g0gMU3ZxBxkxdPOWB8B+h7mELrufLlvoXHdADCeAw4SNgCLhA9MM3DRST1472BKJUBVGjNsxItQxXE1wG7FExxFFKtFxtI90V8BIY6+bbKFL0Vvo4v10keB1Vvp3NxicABYZewFH83UFI3zNIyF8dqAlEpto1C4eeY10v2Ou4xt2UR4JbPdDLqGLD0gMvZ/OkRMngEWiVAoV9BFhlXxzs48Itc3+2zxaxTA2IJEtdPEBiVz9dOlS/a1GM04AC0wpFCroo48tGnBoG203Uh99lNG5lOJUQKDn4ZNrUCJOLqGryAGJkcAJYIGwwhcZiycwUVbzcN2gfYyypskIxai/qfoZpY7OcUQKQgwycbiLLKEDukZrJFsuOAEsACISCV8SE2k1F2P0NMd++01QLucRvz7yFIt67oNMHO4mq48OE9W5G60EcQK4FdhCBQ2YmyCNEb6iztUdIEF5xKKgkaSPB8GQo99BJg4ruYVuVHUvVDpOAIeA7ZNqwNwYnRjhK1rVkT6afqOiH2y4yTfhOo/E4WyhG/VdCg4ngHmRo1BBczELFQx3gnK5kTUYVIMRuChnrhMT0W0xwZ/eQle23QgOVw1mUGQVKugCNlKkQgWx9I9aKiwXrhD0kTgcDUgkyeRjdmJm4TTbfy5xuAJxEWA/DFehglJPUC4WEsi5wCL1TRRtKzk/iCmKuRl4FPPQSdLbT+RmuvgGl/E6Q0gcxnzHFZlwXTTiRkeF3/Yngdl4eimhfBFTOLcLcz+ejqcvEsos4Hw8PSWfTbuCqDmwhR8nYLw3ajARwipV3VSom8Puo96WuZ+MiUw6gQ12X++qaluZ34znkjFvB1PR9zlbELUDOISF7MGfmUs3R8qZcoiITOQtbibNQkzl50a7jSjnsglYh/kO31HVtdYetEVVN6tqt6qmrVfyKkzx1FbM7zwR2FZEGm33g2MkMVWeAS4g4wkT4uksPN0DY3ZlCq96uhx4L6HskM8uXBM4hu0zqsfcUFGFloIVKugnQXkjozBBORcSyGeAszH9pE8CXwZ+gilc2eMMJ4GcjfHreEgCWau+HozyGVr5ufU4jiK6Bl6hlr0ZQ5OdzfJbHuU8fsQMNrKCjq3pp7MDSJuBjbE+1zHAWBFxA035kSSUXwB7Ai8DJwO7YkSqDvOwOcWWsN/C9Q9P/0YoNwLv2m0sI5RrgY6Y2VJTbH9RQZGIvF0AXQRIj7H4eEzENwbTN7TKRglbJUp22+NEZBKmZHiDfavJ7mOtjU7KQfx2BT4F7G/9PLowlZ0v6uUMd7HszkKuRVnJkxzNQo4XkWko81nKq5iHUDUT6eYS7uEknke4h1/xe1VdrU36LsI/OJ4PFnKQQlU7VHUD8A7mJuzAPAwnichUEWmw3RWO3HwAWISnszHX9xkYg/Lj8HQupnr3N+26i4Cz7PK46x8YK4ND8fR8jAvgsl57CeUMQnkFI3Rnx97J2wWwoiPArAotBStUEJsVkp2g3Ex5JygvwDh5PSWBgDKGbtaR5jPiy6koKYSpbGA/YA1KFc3UYgeVEMbzAq/TO3F4tgTSSII7WMgMMh4fkePbXykwdt/tQHtW3mUdUCciFZFsPgTexNPI9uAm4OuY0vf3EwqY/taV1sxoP+BWuxwy9wnArXg992AuG8yfAj8lFA+4GPisfSdvF8CKFEB7UUfCV5BCBf0kKG+izBOURUQ4gw8zniOBh/kWZxJN8P8Y2zOHm1nKkTzFWi7ix9QZW0+ELg5ltT6m6wAkkE4W0hnz9ABAfd1gjdSPICOAw+L4ZsWwDWjLSriuB+pFpKymG+aLiMjbP+GAKeM5KlFFdnTcDKzA03m9lobSAGyw/Xi5aIn93YbxwM7FzcDVsdd5XxMV1QQWQx2mKVqP6ftZo6rrhyJ+dnu1dsBkW4w/bTUmklyrqlEzumzEL5czGMfwcdLcQwuHk+TTnMZHgM0cTYJdaaCKJj7GyywkQYoFjGGzqnYgNGN+h4i/AzsDSCCTJZBG+/cY4FDgb7F1pwMrhuesDXYApVVV1wGrMJGrYro1porIJNvdUXb3lb3WU2L8qRtEZKKITP3tWRzVrdzX2cX5wLaE8nn7kROBJ4DJtmS+cWoLZYbtx3vV+nZAKELY4/qXTW8XwFB2ib33MSDuC5O3C2BFRICxCi11FKBQwQDVScoiQXmQE/xN4vBO7EmKFCkSCF28lx+wkLWYtJMzMKbouZzhFgFLJZCV6mvk4HYQ8E9M0+cXEkgC8x3for7eBWAtNtvU15VFOv0BsdFeC9CSNbg1HhgvIiVfcqwv8qhis3n+B5nTOJZUKtEzTfA0QjkLI0xXYZzdfkwo4+02foS5Fk4CribcwvUvm0eB7xOK4KkCZxLKoZhraz2Z5i8MwQWw7PMARSQqTZXARHxNQxG+ck1QHuQE/36dwawhUi9nsMgYKa9jCWQa8Ev19bAB1vsK0KS+Xp/vPopNjvqLJZvTmWcVm/hvn6lis7WucIMhlCuB3+HpA/2s0+MCOJAVZpyyFcCsCi1pjPDlFZ2Npot5IAY5wX/IFYdzucIN6TgDOQG4x+YC9rXOfwK/KnW7y1KpcJ1nFZuekvoMtopNsV3hQpkK7IunS/pZZxdgO7z8XADLTgBthZZ6MoUKmvKp0BJrzoxlFFVQjrBC15dhToRzBhtmclS4Lvi87jyr2GQLXUlf18WibATQ9suZ/DETtjcNtkKLFb1odC+7gkjJJiiLcwYbdViRiovhkCr7uCo2hWHUC6B9staTKVSwiUEUKih0Dbli0scE/1zOYNlzXt0E/xJmMBWu83jIuSo2Q2DUCqDtX6knj0IFWbl6tXZxySS12uMbjDOYqzhcZtiHXL39F2UrCKb7pR0jjP0ORjnyZ9QJoA396zEi1k3GWDzniQzmKTscx53jmAbjDOYqDpcZg3zIKZm+uwSZh3QrJdY6Ge2MGgHMUaggEr4tBKFQ/SwFOu6BcqpcX00ZkjUYlY/9Za/BqNHaPz1aKHkBtBdAHZmySdF83VzCV/SRtgGOc3CJw33lVDlGJbF+unzsL/MejLIP01pGaYZCKVKyAjjYQgXDnWtViMRhx+gkNhiVj/1lUQajrBiOpcKK6BaakhPAWKGCqHT5FoUKhiNBudiJw47SJc/E4REfjCrXWUrDQckIoBWccRjh66nuGwmf5C4mWpDwfwg5VS5xuAzIMRg16u0v+5in7oy0+mDEBTBWqKAe84N1YIQvXegO4CHkVLnE4TIhazCq7BOH+8h+cBWusxhRAcxVqABz4W1VgnIeicO9hA7XTzfqyRqMconD9FurcsRSwUqFERHAXIUKMNFf/wnKEnMNU23aKNJYB9cLzFRgGZy9Dzy3BvyX4Q/7m9JLrp9uhJBAFgKb1NfvZS3/ItCqvv6yn8+eAuylvp6Z471z6WY9l/Ibvs6lJDkKRelmHU/wFR5kFV/gUBqYzRVcjhuM6sGKYa5q5SUxGaCHUKYB1+HpUfb1bOBaTO3FbmBvPG0nlAeA4/F0/VB2M6yFG20hzcmYwqFgcvk6MW5cEzBP7E2YIqWrVbVZVTujYoxr4ZMt8JJAUkSmAotegz8l4OB94LCbTR259uXw433hdOAd7cMZbDjP25FBAkmqr9f0J3691s8U4hwrIg0yVabQxelcxR+ASdzFDVzKoVzGwaT5PfM4A1jLY9zEOBawkDZblLZVjRtcxYofmArXminq+g6mqGs3JiCZIiKTRaTORtKFJcxrm+cB19nPJTEl9r+IpzMwlWeiqPVXGOOtIVGcgqgiOwL3AI8DH+qE5S/CrW1wYQImPwpf6gaZD5dWQa1A60r43A6qL3SInJqAT6RFxlXBzhthKfAtgGo4+RnzRSQvgJp62GemcZ5K/9U2jb9vjmADIhPVmBy9U5RzdOREArkI85u8ifFy+KstZ/8njMHNEgmkHhsZ2veeRDkYaGQTX+L7/IXzGEc1Y4BpnM4CpnAOD/FZZrMb3TzLetYDaZ5nVdQtYounpqP+Lbvto4BbhvM7GC3YQCBXUdcGoEFE8utvD+VOYHtMK+5KPF1EKJswrnCHA+cTyk1AiClemsIEKpdjqj5fgafX2K0di/H7APgI8DyemoKpnrFQsCwBHiNjtpQXxYwA3/82/HQaHNQJM6fBiY3wqRUQfAjOuRdePAwOq4F5r8K3JsHlIjLtDfPl73k6nHk4HDwejl5iquyur4c5u8F9qrr6OzCxCla/CdcoPI3IzxAZF9v/MswN5xgmJJC5GFvCPYF/x1hhRjSqrweqr9+nGyFNUkTq6CJJG/UEHM1KLqUWH6iniyqq6OZ8DmRbvsBajtDHdQVTmUmKP6vxaO5Q1S4J5JsSyJuYKsOXxPaZt0tYpaKqXfY7XYMxF2rG6MN4jFfyNjYK708zTrUub3sBZxPKNpjMjhfwdF88fdyu96b1CXkMuBE4DvgQcCkAoewErMfrGbWeDiih3Esoywjlgp49mqZvjd1X3hRFAF+HZCe8sR20vAPTmuCVN+GPHdD9BPw9BdtvB5OXwm864Omd4NvVxlKvdbJJeH7gRnj5AdU3EvDCx2EbVW0TmDhRtdnuJgnMAa5GdU/Mk+zC2GHk7RDl2GrmA3eor63qaxPKEtIk6KaKNdwtxjNjGq3U0cZYoAFF2MASoIV2HiPJe4CVTKCZavajnnOo4qN6tfWFzeESpr5epL5uD/waiPcZumtgCKhqp+1+Wo35rjdh7rdGjPfJRDHeIJL10bMJ5TmMF8j2wC6YwZbbstaLCpsuB57E02Y8XQO0E0ojW/7GSeAAzAPuAOAYQlkQe3/Iv3NBBXCZyCH3i1xxFxxrC/E1Al0J6Kw2ArW5HtqroOrLcG4C7quB3VJwZBKqVXVjI2xOmpA76qvpItNU7yTzBHoLeAvVJ+3rxRhBjBgW1zCH7af7qswnzUfYzI42WtiWNsbRxji6SNJEJybVpJUa2hlLC7CSJGmmsVZVm9iZFoRk7Lf/F6Zvanpsd21kBsqyCTFNpwh3DWwltt+0SVVXYYzNWzEpaRMwkeGEV6+UA7tvkkXAMcA8PN0d4wNTC7THLC4josiuO/Z39DrJlr/xW8AjeLoWT1uB31Oge71gfYBpkXlvwl3tUD0D0glYhzGXTiegO5WZIoSCdMOE/zOmJhPXwZcboSopMul1qGuE2vHGSJxWSP0Nxs8RmdQBr/wO5h4n8irQ2Q4r7xXZ92h4ZS18PAGvTsh8buZzcN88+9pRRE5kb1q4nU5SNFDFKSzmHyxjDAvo5DqSbOZ9rLMd70ggPXOgJcgOInrxOsY0+w4J5Hj1dQVZLmESyC7qa+QM9gm2dI7LyyXM0Te2b3UzsDFKuL79XPavTrA43UV1TRUK7E4o6zFN2qHyMrBj7PW9wAWEMtbu/0Dgh4BxlDOOjK8NZUcFiwBTcNAsqN4PErMgWW2eEhuAlqSJAnuNvj0PV+0IF3fA3TLI42iC+/eN9ev9Cb52JFy7GR4ZCzO/a7+UuZBMwU6nwbOFOj9HP2zDfowhRQ0JEig7cj2HcQPCo6Tool+N6x/19e+Yps+tEsj7MINiH46t8m0J5AUJ5HlMZ/k5sffydglzDA7b/7rhE3OZPbGOVHWSKsx9fDtwGaYZPDQ8bQFeIZT329frMQMpT2Hu6WV4Gv2uc4En8jFCilO4PEDJ4Q6lBTZIEeMahvbvGobIMcAcVL9R0P07clIoV7g89ncHcEEs8su1zlQgVF8X9LWOowAUyxUulGOAuXh68QDrXQkswdMHh7KbwvUBGrFbgBmFK7z4mX2sBK5DpGGANZP0ZMQ4io0Vu57fvpjiZ7kQ01HeHzsA5xf5OBxe1n1fKFc4T+9gcM3aF4YqflACc4EdDodjpBjWmSAOh8NRSjgBdDgcFYsTQIfDUbE4AXQ4HBWLE0CHw1GxOAF0OBwVy/8DmoHigpdCRAwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x96 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### absorption check\n",
    "from tools.containers import LayerName\n",
    "from tools.image_process import getLayerNames\n",
    "layer = LayerName(getLayerNames('images/Game_day.svg')[-1])#\n",
    "layer_c = LayerName()\n",
    "# layer_c.triples_\n",
    "# print(layer_c.entities_['subj'])\n",
    "layer_c.absorb(layer)\n",
    "layer_c.absorb(layer)\n",
    "layer_c.absorb(layer)\n",
    "layer_c.triples_\n",
    "# print(layer)\n",
    "# print(layer.entities_)\n",
    "# layer.collapse_subj()\n",
    "# print(layer.entities_)\n",
    "# layer.print_()\n",
    "# layer_c.plot()\n",
    "print(layer_c.entities_) # plot()\n",
    "print(layer_c._get_entities())\n",
    "layer_c.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2scene",
   "language": "python",
   "name": "text2scene"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
