{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "from scipy import sparse\n",
    "\n",
    "from tools.text_process import TfidfEncoder\n",
    "from tools.image_process import getLayerNames, CategEncoder\n",
    "from tools.joint_process import SimiEncoder\n",
    "\n",
    "# random image generator, subject to rules\n",
    "from tools.generator import ranGenLayer\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, img_dir='images', txt_dir='text'):\n",
    "        self.img_dir = img_dir\n",
    "        self.txt_dir = txt_dir\n",
    "        \n",
    "        # fitting the vectorizer will process all the text\n",
    "        # so we have double processed the text here\n",
    "        self.img_encoder = CategEncoder()\n",
    "        self.txt_encoder = TfidfEncoder()\n",
    "        self.joint_encoder = SimiEncoder(self.img_encoder,\n",
    "                                         self.txt_encoder)\n",
    "        \n",
    "        # set features\n",
    "        self.features_ = []\n",
    "        self.features_.extend(self.txt_encoder.vocab_)\n",
    "        self.features_.extend(self.img_encoder.features_)\n",
    "        self.features_.extend(self.joint_encoder.features_)\n",
    "      \n",
    "    def getOneLayerSent(self, txt_name=None, img_name=None,\n",
    "                              ran_txt=False, ran_img=False,\n",
    "                              fake_img=False):\n",
    "        ##### preprocess\n",
    "        ## text\n",
    "        if ran_txt:\n",
    "            all_txt = glob.glob(self.txt_dir+'/*.txt')\n",
    "            # rule out current text\n",
    "            all_txt.remove(txt_name)\n",
    "            txt_name = random.choice(all_txt)\n",
    "        else:\n",
    "            assert(txt_name)\n",
    "            \n",
    "        with open(txt_name, 'r') as f:\n",
    "            sent = f.read()\n",
    "            \n",
    "        ## image\n",
    "        if ran_img:\n",
    "            assert img_name\n",
    "            assert not fake_img\n",
    "            all_img = glob.glob(self.img_dir+'/*.svg')\n",
    "            all_img.remove(img_name)\n",
    "            img_name = random.choice(all_img)\n",
    "            layers = getLayerNames(img_name)\n",
    "        elif fake_img:\n",
    "            assert img_name is None\n",
    "            assert not ran_img\n",
    "            layers = ranGenLayer()\n",
    "        else:\n",
    "            layers = getLayerNames(img_name)\n",
    "            \n",
    "        return layers, sent\n",
    "        \n",
    "    def __getEmbed(self, **kwargs):\n",
    "        \n",
    "        layers, sent = self.getOneLayerSent(**kwargs)\n",
    "        \n",
    "        # tofeature\n",
    "        txt_embed = self.txt_encoder.encode(sent)\n",
    "        img_embed = self.img_encoder.encode(layers)\n",
    "        joint_embed = self.joint_encoder.encode(layers, sent)\n",
    "        \n",
    "        return np.hstack([txt_embed, img_embed, joint_embed])        \n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        img_name = '%s/%i.svg' % (self.img_dir, ind+1)\n",
    "        txt_name = '%s/%i.txt' % (self.txt_dir, ind+1)\n",
    "        \n",
    "        # triplets\n",
    "        triplets = []\n",
    "        # true match\n",
    "        triplets.append(self.__getEmbed(txt_name=txt_name,\n",
    "                                        img_name=img_name))\n",
    "        # fake image\n",
    "        triplets.append(self.__getEmbed(txt_name=txt_name,\n",
    "                                        fake_img=True))\n",
    "        # mismatched text\n",
    "        triplets.append(self.__getEmbed(img_name=img_name,\n",
    "                                        txt_name=txt_name,\n",
    "                                        ran_txt=True))\n",
    "        # mismatched image\n",
    "        triplets.append(self.__getEmbed(img_name=img_name,\n",
    "                                        txt_name=txt_name,\n",
    "                                        ran_img=True))\n",
    "        \n",
    "        xs = np.vstack(triplets)\n",
    "        \n",
    "        # ys\n",
    "        ys = np.array([1,0,0,0]).reshape(-1,1)\n",
    "        \n",
    "        return sparse.csr_matrix(np.hstack([xs, ys]))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(glob.glob(self.img_dir+'/*.svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features:  6398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:48: UserWarning: cell with tag r does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n",
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:48: UserWarning: something with tag n does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n",
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:48: UserWarning: sideways with tag n does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n",
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:48: UserWarning: grid with tag a does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (168, 6399)\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "print('# features: ', len(dataset.features_))\n",
    "data = sparse.vstack([dataset[i] for i in range(len(dataset))])\n",
    "X, y = data[:,:-1], data[:,-1]\n",
    "y = y.toarray().flatten()\n",
    "print('data shape', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2112', 'A311']\n",
      "['indoor.a.01', 'object.n.01', 'appliance.n.02', 'interaction.n.01', 'occupation.n.01']\n",
      "A man looks at his chart with his back.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_S_object.n.01_back.n.01_', 1.845826690498331),\n",
       " ('_S_object.n.01_chart.n.01_', 1.6916760106710724),\n",
       " ('_S_object.n.01_man.n.01_', 1.845826690498331),\n",
       " ('_S_appliance.n.02_back.n.01_', 1.1526795099383855),\n",
       " ('_S_appliance.n.02_chart.n.01_', 1.072636802264849),\n",
       " ('_S_appliance.n.02_man.n.01_', 1.2396908869280152),\n",
       " ('_P_interaction.n.01_back.n.01_', 1.1526795099383855),\n",
       " ('_P_interaction.n.01_chart.n.01_', 1.4403615823901665),\n",
       " ('_P_interaction.n.01_man.n.01_', 1.1526795099383855),\n",
       " ('_P_occupation.n.01_back.n.01_', 1.1526795099383855),\n",
       " ('_P_occupation.n.01_chart.n.01_', 1.4403615823901665),\n",
       " ('_P_occupation.n.01_man.n.01_', 1.1526795099383855)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers, sentence = dataset.getOneLayerSent(txt_name='text/1.txt',\n",
    "                                       img_name='images/6.svg')\n",
    "print(layers)\n",
    "print(dataset.img_encoder.layer2keyword(layers))\n",
    "print(sentence)\n",
    "list(filter(lambda x: x[1]!=0, zip(dataset.joint_encoder.features_, dataset.joint_encoder.encode(layers, sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train:  151\n",
      "# test:  17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight={0: 0.5, 1: 1}, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(# random_state=0,\n",
    "                         solver='liblinear',\n",
    "                         class_weight={1: 1, 0:0.5},\n",
    "                         penalty='l1', #'l2' use l1 to learn sparsely\n",
    "                         C=1.0,\n",
    "                         max_iter=100)\n",
    "\n",
    "ind_test = int(X.shape[0] * 0.9)\n",
    "print('# train: ', ind_test)\n",
    "print('# test: ', X.shape[0] - ind_test)\n",
    "clf.fit(X[:ind_test], y[:ind_test])\n",
    "# clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7647058823529411\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Accuracy: ', clf.score(X[ind_test:],y[ind_test:]))\n",
    "# print(clf.predict(X))\n",
    "# print(clf.predict_proba(X)[:,1])\n",
    "# print(y)\n",
    "plt.hist(clf.predict_proba(X)[:,1])\n",
    "plt.xlabel('False <-----                -----> True')\n",
    "plt.xlim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_S_art.n.01_woman.n.01_', 1.3202551020799698),\n",
       " ('_P_back.n.01_back.n.01_', 0.8651919722315243),\n",
       " ('chart.n.01', -0.8595115985874148),\n",
       " ('_S_chart.n.01_chart.n.01_', 0.791275465887096),\n",
       " ('_S_art.n.01_art.n.01_', 0.6793111985962742),\n",
       " ('_P_stand.v.01_stand.v.01_', 0.6452050823909851),\n",
       " ('_S_object.n.01_woman.n.01_', -0.6260269484847111),\n",
       " ('_P_sit.v.01_sit.v.01_', 0.4802178830737528),\n",
       " ('stand.v.01', -0.45469933499800347),\n",
       " ('_S_chart.n.01_icon.n.01_', 0.4370358883681366),\n",
       " ('_P_enjoyment.n.02_woman.n.01_', 0.31067535131261936),\n",
       " ('_Decoration_', -0.30605236774055855),\n",
       " ('_S_nature.n.03_rock.n.01_', 0.29474780639501075),\n",
       " ('_S_city.n.01_city.n.01_', 0.24977515821435495),\n",
       " ('_P_stand.v.01_be.v.01_', -0.24918892514834753),\n",
       " ('_S_wall.n.01_window.n.01_', 0.23564653897772816),\n",
       " ('_Person_front_', 0.2238157240478356),\n",
       " ('bucket.n.01', -0.21701389765974885),\n",
       " ('_S_wall.n.01_hospital.n.01_', 0.20562824439570102),\n",
       " ('_S_furniture.n.01_sofa.n.01_', 0.1994414296773402),\n",
       " ('_S_nature.n.03_park.n.01_', 0.19130429932213455),\n",
       " ('stand.v.01 following.s.02', -0.19030975393636523),\n",
       " ('be.v.01 indicate.v.02', -0.18985770913875843),\n",
       " ('_P_show.v.01_show.v.01_', 0.18460089119039963),\n",
       " ('_S_abstraction.n.01_', -0.15720414862915827),\n",
       " ('_P_person.n.02_telephone.n.01_', -0.14337256700770928),\n",
       " ('airport.n.01', -0.1426245542914912),\n",
       " ('_NLayers_', -0.14026213742850627),\n",
       " ('_P_interaction.n.01_', 0.13465025374377168),\n",
       " ('_P_person.n.02_man.n.01_', 0.1198779932112836),\n",
       " ('work.n.01 art.n.01', -0.09936390888420378),\n",
       " ('_S_street.n.01_street.n.01_', 0.09548479254540918),\n",
       " ('earth.n.01', 0.0881065660455014),\n",
       " ('_S_object.n.01_airport.n.01_', 0.07552630979205882),\n",
       " ('_S_city.n.01_woman.n.01_', -0.0730715945406234),\n",
       " ('stand.v.01 earth.n.01', 0.07002904476593581),\n",
       " ('icon.n.01', -0.05959215073830078),\n",
       " ('_P_abstraction.n.01_', -0.057178327519346706),\n",
       " ('work.n.01', -0.037241370227408754),\n",
       " ('_P_lean.v.01_lean.v.01_', 0.034847252774880384),\n",
       " ('heart.n.01', -0.027803590336296365),\n",
       " ('love.v.01', 0.027398497336284983),\n",
       " ('indicate.v.02 chart.n.01', -0.019019636178022065),\n",
       " ('_S_object.n.01_web.n.01_', -0.010724390535425564),\n",
       " ('girl.n.01', -0.008647076737971913),\n",
       " ('woman.n.01 flip.v.06', -0.008616594819151922),\n",
       " ('window.n.01', -0.00860166618981562),\n",
       " ('girl.n.01 be.v.01', -0.0044703120361132675),\n",
       " ('flip.v.06', -0.003061017198679715),\n",
       " ('coffee.n.01 airport.n.01', -0.0026242339458734296),\n",
       " ('art.n.01', -0.002597214523108072),\n",
       " ('heart.n.01 bucket.n.01', -0.0025319858306964128),\n",
       " ('man.n.01 drink.v.01', -0.0016452231273219717),\n",
       " ('indicate.v.02', -0.001641408480296354),\n",
       " ('drink.v.01', -0.0010775251369210983),\n",
       " ('drink.v.01 coffee.n.01', -0.0006011627374822451),\n",
       " ('coffee.n.01', -0.0004330930501149326),\n",
       " ('flip.v.06 heart.n.01', -0.00017476668554071794),\n",
       " ('show.v.01 love.v.01', 0.00010467208116762971),\n",
       " ('airport.n.01 window.n.01', -3.7633645818709248e-06)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dict(zip(dataset.features_, clf.coef_.tolist()[0]))\n",
    "d_filter = filter(lambda x: x[1] != 0, d.items())\n",
    "sorted(d_filter, key=lambda x: abs(x[1]))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo - unseen keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad', 'ae', 'bd', 'be', 'cd', 'ce']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = ['a','b','c']\n",
    "B = ['d','e']\n",
    "[a+b for a in A for b in B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2scene",
   "language": "python",
   "name": "text2scene"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
