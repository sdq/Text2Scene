{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "from scipy import sparse\n",
    "\n",
    "from tools.text_process import LemmaTokenizer\n",
    "from tools.image_process import getLayerNames, image2feature\n",
    "from tools.joint_process import getCrossSimi\n",
    "from models.vectorizer import getVectorizer\n",
    "\n",
    "# random image generator, subject to rules\n",
    "from tools.generator import ranGenLayer\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, img_dir='images', txt_dir='text'):\n",
    "        self.img_dir = img_dir\n",
    "        self.txt_dir = txt_dir\n",
    "        self.tokenizer = LemmaTokenizer()\n",
    "        # this operation will process all the text and train a vectorizer\n",
    "        # so we have double processed the text here\n",
    "        self.vectorizer = getVectorizer()\n",
    "        \n",
    "        # set features\n",
    "        self.__get_featureNames()\n",
    "      \n",
    "    def getOneLayerSent(self, txt_name=None, img_name=None,\n",
    "                              ran_txt=False, ran_img=False,\n",
    "                              fake_img=False):\n",
    "        ##### preprocess\n",
    "        ## text\n",
    "        if ran_txt:\n",
    "            all_txt = glob.glob(self.txt_dir+'/*.txt')\n",
    "            # rule out current text\n",
    "            all_txt.remove(txt_name)\n",
    "            txt_name = random.choice(all_txt)\n",
    "        else:\n",
    "            assert(txt_name)\n",
    "            \n",
    "        with open(txt_name, 'r') as f:\n",
    "            orig_sent = f.read()\n",
    "            sent = self.tokenizer(orig_sent)\n",
    "            \n",
    "        ## image\n",
    "        if ran_img:\n",
    "            assert img_name\n",
    "            assert not fake_img\n",
    "            all_img = glob.glob(self.img_dir+'/*.svg')\n",
    "            all_img.remove(img_name)\n",
    "            img_name = random.choice(all_img)\n",
    "            layers = getLayerNames(img_name)\n",
    "        elif fake_img:\n",
    "            assert img_name is None\n",
    "            assert not ran_img\n",
    "            layers = ranGenLayer()\n",
    "        else:\n",
    "            layers = getLayerNames(img_name)\n",
    "            \n",
    "        return layers, sent\n",
    "    \n",
    "    def __flattenSparse(self, matrix):\n",
    "        return matrix.toarray().flatten()\n",
    "        \n",
    "    def __getEmbed(self, **kwargs):\n",
    "        \n",
    "        layers, sent = self.getOneLayerSent(**kwargs)\n",
    "        \n",
    "        # tofeature\n",
    "        txt_embed = self.vectorizer.transform([sent]).toarray()[0]\n",
    "        img_embed = image2feature(layers)\n",
    "        joint_embed = self.__flattenSparse(getCrossSimi(layers, sent, self.vocab_))\n",
    "        \n",
    "        return np.hstack([txt_embed, img_embed, joint_embed])        \n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        img_name = '%s/%i.svg' % (self.img_dir, ind+1)\n",
    "        txt_name = '%s/%i.txt' % (self.txt_dir, ind+1)\n",
    "        \n",
    "        # triplets\n",
    "        triplets = []\n",
    "        # true match\n",
    "        triplets.append(self.__getEmbed(txt_name=txt_name,\n",
    "                                        img_name=img_name))\n",
    "        # fake image\n",
    "        triplets.append(self.__getEmbed(txt_name=txt_name,\n",
    "                                        fake_img=True))\n",
    "        # mismatched text\n",
    "        triplets.append(self.__getEmbed(img_name=img_name,\n",
    "                                        txt_name=txt_name,\n",
    "                                        ran_txt=True))\n",
    "        # mismatched image\n",
    "        triplets.append(self.__getEmbed(img_name=img_name,\n",
    "                                        txt_name=txt_name,\n",
    "                                        ran_img=True))\n",
    "        \n",
    "        xs = np.vstack(triplets)\n",
    "        \n",
    "        # ys\n",
    "        ys = np.array([1,0,0,0]).reshape(-1,1)\n",
    "        \n",
    "        return sparse.csr_matrix(np.hstack([xs, ys]))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(glob.glob(self.img_dir+'/*.svg'))\n",
    "    \n",
    "    def __get_featureNames(self):\n",
    "        \"\"\"\n",
    "        todo - embed this into image2feature\n",
    "        \"\"\"\n",
    "        self.features_ = []\n",
    "        # text features\n",
    "        self.vocab_, _ = zip(*sorted(self.vectorizer.vocabulary_.items(),\n",
    "                                     key=lambda x:x[::-1]))\n",
    "        self.features_.extend(list(self.vocab_))\n",
    "\n",
    "        # image features\n",
    "        from tools.image_process import getNestedKey\n",
    "        from rules.category import surrouding_dict, person_dict\n",
    "        self.features_.append('_NLayers_')\n",
    "        self.features_.extend(['_Background_',\n",
    "                               '_Surroundings_',\n",
    "                               '_Person_',\n",
    "                               '_Decoration_',\n",
    "                               '_Person_front_',\n",
    "                               '_Surrounding_front_'])\n",
    "        srd_keys = getNestedKey(surrouding_dict)\n",
    "        prs_keys = getNestedKey(person_dict)\n",
    "        self.categ_ = srd_keys + prs_keys\n",
    "        self.features_.extend(['_S_%s_' % k for k in srd_keys])\n",
    "        self.features_.extend(['_P_%s_' % k for k in prs_keys])\n",
    "\n",
    "        # joint features\n",
    "        self.features_.extend(['_S_%s_%s_' % (k, t) for k in srd_keys for t in self.vocab_])\n",
    "        self.features_.extend(['_P_%s_%s_' % (k, t) for k in prs_keys for t in self.vocab_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2112', 'A311']\n",
      "['indoor.a.01', 'object.n.01', 'appliance.n.02']\n",
      "['interaction.n.01', 'occupation.n.01']\n",
      "['man.n.01', 'look.v.01', 'chart.n.01', 'back.n.01']\n"
     ]
    }
   ],
   "source": [
    "layers, sent = dataset.getOneLayerSent(txt_name='text/1.txt',\n",
    "                                       img_name='images/6.svg')\n",
    "print(layers)\n",
    "from tools.image_process import getNestedKeyWithCode, getNestedKey\n",
    "from rules.category import surrouding_dict, person_dict\n",
    "print(getNestedKeyWithCode(surrouding_dict, [1,1,2]))\n",
    "print(getNestedKeyWithCode(person_dict, [1,1]))\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "front.n.01 : the side that is forward or prominent\n",
      "battlefront.n.01 : the line along which opposing armies face each other\n",
      "front.n.03 : the outward appearance of a person\n",
      "front.n.04 : the side that is seen or that goes first\n",
      "front_man.n.01 : a person used as a cover for some questionable activity\n",
      "front.n.06 : a sphere of activity involving effort\n",
      "front.n.07 : (meteorology) the atmospheric phenomenon created at the boundary between two different air masses\n",
      "presence.n.02 : the immediate proximity of someone or something\n",
      "front.n.09 : the part of something that is nearest to the normal viewer\n",
      "movement.n.04 : a group of people with a common ideology who try together to achieve certain general goals\n",
      "front.v.01 : be oriented in a certain direction, often with respect to another reference point; be opposite to\n",
      "front.v.02 : confront bodily\n",
      "front.a.01 : relating to or located in the front\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "for syn in wn.synsets('front'):\n",
    "    print(syn.name(),':', syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "WordNetError",
     "evalue": "Computing the lch similarity requires Synset('stand.v.01') and Synset('stand.n.02') to have the same part of speech.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWordNetError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bd7b921613b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m wn.lch_similarity(wn.synset('stand.v.01'),\n\u001b[0;32m----> 2\u001b[0;31m                   wn.synset('stand.n.02'))\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/text2scene/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mlch_similarity\u001b[0;34m(self, synset1, synset2, verbose, simulate_root)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlch_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msynset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlch_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m     \u001b[0mlch_similarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlch_similarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/text2scene/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mlch_similarity\u001b[0;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[1;32m    864\u001b[0m             raise WordNetError(\n\u001b[1;32m    865\u001b[0m                 \u001b[0;34m'Computing the lch similarity requires '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0;34m'%s and %s to have the same part of speech.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m             )\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWordNetError\u001b[0m: Computing the lch similarity requires Synset('stand.v.01') and Synset('stand.n.02') to have the same part of speech."
     ]
    }
   ],
   "source": [
    "wn.lch_similarity(wn.synset('stand.v.01'),\n",
    "                  wn.synset('stand.n.02'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 4)\t1.3350010667323402\n",
      "  (1, 14)\t1.2396908869280152\n",
      "  (1, 45)\t1.8718021769015913\n",
      "  (1, 50)\t1.4403615823901665\n",
      "  (3, 4)\t1.2396908869280152\n",
      "  (3, 14)\t1.1526795099383855\n",
      "  (3, 50)\t1.3350010667323402\n",
      "  (17, 45)\t1.8718021769015913\n",
      "  (18, 4)\t1.3350010667323402\n",
      "  (18, 14)\t1.6916760106710724\n",
      "  (18, 50)\t1.3350010667323402\n"
     ]
    }
   ],
   "source": [
    "from tools.image_process import getCrossSimi\n",
    "print(getCrossSimi(layers, sent, dataset.vocab_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concrete\n",
      "appliance\n",
      "interact\n",
      "office\n",
      "--\n",
      "back.n.01\n",
      "chart.n.01\n",
      "look.v.01\n",
      "man.n.01\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(dataset.categ_[1])\n",
    "print(dataset.categ_[3])\n",
    "print(dataset.categ_[17])\n",
    "print(dataset.categ_[18])\n",
    "print('--')\n",
    "print(dataset.vocab_[4])\n",
    "print(dataset.vocab_[14])\n",
    "print(dataset.vocab_[45])\n",
    "print(dataset.vocab_[50])\n",
    "# The problem is, bigrams will never contribute to the cross similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:46: UserWarning: sideways with tag n does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n",
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:46: UserWarning: grid with tag a does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n",
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:46: UserWarning: cell with tag r does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n",
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:46: UserWarning: something with tag n does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features:  5854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:46: UserWarning: sideways with tag n does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n",
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:46: UserWarning: something with tag n does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n",
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:46: UserWarning: grid with tag a does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n",
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:46: UserWarning: cell with tag r does not belong to any synsets.\n",
      "  warnings.warn('%s with tag %s does not belong to any synsets.' % (lemma, tag))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (148, 5855)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "dataset = Dataset()\n",
    "print('# features: ', len(dataset.features_))\n",
    "data = sparse.vstack([dataset[i] for i in range(len(dataset))])\n",
    "X, y = data[:,:-1], data[:,-1]\n",
    "y = y.toarray().flatten()\n",
    "print('data shape', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train:  133\n",
      "# test:  15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight={0: 0.5, 1: 1}, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(# random_state=0,\n",
    "                         solver='liblinear',\n",
    "                         class_weight={1: 1, 0:0.5},\n",
    "                         penalty='l1', #'l2' use l1 to learn sparsely\n",
    "                         C=1.0,\n",
    "                         max_iter=100)\n",
    "\n",
    "ind_test = int(X.shape[0] * 0.9)\n",
    "print('# train: ', ind_test)\n",
    "print('# test: ', X.shape[0] - ind_test)\n",
    "clf.fit(X[:ind_test], y[:ind_test])\n",
    "# clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Accuracy: ', clf.score(X[ind_test:],y[ind_test:]))\n",
    "# print(clf.predict(X))\n",
    "# print(clf.predict_proba(X)[:,1])\n",
    "# print(y)\n",
    "plt.hist(clf.predict_proba(X)[:,1])\n",
    "plt.xlabel('False <-----                -----> True')\n",
    "plt.xlim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_P_stand.v.01_stand.v.01_', 1.0524853581684748),\n",
       " ('_P_enjoyment.n.02_woman.n.01_', 0.9253363311079977),\n",
       " ('_P_show.v.01_show.v.01_', 0.8303021858289612),\n",
       " ('_P_back.n.01_back.n.01_', 0.8210177778056167),\n",
       " ('_P_interaction.n.01_chart.n.01_', -0.6539910088516735),\n",
       " ('_S_chart.n.01_chart.n.01_', 0.6220828750673327),\n",
       " ('_S_object.n.01_woman.n.01_', -0.6117637274630178),\n",
       " ('_P_sit.v.01_sit.v.01_', 0.6051651853355627),\n",
       " ('_P_stand.v.01_be.v.01_', -0.5217148557649905),\n",
       " ('_P_occupation.n.01_', 0.47973805390792656),\n",
       " ('stand.v.01', -0.4439801185954333),\n",
       " ('chart.n.01', -0.4437458447216492),\n",
       " ('_S_street.n.01_street.n.01_', 0.44054373735875585),\n",
       " ('_S_chart.n.01_icon.n.01_', 0.3866140535522039),\n",
       " ('_S_object.n.01_earth.n.01_', 0.3861562723117798),\n",
       " ('_P_interaction.n.01_woman.n.01_', 0.26379609197098),\n",
       " ('_NLayers_', -0.23912939627580848),\n",
       " ('_S_abstraction.n.01_woman.n.01_', 0.23799709641976388),\n",
       " ('woman.n.01 stand.v.01', -0.2330950183563092),\n",
       " ('_Decoration_', -0.22434369909353385),\n",
       " ('_P_abstraction.n.01_', -0.21179059042984172),\n",
       " ('_S_abstraction.n.01_', -0.17694606217160885),\n",
       " ('show.v.01 chart.n.01', -0.14985273053068682),\n",
       " ('_P_interaction.n.01_man.n.01_', 0.14157567141882066),\n",
       " ('_S_vehicle.n.01_airplane.n.01_', 0.13159233438492945),\n",
       " ('man.n.01 show.v.01', -0.1275937274977871),\n",
       " ('_P_lean.v.01_lean.v.01_', 0.12150344598799398),\n",
       " ('front.n.01', -0.10999916294621295),\n",
       " ('stand.v.01 front.n.01', -0.08226071316526716),\n",
       " ('walk.v.01', 0.07540938417686041),\n",
       " ('stand.v.01 earth.n.01', 0.06824989247730417),\n",
       " ('mountain.n.01', -0.05200735562187501),\n",
       " ('be.v.01 sit.v.01', -0.0451980604091577),\n",
       " ('man.n.01 woman.n.01', 0.04083628006171307),\n",
       " ('earth.n.01', 0.040488180919496956),\n",
       " ('screening.n.01 confront.v.02', -0.03859447695312635),\n",
       " ('two.n.01 chart.n.01', 0.037581932845108953),\n",
       " ('_S_art.n.01_woman.n.01_', 0.03375717891844267),\n",
       " ('look.v.01', -0.03371377159025801),\n",
       " ('_P_stand.v.01_', -0.03322673004327469),\n",
       " ('pull.v.01', 0.029411226592746677),\n",
       " ('man.n.01 be.v.01', -0.027568282006644587),\n",
       " ('confront.v.02', -0.02639372056949443),\n",
       " ('two.n.01', 0.023898702018509358),\n",
       " ('compass.n.01', 0.022818084781423192),\n",
       " ('screening.n.01', -0.021393168439608288),\n",
       " ('pull.v.01 sword.n.01', 0.019872272560419523),\n",
       " ('circle.n.01 compass.n.01', 0.018716653543704213),\n",
       " ('rock.n.01', 0.018466376145574448),\n",
       " ('mountain.n.01 moonlight.n.01', -0.01515210023731942),\n",
       " ('man.n.01 pull.v.01', 0.01481126709569855),\n",
       " ('following.s.02 icon.n.01', -0.01386062049938118),\n",
       " ('_S_city.n.01_city.n.01_', 0.011294551368702473),\n",
       " ('chart.n.01 screening.n.01', -0.010930411027037687),\n",
       " ('_S_art.n.01_art.n.01_', 0.00842412969719471),\n",
       " ('show.v.01 two.n.01', 0.008183528620124409),\n",
       " ('man.n.01 look.v.01', -0.007142316049988764),\n",
       " ('telephone.n.01', 0.006416819524556738),\n",
       " ('woman.n.01 play.n.01', 0.0051027881785172475),\n",
       " ('sword.n.01 rock.n.01', 0.004713742640656862),\n",
       " ('play.n.01 circle.n.01', 0.004637054297110519),\n",
       " ('house.n.01 mountain.n.01', -0.004379242978211254),\n",
       " ('lean.v.01', 0.0031752049230055744),\n",
       " ('front.n.01 chart.n.01', -0.0029239635181628463),\n",
       " ('sit.v.01 wood.n.01', -0.001145356113809806),\n",
       " ('house.n.01', -0.0005311883952794546),\n",
       " ('play.n.01', 0.0003456626586035203),\n",
       " ('moonlight.n.01', -0.00030320348802604746),\n",
       " ('wood.n.01', -0.00014664905739495814),\n",
       " ('circle.n.01', 4.369391105750345e-05),\n",
       " ('sword.n.01', 1.5817657935961864e-05),\n",
       " ('chart.n.01 back.n.01', -1.0855399492430508e-05),\n",
       " ('back.n.01', -2.734502638790393e-06),\n",
       " ('look.v.01 chart.n.01', -1.4464855295820107e-06)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dict(zip(dataset.features_, clf.coef_.tolist()[0]))\n",
    "d_filter = filter(lambda x: x[1] != 0, d.items())\n",
    "sorted(d_filter, key=lambda x: abs(x[1]))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo - unseen keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2scene",
   "language": "python",
   "name": "text2scene"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
