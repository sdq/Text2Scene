{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/1.svg\n",
      "['A2122', 'A3212']\n",
      "images/10.svg\n",
      "['A1', 'A2121_1_', 'A323']\n",
      "images/11.svg\n",
      "['A1', 'A2121_1_', 'A313']\n",
      "images/12.svg\n",
      "['A2121', 'A3211']\n",
      "images/13.svg\n",
      "['A2122', 'A3213']\n",
      "images/14.svg\n",
      "['A1', 'A323', 'A2222']\n",
      "images/15.svg\n",
      "['A1', 'A325']\n",
      "images/16.svg\n",
      "['A2221', 'A322']\n",
      "images/17.svg\n",
      "['A2121', 'A3213']\n",
      "images/18.svg\n",
      "['A1', 'A2121_1_', 'A3212']\n",
      "images/19.svg\n",
      "['A2121_1_', 'A3213']\n",
      "images/2.svg\n",
      "['A4', 'A2121', 'A324']\n",
      "images/20.svg\n",
      "['A2113', 'A311']\n",
      "images/3.svg\n",
      "['A4', 'A2122', 'A312']\n",
      "images/4.svg\n",
      "['A2211', 'A311']\n",
      "images/5.svg\n",
      "['A2212', 'A3211']\n",
      "images/6.svg\n",
      "['A2112', 'A311']\n",
      "images/7.svg\n",
      "['A2121', 'A324']\n",
      "images/8.svg\n",
      "['A311', 'A2121']\n",
      "images/9.svg\n",
      "['A2121']\n",
      "(20, 38)\n"
     ]
    }
   ],
   "source": [
    "from tools.image_process import getLayerNames, checkLayerNames, image2feature\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "img_embed = []\n",
    "layer_names = []\n",
    "for fileName in sorted(glob.glob('images/*.svg')):\n",
    "    # base, ext = os.path.splitext(fileName)\n",
    "    # if ext == '.svg' and base != 'test' and base != 'stack':\n",
    "    if 'test' in fileName or 'stack' in fileName:\n",
    "        continue\n",
    "    print(fileName)\n",
    "    # print(get_id_list(fileName))\n",
    "    names = getLayerNames(fileName)\n",
    "    checkLayerNames(names)\n",
    "    print(names)\n",
    "    layer_names.append(names)\n",
    "    # print(image2feature(names))\n",
    "    # print(type(image2feature(names)), type(img_embed))\n",
    "    img_embed.append(image2feature(names))\n",
    "    \n",
    "img_embed = np.array(img_embed)\n",
    "print(img_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caveats\n",
    "# The first <image> element in a <g> is the shadow, namely the extra .png file in the dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test corpus\n",
    "test_text = ['A man is lying on the sofa.',\n",
    "             'A man is sitting next to a computer.',\n",
    "             'A man is presenting a chart. The light is on. we',\n",
    "             'A woman is standing next to a bucket.',\n",
    "             \"It's a deer in a sock\",\n",
    "             'A christmas tree with presents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reader and vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/1.txt\n",
      "text/10.txt\n",
      "text/11.txt\n",
      "text/12.txt\n",
      "text/13.txt\n",
      "text/14.txt\n",
      "text/15.txt\n",
      "text/16.txt\n",
      "text/17.txt\n",
      "text/18.txt\n",
      "text/19.txt\n",
      "text/2.txt\n",
      "text/20.txt\n",
      "text/3.txt\n",
      "text/4.txt\n",
      "text/5.txt\n",
      "text/6.txt\n",
      "text/7.txt\n",
      "text/8.txt\n",
      "text/9.txt\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(20, 103)\n",
      "103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:48: UserWarning: \"sideways\" does not belong to any synsets.\n",
      "  warnings.warn('\"%s\" does not belong to any synsets.' % lemma)\n",
      "/Users/dongjustin/Documents/Text2Scene/Text2Scene/tools/text_process.py:48: UserWarning: \"grid\" does not belong to any synsets.\n",
      "  warnings.warn('\"%s\" does not belong to any synsets.' % lemma)\n"
     ]
    }
   ],
   "source": [
    "# text reader\n",
    "import glob\n",
    "from tools.text_process import LemmaTokenizer\n",
    "tokenizer = LemmaTokenizer()\n",
    "sentences = []\n",
    "for fileName in sorted(glob.glob('text/*.txt')):\n",
    "    print(fileName)\n",
    "    with open(fileName, 'r') as f:\n",
    "        sent = f.read()\n",
    "        # print(sent)\n",
    "        tokens = tokenizer(sent)\n",
    "        # print(tokens)\n",
    "        # print('----')\n",
    "        sentences.append(tokens)\n",
    "        \n",
    "## vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                             norm=None,\n",
    "                             sublinear_tf=True,\n",
    "                             stop_words=[],\n",
    "                             lowercase=False,\n",
    "                             tokenizer=lambda l: l)\n",
    "text_embed = vectorizer.fit_transform(sentences)\n",
    "print(type(text_embed))\n",
    "print(text_embed.shape)\n",
    "# print(vectorizer.fit_transform(sentences).toarray())\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between category keywords and the description\n",
    "**suppress cats not in image??**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.text_process import keywordSimi\n",
    "from tools.image_process import image2SimiFeature\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man.n.01', 'look.v.01', 'chart.n.01', 'back.n.01']\n",
      "['man.n.01', 'walk.v.01', 'past.n.01', 'web.n.01', 'icon.n.01']\n",
      "['man.n.01', 'woman.n.01', 'be.v.01', 'interact.v.01']\n",
      "['man.n.01', 'lean.v.01', 'set.n.01', 'chart.n.01']\n",
      "['woman.n.01', 'stand.v.01', 'following.s.02', 'poster.n.01', 'planet.n.01']\n",
      "['man.n.01', 'pull.v.01', 'sword.n.01', 'rock.n.01']\n",
      "['woman.n.01', 'be.v.01', 'show.v.01', 'love.v.01']\n",
      "['man.n.01', 'be.v.01', 'sit.v.01', 'wood.n.01']\n",
      "['man.n.01', 'stand.v.01', 'following.s.02', 'icon.n.01']\n",
      "['man.n.01', 'stand.v.01', 'front.n.01', 'chart.n.01', 'show.v.01', 'muscle.n.01']\n",
      "['woman.n.01', 'stand.v.01', 'front.n.01', 'chart.n.01']\n",
      "['man.n.01', 'show.v.01', 'chart.n.01']\n",
      "['man.n.01', 'drink.v.01', 'coffee.n.01', 'airport.n.01', 'window.n.01']\n",
      "['man.n.01', 'woman.n.01', 'play.n.01', 'circle.n.01', 'compass.n.01']\n",
      "['girl.n.01', 'be.v.01', 'lean.v.01', 'car.n.01']\n",
      "['girl.n.01', 'be.v.01', 'stand.v.01', 'edge.n.01', 'city.n.01']\n",
      "['man.n.01', 'be.v.01', 'clean.v.01', 'computer.n.01']\n",
      "['male_child.n.01', 'be.v.01', 'show.v.01', 'form.n.01']\n",
      "['be.v.01', 'three.n.01', 'hallmark.n.01', 'about.r.03', 'person.n.01']\n",
      "['mobile.n.01', 'telephone.n.01', 'chart.n.01']\n",
      "(20, 33)\n",
      "(20, 38)\n"
     ]
    }
   ],
   "source": [
    "feature_simis = []\n",
    "for layers, sentence in zip(layer_names, sentences):\n",
    "    print(sentence)\n",
    "    feature_simis.append(image2SimiFeature(layers, sentence))\n",
    "\n",
    "feature_simis = np.array(feature_simis)\n",
    "print(feature_simis.shape)\n",
    "print(img_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- text\n",
      "(20, 103)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "--- image\n",
      "(20, 38)\n",
      "<class 'numpy.ndarray'>\n",
      "--- joint\n",
      "(20, 33)\n",
      "<class 'numpy.ndarray'>\n",
      "---\n",
      "(20, 174)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "print('--- text')\n",
    "print(text_embed.shape)\n",
    "print(type(text_embed))\n",
    "print('--- image')\n",
    "print(img_embed.shape)\n",
    "print(type(img_embed))\n",
    "print('--- joint')\n",
    "print(feature_simis.shape)\n",
    "print(type(feature_simis))\n",
    "print('---')\n",
    "embed_all = scipy.sparse.hstack([text_embed, \n",
    "                                 img_embed,\n",
    "                                 feature_simis])\n",
    "print(embed_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = parse('SB.svg')\n",
    "# ele = [c for c in doc.childNodes if c.nodeType == 1]\n",
    "# assert(len(ele) == 1)\n",
    "# [n.getAttribute('id') for n in ele[0].childNodes if n.nodeType==1 and n.tagName=='g']\n",
    "# [g.getAttribute('id') for g in doc.getElementsByTagName('g') if g.hasAttribute('id')]\n",
    "# id_list = [g.getAttribute('id') for g in doc.getElementsByTagName('g') if g.hasAttribute('id')]\n",
    "# assert(id_list)\n",
    "\n",
    "# get the tagname: element.tagName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the order may be reversed, that's weird\n",
    "# at least. background must be at the bottom\n",
    "# but what if there are only person and surronding\n",
    "\n",
    "# it's reversed\n",
    "# because the first object is at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DOM Element: image at 0x112681cc0>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.getElementsByTagName('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_x31__x5F_1_x5F_1']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[g.getAttribute('id') for g in doc.getElementsByTagName('*') if g.hasAttribute('id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2scene",
   "language": "python",
   "name": "text2scene"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
