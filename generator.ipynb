{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det(man-2, A:DT-1)\n",
      "nsubj(playing-7, man:NN-2)\n",
      "cc(man-2, and:CC-3)\n",
      "det(man-5, a:DT-4)\n",
      "conj(man-2, man:NN-5)\n",
      "aux(playing-7, are:VBP-6)\n",
      "ROOT(playing-7, playing:VBG-7)\n",
      "dobj(playing-7, computer:NN-8)\n",
      "cc(playing-7, and:CC-9)\n",
      "conj(playing-7, sitting:VBG-10)\n",
      "prep(sitting-10, on:IN-11)\n",
      "det(ground-13, the:DT-12)\n",
      "pobj(on-11, ground:NN-13)\n",
      "prep(sitting-10, in:IN-14)\n",
      "det(wild-16, the:DT-15)\n",
      "pobj(in-14, wild:NN-16)\n",
      "punct(playing-7, .:.-17)\n",
      "det(woman-19, A:DT-18)\n",
      "nsubj(waving-21, woman:NN-19)\n",
      "aux(waving-21, is:VBZ-20)\n",
      "ROOT(waving-21, waving:VBG-21)\n",
      "prep(waving-21, under:IN-22)\n",
      "det(tree-24, the:DT-23)\n",
      "pobj(under-22, tree:NN-24)\n",
      "punct(waving-21, .:.-25)\n",
      "det(man-27, A:DT-26)\n",
      "nsubj(lying-29, man:NN-27)\n",
      "aux(lying-29, is:VBZ-28)\n",
      "ROOT(lying-29, lying:VBG-29)\n",
      "prep(lying-29, on:IN-30)\n",
      "det(games-34, the:DT-31)\n",
      "compound(games-34, sofa:NN-32)\n",
      "compound(games-34, playing:NN-33)\n",
      "pobj(on-30, games:NNS-34)\n",
      "punct(lying-29, .:.-35)\n",
      "expl(are-37, There:EX-36)\n",
      "ROOT(are-37, are:VBP-37)\n",
      "det(trees-39, some:DT-38)\n",
      "attr(are-37, trees:NNS-39)\n",
      "cc(trees-39, and:CC-40)\n",
      "conj(trees-39, plants:NNS-41)\n",
      "prep(trees-39, in:IN-42)\n",
      "det(park-44, the:DT-43)\n",
      "pobj(in-42, park:NN-44)\n",
      "punct(are-37, .:.-45)\n",
      "det(alien-47, An:DT-46)\n",
      "nsubj(lying-49, alien:NN-47)\n",
      "aux(lying-49, is:VBZ-48)\n",
      "ROOT(lying-49, lying:VBG-49)\n",
      "xcomp(lying-49, crying:VBG-50)\n",
      "punct(lying-49, .:.-51)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'A man and a man are playing computer and sitting on the ground in the wild. A woman is waving under the tree. A man is lying on the sofa playing games. There are some trees and plants in the park. An alien is lying crying.')\n",
    "# parse(doc)\n",
    "for token in doc:\n",
    "    print(\"{2}({3}-{6}, {0}:{1}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[computer, ground, tree, sofa, playing, games, trees, plants]\n",
      "ground None\n",
      "ground None\n",
      "wild tree\n",
      "tree tree\n",
      "sofa None\n",
      "sofa None\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-90644c223dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'VERB'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m \u001b[0mground_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnested\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;31m# print(map_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-90644c223dcc>\u001b[0m in \u001b[0;36mground_obj\u001b[0;34m(tokens_, nested_)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 obj_ = get_simi_obj(token.lemma_,\n\u001b[1;32m    124\u001b[0m                                     \u001b[0mlayerbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_merge_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnested_entities_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'have'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                                     thresh=0.2)\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mobj_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-90644c223dcc>\u001b[0m in \u001b[0;36mget_simi_obj\u001b[0;34m(token, keywords, thresh)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_simi_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_related\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msimi_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimi_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msimi_\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-90644c223dcc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_simi_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_related\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msimi_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimi_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msimi_\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-90644c223dcc>\u001b[0m in \u001b[0;36mquery_related\u001b[0;34m(t, k)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://api.conceptnet.io/relatedness?node1=/c/en/%s&node2=/c/en/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_simi_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/text2scene/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/text2scene/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/text2scene/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/text2scene/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from rules.labels import subjects\n",
    "from collections import defaultdict\n",
    "\n",
    "def incre_name(s, dic):\n",
    "    count = 0\n",
    "    for k in dic:\n",
    "        # remove any tail digits\n",
    "        sub = re.sub(r'(?<=\\w)\\d+$','', k)\n",
    "        if re.match(r'%s\\d*' % sub, s):\n",
    "            # print(count, r'%s\\d*' % sub, s)\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        return '%s%i' % (s, count)\n",
    "    return s\n",
    "\n",
    "def get_tokens(doc):\n",
    "    return [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "    \n",
    "def ground_subj(tokens_, nested_):\n",
    "    \n",
    "    ## query the base to identify a subject\n",
    "    tokens_copy = tokens_.copy()\n",
    "    for token in tokens_copy:\n",
    "        for type_ in subjects:\n",
    "            if token.lemma_ in subjects[type_]:\n",
    "                tokens_.remove(token)\n",
    "                nested_[token] = defaultdict(set)\n",
    "#                 if token.lemma_ not in nested_:\n",
    "#                     map_dict[token] = token.lemma_\n",
    "#                     nested_[token.lemma_] = defaultdict(set)\n",
    "#                 else:\n",
    "#                     lemma_i = incre_name(token.lemma_, nested_)\n",
    "#                     map_dict[token] = lemma_i\n",
    "#                     nested_[lemma_i] = defaultdict(set)\n",
    "\n",
    "def ground_act(tokens_, nested_):\n",
    "    \n",
    "    ## ---- syntactical parency\n",
    "    tokens_copy = tokens_.copy()\n",
    "    for token in tokens_copy:\n",
    "        for key in nested_:\n",
    "            if key.head == token:\n",
    "                assert(key.dep_ == 'nsubj')\n",
    "                tokens_.remove(token)\n",
    "                nested_[key][token] = set()\n",
    "    \n",
    "    ## ---- conjuncted verbs\n",
    "    tokens_copy = tokens_.copy()\n",
    "    # cannot pickle a spacy.token\n",
    "    ## nested_copy = deepcopy(nested_)\n",
    "    # cannnot modify a dict during iteration, thus save keys\n",
    "    saved_tups = []\n",
    "    for token in tokens_copy:\n",
    "        for subj in nested_:\n",
    "            for act in nested_[subj]:\n",
    "                if token.head == act and token.pos_ == 'VERB':\n",
    "                    assert(token.dep_ in ['conj', 'xcomp', 'advcl']), token.dep_\n",
    "                    tokens_.remove(token)\n",
    "                    saved_tups.append((subj, token))\n",
    "    for subj, token in saved_tups:\n",
    "        nested_[subj][token] = set()\n",
    "    \n",
    "    ## ---- other verbs\n",
    "    ### Finally,\n",
    "    ### if the verb has the common root with any of the subjects, bind it. This cause confusion when a sentence contains two or more subjects\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    for token in tokens_copy:\n",
    "        if token.pos_ == 'VERB':\n",
    "            for subj in nested_:\n",
    "                if token.sent.root == subj.sent.root:\n",
    "                    tokens_.remove(token)\n",
    "                    saved_tups.append(subj, token)\n",
    "    for subj, token in saved_tups:\n",
    "        nested_[subj][token] = set()    \n",
    "    \n",
    "# static var stuff\n",
    "# related dict needs to transformed to dict\n",
    "def query_related(t, k):\n",
    "    import dill\n",
    "    with open('relateDict.pkl', 'rb') as f:\n",
    "        relateDict = dill.load(f) \n",
    "    if k in relateDict and t in relateDict[k]:\n",
    "        return relateDict[k][t]\n",
    "    \n",
    "    import requests\n",
    "    return requests.get('http://api.conceptnet.io/relatedness?node1=/c/en/%s&node2=/c/en/%s' % (k, t)).json()['value']\n",
    "        \n",
    "def get_simi_obj(token, keywords, thresh=0.5):\n",
    "    tups = [(k, query_related(token, k)) for k in keywords]\n",
    "    simi_obj, simi_ = sorted(tups, key=lambda x: x[1])[-1]\n",
    "    if simi_ >= thresh:\n",
    "        return simi_obj\n",
    "#     return None\n",
    "#     for keyword in keywords:\n",
    "#         if  > thresh:\n",
    "#             return keyword\n",
    "    return None\n",
    "    \n",
    "def ground_obj(tokens_, nested_):\n",
    "    \n",
    "    ## sytactically ground\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    for token in tokens_copy:\n",
    "        for subj in nested_:\n",
    "            for act in nested_[subj]:\n",
    "                if token.head == act and token.pos_ == 'NOUN':\n",
    "                    assert(token.dep_ in ['dobj'])\n",
    "                    tokens_.remove(token)\n",
    "                    saved_tups.append((subj, act, token))\n",
    "    for subj, act, token in saved_tups:\n",
    "        nested_[subj][act].add(token)\n",
    "        \n",
    "    ## query the base knowledge for surrounding objects\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    for token in tokens_copy:\n",
    "        for subj in nested_:\n",
    "            if subj.lemma_ in subjects['surrounding']:\n",
    "                assert(subj.lemma_ in layerbase.layer_merge_.nested_entities_)\n",
    "                if token.lemma_ in layerbase.layer_merge_.nested_entities_[subj.lemma_]['have']:            \n",
    "                    print(subj, token)\n",
    "                # get the most similar obj in the vocabulary under this subject\n",
    "                obj_ = get_simi_obj(token.lemma_,\n",
    "                                    layerbase.layer_merge_.nested_entities_[subj.lemma_]['have'],\n",
    "                                    thresh=0.2)\n",
    "                print(token.lemma_, obj_)\n",
    "                if obj_:\n",
    "                    tokens_.remove(token)\n",
    "                    saved_tups.append((subj, token))\n",
    "                    # prevent other subjs containing the same object emerging\n",
    "                    # first come, first serve\n",
    "                    break \n",
    "    for subj, token in saved_tups:\n",
    "        nested_[subj]['have'].add(token)\n",
    "        \n",
    "#     ## other objects, just query the surroundings in knowledge and see which it belongs to\n",
    "#     tokens_copy = tokens_.copy()\n",
    "#     for token in tokens_copy:    \n",
    "#         for subj in layerbase.layer_merge_.nested_entities_:\n",
    "#             if subj not in subjects['surrounding']:\n",
    "#                 continue\n",
    "#             for act in layerbase.layer_merge_.nested_entities_[subj]:\n",
    "#                 obj_ = get_simi_obj(token.lemma_,\n",
    "#                                     layerbase.layer_merge_.nested_entities_[subj][act],\n",
    "#                                     thresh=0.3)\n",
    "#                 if obj_:\n",
    "#                     assert(act == 'have'), (act, token)\n",
    "#                     # assert(subj not in [s.lemma_ for s in nested_])\n",
    "#                     ## Attention! here the key type is string now\n",
    "#                     print(token, subj, obj_)\n",
    "#                     tokens_.remove(token)\n",
    "#                     nested_[subj][act].add(token)\n",
    "#                     break\n",
    "                    \n",
    "\n",
    "# def comb_obj(tokens_, nested_):\n",
    "    \n",
    "#     ## layers built from clusters\n",
    "#     tokens_copy = tokens_.copy()\n",
    "#     for token in tokens_copy:\n",
    "    \n",
    "    \n",
    "\n",
    "####\n",
    "# technically all entities can be grounded based on similarity, no need to exactly same\n",
    "####\n",
    "nested = defaultdict(lambda: defaultdict(set))\n",
    "tokens = get_tokens(doc)\n",
    "# map_dict = defaultdict(str)\n",
    "ground_subj(tokens, nested)\n",
    "# print(tokens)\n",
    "ground_act(tokens, nested)\n",
    "## no verbs will be left\n",
    "assert(all([t.pos_ != 'VERB' for t in tokens]))\n",
    "print(tokens)\n",
    "ground_obj(tokens, nested)\n",
    "print(tokens)\n",
    "# print(map_dict)\n",
    "nested\n",
    "\n",
    "## why don't we use scapy token as dict keys, such that no overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relateDict['plant']['plant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balloon',\n",
       " 'beach',\n",
       " 'boat',\n",
       " 'cloud',\n",
       " 'firework',\n",
       " 'fish',\n",
       " 'fish_tank',\n",
       " 'guideboard',\n",
       " 'hill',\n",
       " 'lake',\n",
       " 'leaf',\n",
       " 'plant',\n",
       " 'rock',\n",
       " 'sign',\n",
       " 'sun',\n",
       " 'sunflower',\n",
       " 'tree',\n",
       " 'water',\n",
       " 'wind'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layerbase.layer_merge_.nested_entities_['wild']['have']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.instance import Node\n",
    "from tools.containers import Picture, Description\n",
    "from tools.knowledge import LayerBase, TextBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tools.image_process import LayerName\n",
    "from tools.common import ravel\n",
    "\n",
    "class LayerBase():\n",
    "    \"\"\"\n",
    "    layer Base knowledge, show only built on train set!\n",
    "    \"\"\"\n",
    "    def __init__(self, filenames=[],\n",
    "                 img_dir='images',\n",
    "                 ext='.svg'):\n",
    "\n",
    "        \"\"\"\n",
    "        need other dictionary to save the layer frequency\n",
    "        \"\"\"\n",
    "        if not filenames:\n",
    "            filenames = glob.glob('%s/*%s' % (img_dir, ext))\n",
    "        else:\n",
    "            filenames = ['%s/%s%s' % (img_dir, name, ext) for name in filenames]\n",
    "\n",
    "        self.layer_merge_ = LayerName()\n",
    "        self.pictures_ = []\n",
    "        for svg in filenames:\n",
    "            picture = Picture(svg)\n",
    "            self.layer_merge_.absorb(picture.layer_merge_)\n",
    "            self.pictures_.append(picture)\n",
    "        self.entities_ = self.layer_merge_.entities_\n",
    "\n",
    "        # picture vocab contains no dupicates\n",
    "        self.pic_vocab_ = set(self.pictures_)\n",
    "\n",
    "        # layer vocab contains no dupicates\n",
    "        self.layer_vocab_ = set([layer for picture in self.pictures_ for layer in picture.layers_])\n",
    "\n",
    "        # keyword vocab contains no dupicates\n",
    "        # here we explicitly need the order the make sure results reproducable\n",
    "        ## such as index and line up features\n",
    "        self.vocab_ = sorted(ravel(self.layer_merge_.entities_))\n",
    "\n",
    "    def index(self, keyword):\n",
    "        assert(isinstance(keyword, Node))\n",
    "        return self.vocab_.index(keyword)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerbase = LayerBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dill\n",
    "relateDict['sofa']['sofa']\n",
    "# mirror_dict = defaultdict(lambda: defaultdict(float))\n",
    "# for k1 in relateDict:\n",
    "#     for k2 in relateDict[k1]:\n",
    "#         mirror_dict[k2][k1] = relateDict[k1][k2]\n",
    "# mirror_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.415"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get('http://api.conceptnet.io/relatedness?node1=/c/en/%s&node2=/c/en/%s' % ('fishing_rod', 'fish')).json()['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function tools.image_process.LayerName.collapse_subj.<locals>.<lambda>()>,\n",
       "            {'background': {},\n",
       "             'accessory': {'have': {'leaf', 'plant'}},\n",
       "             'other': {'have': {'bulletin',\n",
       "               'camera',\n",
       "               'circle',\n",
       "               'cloud',\n",
       "               'drone',\n",
       "               'gamepad',\n",
       "               'plant',\n",
       "               'square',\n",
       "               'sun',\n",
       "               'tree',\n",
       "               'triangle',\n",
       "               'webpage',\n",
       "               'website',\n",
       "               'windmill'}},\n",
       "             'man': {'point_to': set(),\n",
       "              'stand': set(),\n",
       "              'sit': set(),\n",
       "              'hold': {'cube', 'fishing_rod', 'paper', 'phone'},\n",
       "              'fishing': set(),\n",
       "              'raise': {'arm'},\n",
       "              'drink': {'beer'},\n",
       "              'eat': {'chips'},\n",
       "              'touch': set(),\n",
       "              'play': {'computer'},\n",
       "              'think': set(),\n",
       "              'walk': set(),\n",
       "              'lie_on': {'stone'},\n",
       "              'talk': set(),\n",
       "              'collect': {'data'},\n",
       "              'exercise': set(),\n",
       "              'step_on': {'stone'},\n",
       "              'work': set(),\n",
       "              'lie': set(),\n",
       "              'watch': {'movie'}},\n",
       "             'office': {'have': {'phone'}},\n",
       "             'wild': {'have': {'balloon',\n",
       "               'beach',\n",
       "               'boat',\n",
       "               'cloud',\n",
       "               'firework',\n",
       "               'fish',\n",
       "               'fish_tank',\n",
       "               'guideboard',\n",
       "               'hill',\n",
       "               'lake',\n",
       "               'leaf',\n",
       "               'plant',\n",
       "               'rock',\n",
       "               'sign',\n",
       "               'sun',\n",
       "               'sunflower',\n",
       "               'tree',\n",
       "               'water',\n",
       "               'wind'}},\n",
       "             'woman': {'sport': set(),\n",
       "              'run': set(),\n",
       "              'have': {'head_shot'},\n",
       "              'watch': set(),\n",
       "              'stand': set(),\n",
       "              'hold': {'flag', 'flower', 'pencil', 'phone'},\n",
       "              'play': {'computer', 'phone'},\n",
       "              'sit': set(),\n",
       "              'work': set(),\n",
       "              'write': set(),\n",
       "              'bend': set(),\n",
       "              'present': set(),\n",
       "              'back_on': set(),\n",
       "              'touch': set(),\n",
       "              'walk': set(),\n",
       "              'talk': set(),\n",
       "              'show': set(),\n",
       "              'point_to': set(),\n",
       "              'take': {'pictures'},\n",
       "              'lie': set(),\n",
       "              'sit_on': {'chair'},\n",
       "              'run_over': {'hurdle'},\n",
       "              'exercise': set()},\n",
       "             'chart': {'have': {'arrow', 'folder', 'list', 'money', 'paper'}},\n",
       "             'home': {'have': {'apple',\n",
       "               'bookcase',\n",
       "               'chips',\n",
       "               'house',\n",
       "               'sofa',\n",
       "               'sun',\n",
       "               'toy',\n",
       "               'tree',\n",
       "               'wall_clock'}},\n",
       "             'alien': {'have': {'robot', 'snowman'}, 'stand': set()},\n",
       "             'park': {'have': {'plant'}}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layerbase.layer_merge_.nested_entities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2scene",
   "language": "python",
   "name": "text2scene"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
