{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det(man-2, A:DT-1)\n",
      "nsubj(playing-7, man:NN-2)\n",
      "cc(man-2, and:CC-3)\n",
      "det(boy-5, a:DT-4)\n",
      "conj(man-2, boy:NN-5)\n",
      "aux(playing-7, are:VBP-6)\n",
      "ROOT(playing-7, playing:VBG-7)\n",
      "dobj(playing-7, computer:NN-8)\n",
      "cc(playing-7, and:CC-9)\n",
      "conj(playing-7, sitting:VBG-10)\n",
      "prep(sitting-10, on:IN-11)\n",
      "det(ground-13, the:DT-12)\n",
      "pobj(on-11, ground:NN-13)\n",
      "prep(sitting-10, in:IN-14)\n",
      "det(wild-16, the:DT-15)\n",
      "pobj(in-14, wild:NN-16)\n",
      "punct(playing-7, .:.-17)\n",
      "det(woman-19, A:DT-18)\n",
      "nsubj(waving-21, woman:NN-19)\n",
      "aux(waving-21, is:VBZ-20)\n",
      "ROOT(waving-21, waving:VBG-21)\n",
      "prep(waving-21, under:IN-22)\n",
      "det(tree-24, the:DT-23)\n",
      "pobj(under-22, tree:NN-24)\n",
      "punct(waving-21, .:.-25)\n",
      "det(girl-27, A:DT-26)\n",
      "nsubj(sitting-29, girl:NN-27)\n",
      "aux(sitting-29, is:VBZ-28)\n",
      "ROOT(sitting-29, sitting:VBG-29)\n",
      "prep(sitting-29, on:IN-30)\n",
      "det(chair-32, the:DT-31)\n",
      "pobj(on-30, chair:NN-32)\n",
      "punct(sitting-29, .:.-33)\n",
      "det(man-35, A:DT-34)\n",
      "nsubj(lying-37, man:NN-35)\n",
      "aux(lying-37, is:VBZ-36)\n",
      "ROOT(lying-37, lying:VBG-37)\n",
      "prep(lying-37, on:IN-38)\n",
      "det(games-42, the:DT-39)\n",
      "compound(games-42, sofa:NN-40)\n",
      "compound(games-42, playing:NN-41)\n",
      "pobj(on-38, games:NNS-42)\n",
      "punct(lying-37, .:.-43)\n",
      "expl(are-45, There:EX-44)\n",
      "ROOT(are-45, are:VBP-45)\n",
      "det(trees-47, some:DT-46)\n",
      "attr(are-45, trees:NNS-47)\n",
      "cc(trees-47, and:CC-48)\n",
      "conj(trees-47, plants:NNS-49)\n",
      "cc(plants-49, and:CC-50)\n",
      "conj(plants-49, flowers:NNS-51)\n",
      "prep(trees-47, in:IN-52)\n",
      "det(park-54, the:DT-53)\n",
      "pobj(in-52, park:NN-54)\n",
      "punct(are-45, .:.-55)\n",
      "det(alien-57, An:DT-56)\n",
      "nsubj(lying-59, alien:NN-57)\n",
      "aux(lying-59, is:VBZ-58)\n",
      "ROOT(lying-59, lying:VBG-59)\n",
      "xcomp(lying-59, crying:VBG-60)\n",
      "punct(lying-59, .:.-61)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'A man and a boy are playing computer and sitting on the ground in the wild. A woman is waving under the tree. A girl is sitting on the chair. A man is lying on the sofa playing games. There are some trees and plants and flowers in the park. An alien is lying crying.')\n",
    "# parse(doc)\n",
    "# 'he is '\n",
    "# 'mike is'\n",
    "for token in doc:\n",
    "    print(\"{2}({3}-{6}, {0}:{1}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rules.labels import subjects\n",
    "from tools.knowledge import LayerBase, TextBase\n",
    "layerbase = LayerBase()\n",
    "# prs_subjects = set([subj for subj in layerbase.entities_['subj'] if subj in subjects['character']])\n",
    "# prs_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similairties between subjects: the largest common subset\n",
    "#     E.g. 1. text: man man and base: man woman\n",
    "#              man - man: 1 + man - woman: 0.5\n",
    "#          2. TEXT: man man and BASE: man\n",
    "#              man - man: 1\n",
    "#              so choose man and woman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from rules.labels import subjects\n",
    "from collections import defaultdict\n",
    "from tools.common import ddict2dict\n",
    "from tools.instance import Node\n",
    "from query_relatedness import query_simi\n",
    "\n",
    "# class Ground:\n",
    "#     def __init__(self):\n",
    "#         self.layerbase = LayerBase()\n",
    "#         self.prs_subjects = set([subj for subj in layerbase.entities_['subj'] if subj in subjects['character']])\n",
    "#         # self.srd_subjects = \n",
    "\n",
    "def incre_name(s, dic):\n",
    "    count = 0\n",
    "    for k in dic:\n",
    "        # remove any tail digits\n",
    "        sub = re.sub(r'(?<=\\w)\\d+$','', k)\n",
    "        if re.match(r'%s\\d*' % sub, s):\n",
    "            # print(count, r'%s\\d*' % sub, s)\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        return '%s%i' % (s, count)\n",
    "    return s\n",
    "\n",
    "def get_tokens(doc):\n",
    "    return [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "\n",
    "def get_simi_keyword(token, keywords, thresh=0.5):\n",
    "    \n",
    "    tups = [(k, query_simi(token.lemma_, k.t)) for k in keywords]\n",
    "    simi_key, simi_ = sorted(tups, key=lambda x: (x[1], x[0].count))[-1]\n",
    "    if simi_ >= thresh:\n",
    "        return simi_key, simi_\n",
    "#     return None\n",
    "#     for keyword in keywords:\n",
    "#         if  > thresh:\n",
    "#             return keyword\n",
    "    return None\n",
    "\n",
    "class MappedToken:\n",
    "    def __init__(self, token, keyword):\n",
    "        if token:\n",
    "            assert(isinstance(token, spacy.tokens.token.Token))\n",
    "        assert(isinstance(keyword, Node))\n",
    "        self.token = token\n",
    "        self.keyword = keyword\n",
    "        \n",
    "        self.tup = (self.token, self.keyword)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.token is None:\n",
    "            return '->%s' % (self.keyword.t)\n",
    "        return '%s->%s' % (self.token.lemma_, self.keyword.t)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.tup == other.tup\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.tup)\n",
    "    \n",
    "def ground_subj(tokens_, nested_):\n",
    "    \n",
    "    ## query the base to identify a subject\n",
    "    tokens_copy = tokens_.copy()\n",
    "    subjs = [subj for subj in layerbase.collocations_]\n",
    "    for token in tokens_copy:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            tup = get_simi_keyword(token, subjs, thresh=0.5)\n",
    "            if tup:\n",
    "                subj_, _ = tup\n",
    "                tokens_.remove(token)\n",
    "                nested_[MappedToken(token, subj_)] = defaultdict(set)\n",
    "#         for type_ in subjects:\n",
    "#             if token.lemma_ in subjects[type_]:\n",
    "#                 tokens_.remove(token)\n",
    "#                 nested_[token] = defaultdict(set)\n",
    "#                 if token.lemma_ not in nested_:\n",
    "#                     map_dict[token] = token.lemma_\n",
    "#                     nested_[token.lemma_] = defaultdict(set)\n",
    "#                 else:\n",
    "#                     lemma_i = incre_name(token.lemma_, nested_)\n",
    "#                     map_dict[token] = lemma_i\n",
    "#                     nested_[lemma_i] = defaultdict(set)\n",
    "\n",
    "def bind_keyword(token, subj, act=None, thresh=0.3):\n",
    "    # bind keyword\n",
    "    if act is None:\n",
    "        # used to bind actions\n",
    "        # keywords = [k for k in layerbase.collocations_[subj]]\n",
    "        keywords = [k for k in layerbase.entities_['act']]\n",
    "        attr = 'act'\n",
    "    else:\n",
    "        # used to bind objects\n",
    "        # maybe, search all objects regardless of subj and act\n",
    "#         if act == Node('', attr='act'):\n",
    "#             # unbound action, retrieve objects under all actions under this subject\n",
    "#             keywords = [k for a in layerbase.collocations_[subj] for k in layerbase.collocations_[subj][a]]\n",
    "#         else:\n",
    "#             # bound action, retrieve objects under this subject and this actions\n",
    "#             keywords = [k for k in layerbase.collocations_[subj][act]]\n",
    "        keywords = [k for k in layerbase.entities_['obj']]\n",
    "        attr = 'obj'\n",
    "        \n",
    "    tup = get_simi_keyword(token, keywords, thresh=thresh)\n",
    "    if tup:\n",
    "        k_, _ = tup\n",
    "    else:\n",
    "        k_ = Node('', attr=attr)\n",
    "    return MappedToken(token, k_)\n",
    "\n",
    "def find_most_simi_subj(subjs, token, dest='obj'):\n",
    "    assert(isinstance(token, spacy.tokens.token.Token))\n",
    "    assert(all([isinstance(subj, Node) for subj in subjs]))\n",
    "    # assert(all([s.t in subjects['surrounding'] for s in subjs]))\n",
    "    # search all subjects, not just surrounding subjects. E.g. Alien have robot\n",
    "    if dest == 'obj':\n",
    "        assert(all([Node('have', attr='act') in layerbase.collocations_[s] for s in subjs]))\n",
    "    elif dest == 'act':\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError\n",
    "        \n",
    "    most_simi_ = []\n",
    "    for subj in subjs:\n",
    "        if dest == 'obj':\n",
    "            tup = get_simi_keyword(token,\n",
    "                                   layerbase.collocations_[subj][Node('have', attr='act')],\n",
    "                                   thresh=0.2)\n",
    "        elif dest == 'act' :\n",
    "            if not layerbase.collocations_[subj]: continue\n",
    "            tup = get_simi_keyword(token, layerbase.collocations_[subj], thresh=0.2)\n",
    "        else:\n",
    "            raise KeyError\n",
    "        if tup:\n",
    "            most_simi_.append((subj,) + tup)\n",
    "            \n",
    "    if most_simi_:\n",
    "        print(most_simi_)\n",
    "        if dest == 'obj':\n",
    "            sort_f = lambda x: (x[-1], get_element_from_set(layerbase.collocations_[x[0]][Node('have', attr='act')], x[1]).count)\n",
    "        elif dest == 'act':\n",
    "            sort_f = lambda x: (x[-1], get_element_from_set(layerbase.collocations_[x[0]], x[1]).count)\n",
    "        else:\n",
    "            raise KeyError\n",
    "        return sorted(most_simi_, key=sort_f)[-1]\n",
    "    return None\n",
    "\n",
    "def ground_act(tokens_, nested_):\n",
    "    \n",
    "    ## ---- syntactical parency\n",
    "    tokens_copy = tokens_.copy()\n",
    "    for token in tokens_copy:\n",
    "        for key in nested_:\n",
    "            if token == key.token.head:\n",
    "                assert(key.token.dep_ == 'nsubj'), key.token.dep_\n",
    "                tokens_.remove(token)\n",
    "                \n",
    "                # bind keyword\n",
    "#                 acts = [a for a in layerbase.collocations_[key.keyword]]\n",
    "#                 tup = get_simi_keyword(token, acts, thresh=0.3)\n",
    "#                 if tup:\n",
    "#                     act_, _ = tup\n",
    "#                 else:\n",
    "#                     act_ = Node('', attr='act')\n",
    "                # print(token, key.keyword)\n",
    "                nested_[key][bind_keyword(token, key.keyword)] = set()\n",
    "    \n",
    "    ## ---- conjuncted verbs\n",
    "    tokens_copy = tokens_.copy()\n",
    "    # cannot pickle a spacy.token\n",
    "    ## nested_copy = deepcopy(nested_)\n",
    "    # cannnot modify a dict during iteration, thus save keys\n",
    "    saved_tups = []\n",
    "    for token in tokens_copy:\n",
    "        for subj in nested_:\n",
    "            for act in nested_[subj]:\n",
    "                if token.head == act.token and token.pos_ == 'VERB':\n",
    "                    assert(token.dep_ in ['conj', 'xcomp', 'advcl']), token.dep_\n",
    "                    tokens_.remove(token)\n",
    "                    saved_tups.append((subj, token))\n",
    "    for subj, token in saved_tups:   \n",
    "        nested_[subj][bind_keyword(token, subj.keyword)] = set()\n",
    "    \n",
    "    ## ---- other verbs\n",
    "    ### if the verb has the common root with any of the subjects, bind it. This cause confusion when a sentence contains two or more subjects\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    for token in tokens_copy:\n",
    "        if token.pos_ == 'VERB':\n",
    "            for subj in nested_:\n",
    "                if token.sent.root == subj.sent.root:\n",
    "                    tokens_.remove(token)\n",
    "                    saved_tups.append(subj, token)\n",
    "    for subj, token in saved_tups:\n",
    "        nested_[subj][bind_keyword(token, subj.keyword)] = set() \n",
    "        \n",
    "    ### finally if still some verbs left, use knowledge base\n",
    "    ## subject-object collocation knowledge ground, subjects in the layerbase\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    subjs = [subj for subj in layerbase.collocations_]\n",
    "    for token in tokens_copy:\n",
    "        if token.pos_ == 'VERB':\n",
    "            tup = find_most_simi_subj(subjs, token, dest='act')\n",
    "            if tup:\n",
    "                subj_, act_, s_ = tup\n",
    "                tokens_.remove(token)\n",
    "                saved_tups.append((subj_, act_, token))\n",
    "    for subj, act, token in saved_tups:\n",
    "        nested_[MappedToken(None, subj)][MappedToken(token, act)] = set()\n",
    "    \n",
    "# static var stuff\n",
    "# related dict needs to transformed to dict\n",
    "# def query_related(t, k):\n",
    "#     import dill\n",
    "#     with open('relateDict.pkl', 'rb') as f:\n",
    "#         relateDict = dill.load(f) \n",
    "#     if k in relateDict and t in relateDict[k]:\n",
    "#         return relateDict[k][t]\n",
    "    \n",
    "#     import requests\n",
    "#     return requests.get('http://api.conceptnet.io/relatedness?node1=/c/en/%s&node2=/c/en/%s' % (k, t)).json()['value']\n",
    "\n",
    "def get_element_from_set(set_, e_):\n",
    "    for e in set_:\n",
    "        if e == e_:\n",
    "            return e\n",
    "\n",
    "\n",
    "        \n",
    "def ground_obj(tokens_, nested_):\n",
    "    \n",
    "    ## syntactically ground\n",
    "    ### E.g. man plays computer: computer -> play -> man\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    for token in tokens_copy:\n",
    "        for subj in nested_:\n",
    "            for act in nested_[subj]:\n",
    "                if token.head == act.token and token.pos_ == 'NOUN':\n",
    "                    if token.dep_ in ['dobj']:\n",
    "                        # unbound subjects are neglected, such as \"couple\"\n",
    "                        tokens_.remove(token)\n",
    "                        saved_tups.append((subj, act, token))\n",
    "    for subj, act, token in saved_tups:\n",
    "        # nested_[subj][act].add(token)\n",
    "        nested_[subj][act].add(bind_keyword(token, subj.keyword, act=act.keyword))\n",
    "        \n",
    "    # ground to current surrounding subjects based on knowledge\n",
    "    ## subject-object collocation knowledge ground, current subjects only\n",
    "    ### surrounding only. because obj in character must be syntactically grounded\n",
    "    ### not necessarily, obj can be knowledgely grounded to characters if th action is have. E.g. Alien have robot\n",
    "    \n",
    "    ## srd_subjects = [(subj.keyword, subj) for subj in nested_ if subj.keyword.t in subjects['surrounding']]\n",
    "    srd_subjects = [(subj.keyword, subj) for subj in nested_ if Node('have', attr='act') in layerbase.collocations_[subj.keyword]]\n",
    "    ## if no surrounding subjects are captured now, skip this step\n",
    "    if srd_subjects:\n",
    "        subjs, mapped = zip(*srd_subjects)\n",
    "        tokens_copy = tokens_.copy()\n",
    "        saved_tups = []\n",
    "\n",
    "        for token in tokens_copy:\n",
    "            tup = find_most_simi_subj(subjs, token)\n",
    "            if tup:\n",
    "                subj_, obj_, s_ = tup\n",
    "                # print(token, obj_, s_, subj_)\n",
    "                tokens_.remove(token)\n",
    "                ## use original text in the description as key\n",
    "                ## or use grounding keyword as key\n",
    "                # saved_tups.append((subj, token))\n",
    "                saved_tups.append((mapped[subjs.index(subj_)], obj_, token))\n",
    "        for subj, obj, token in saved_tups:\n",
    "            if MappedToken(None, Node('have', attr='act')) not in nested_[subj]:\n",
    "                nested_[subj][MappedToken(None, Node('have', attr='act'))] = set()\n",
    "            nested_[subj][MappedToken(None, Node('have', attr='act'))].add(MappedToken(token, obj))\n",
    "        \n",
    "#         for subj in nested_:\n",
    "#             if subj.lemma_ in subjects['surrounding']:\n",
    "#                 assert(subj.lemma_ in layerbase.layer_merge_.nested_entities_)\n",
    "# #                 if token.lemma_ in layerbase.layer_merge_.nested_entities_[subj.lemma_]['have']:            \n",
    "# #                     print(subj, token)\n",
    "#                 # get the most similar obj in the vocabulary under this subject\n",
    "#                 obj_ = get_simi_obj(token.lemma_,\n",
    "#                                     layerbase.layer_merge_.nested_entities_[subj.lemma_]['have'],\n",
    "#                                     thresh=0.2)\n",
    "#                 print(token.lemma_, obj_)\n",
    "#                 if obj_:\n",
    "#                     tokens_.remove(token)\n",
    "#                     saved_tups.append((subj, token))\n",
    "#                     # prevent other subjs containing the same object emerging\n",
    "#                     # first come, first serve\n",
    "#                     break \n",
    "#     for subj, token in saved_tups:\n",
    "#         nested_[subj]['have'].add(token)\n",
    "        \n",
    "#     ## other objects, just query the surroundings in knowledge and see which it belongs to\n",
    "    ## subject-object collocation knowledge ground, subjects in the layerbase\n",
    "    tokens_copy = tokens_.copy()\n",
    "    saved_tups = []\n",
    "    # subjs = [subj for subj in layerbase.collocations_ if subj.t in subjects['surrounding']]\n",
    "    subjs = [subj for subj in layerbase.collocations_ if Node('have', attr='act') in layerbase.collocations_[subj]]\n",
    "    for token in tokens_copy:\n",
    "        tup = find_most_simi_subj(subjs, token)\n",
    "        if tup:\n",
    "            subj_, obj_, s_ = tup\n",
    "            # print(token, obj_, s_, subj_)\n",
    "            tokens_.remove(token)\n",
    "            ## use original text in the description as key\n",
    "            ## or use grounding keyword as key\n",
    "            # saved_tups.append((subj, token))\n",
    "            saved_tups.append((subj_, obj_, token))\n",
    "    for subj, obj, token in saved_tups:\n",
    "        if MappedToken(None, Node('have', attr='act')) not in nested_[MappedToken(None, subj)]:\n",
    "            nested_[MappedToken(None, subj)][MappedToken(None, Node('have', attr='act'))] = set()\n",
    "        nested_[MappedToken(None, subj)][MappedToken(None, Node('have', attr='act'))].add(MappedToken(token, obj))\n",
    "        \n",
    "#         for subj in layerbase.layer_merge_.nested_entities_:\n",
    "#             if subj not in subjects['surrounding']:\n",
    "#                 continue\n",
    "#             for act in layerbase.layer_merge_.nested_entities_[subj]:\n",
    "#                 obj_ = get_simi_obj(token.lemma_,\n",
    "#                                     layerbase.layer_merge_.nested_entities_[subj][act],\n",
    "#                                     thresh=0.3)\n",
    "#                 if obj_:\n",
    "#                     assert(act == 'have'), (act, token)\n",
    "#                     # assert(subj not in [s.lemma_ for s in nested_])\n",
    "#                     ## Attention! here the key type is string now\n",
    "#                     print(token, subj, obj_)\n",
    "#                     tokens_.remove(token)\n",
    "#                     nested_[subj][act].add(token)\n",
    "#                     break\n",
    "                    \n",
    "\n",
    "# def comb_obj(tokens_, nested_):\n",
    "    \n",
    "#     ## layers built from clusters\n",
    "#     tokens_copy = tokens_.copy()\n",
    "#     for token in tokens_copy:\n",
    "\n",
    "def conj_copy(nested_):\n",
    "    # subjs = []\n",
    "    saved_tups = []\n",
    "    for subj in nested_:\n",
    "        if subj.token:\n",
    "            if subj.token.dep_ == 'conj':\n",
    "                assert(not nested_[subj])\n",
    "                for subj_ in nested:\n",
    "                    if subj.token.head == subj_.token and nested_[subj_]:\n",
    "                        saved_tups.append((subj, subj_))\n",
    "                        break\n",
    "                        \n",
    "    for subj, subj_ in saved_tups:\n",
    "        nested_[subj] = nested_[subj_]\n",
    "    # if conj, copy syntactic children    \n",
    "    \n",
    "def slice_into_layers(nested_):\n",
    "    \n",
    "    # solidify first\n",
    "    nested_ = ddict2dict(nested_)\n",
    "    \n",
    "    subjs = list(nested_)\n",
    "    group_list = [[subjs[0]]]\n",
    "    subjs = subjs[1:]\n",
    "    while subjs:\n",
    "        for subj in subjs:\n",
    "            # for group in group_list:\n",
    "            matched = [g for g in group_list if subj.token and g[0].token and g[0].token.sent.root==subj.token.sent.root and subj.keyword.t in subjects['character']]\n",
    "#                 if subj.token.sent.root == group[0].token.sent.root:\n",
    "#                     group.append(subj)\n",
    "#                     break\n",
    "            if matched:\n",
    "                assert(len(matched) == 1)\n",
    "                matched[0].append(subj)\n",
    "                subjs.remove(subj)\n",
    "            else:\n",
    "                group_list.append([subj])\n",
    "                subjs.remove(subj)\n",
    "                \n",
    "    layers = []\n",
    "    for group in group_list:\n",
    "        layer = {}\n",
    "        for subj in group:\n",
    "            layer[subj] = nested_[subj]\n",
    "        layers.append(layer)\n",
    "    return tuple(layers)\n",
    "        \n",
    "\n",
    "####\n",
    "# technically all entities can be grounded based on similarity, no need to exactly same\n",
    "####\n",
    "\n",
    "def ground(filename):\n",
    "    with open(filename) as f:\n",
    "        text = f.read()\n",
    "    print(text)\n",
    "    doc = nlp(u'%s' % text.strip('\\n'))\n",
    "    \n",
    "    nested = defaultdict(lambda: defaultdict(set))\n",
    "    tokens = get_tokens(doc)\n",
    "    print(tokens)\n",
    "    # map_dict = defaultdict(str)\n",
    "    ground_subj(tokens, nested)\n",
    "    print('-> ground subjects:', ddict2dict(nested))\n",
    "    # print(tokens)\n",
    "    ground_act(tokens, nested)\n",
    "    print('-> ground actions:', ddict2dict(nested))\n",
    "    # print(tokens)\n",
    "    # no verbs will be left\n",
    "    import warnings\n",
    "    \n",
    "    if not all([t.pos_ != 'VERB' for t in tokens]):\n",
    "        warnings.warn('Verbs not exhausted!')\n",
    "        print([t for t in tokens if t.pos_ == 'VERB'])\n",
    "\n",
    "    # print(tokens)\n",
    "    ground_obj(tokens, nested)\n",
    "    print('-> left ungrounded tokens:', tokens)\n",
    "    conj_copy(nested)\n",
    "    layers = slice_into_layers(nested)\n",
    "    return layers\n",
    "# nested\n",
    "# # print(map_dict)\n",
    "# nested\n",
    "\n",
    "## why don't we use scapy token as dict keys, such that no overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A woman is running over hurdles.\n",
      "\n",
      "[woman, running, hurdles]\n",
      "-> ground subjects: {woman->woman: {}}\n",
      "-> ground actions: {woman->woman: {run->run: set()}}\n",
      "[(woman(subj), head_shot(obj), 0.253)]\n",
      "-> left ungrounded tokens: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({woman->woman: {run->run: set(), ->have: {hurdle->head_shot}}},)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'track_and_field'\n",
    "ground('text/%s.txt' % name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#background, #woman(exercise,run_over[hurdle]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools.containers import Picture\n",
    "Picture('images/%s.svg' % name).layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(layerbase.collocations_[Node('wild', attr='subj')][Node('have', attr='act')])[-3].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{man(subj): {sit(act): set(), hold(act): set()},\n",
       " woman(subj): {lie(act): set(), talk(act): set()}}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layerbase.pic_vocab_)[11].layers_[-1].nested_entities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  track and field: A woman is running over hurdles.\n",
    "#  hurdle isn't captured by woman. Extend the object capture to any subject\n",
    "\n",
    "#  A man talks with his friend under the tree.\n",
    "#   \"friend\" is not bound but is important. similar cases include couple.\n",
    "\n",
    "# For_sale.txt [resolved]\n",
    "#    syntactically bound surrounding subjects have no action\n",
    "#    thus add default action \n",
    "\n",
    "# Follow_me_drone\n",
    "#    \"woman watch drone\" rather than \"woman watch\" and \"other drone\"\n",
    "#    this is a syntactical grounding case, thus nothing can be done\n",
    "#    may require the difference between strong interaction and weak interaction. E.g. play and watch\n",
    "#    need special care when compare the similarity. keywords in a bag may be suitable in this case. Previously we propose layerwise retrieval, now picture level? The slicing is already done anyway. But then we cannot ensure, e.g. two men in a group is correctly bound, duplicate subjects information is lost after bagging.\n",
    "\n",
    "\n",
    "# text/Focus.txt [resolved]\n",
    "#    tree -> home rather than tree -> wild\n",
    "#    add a new ordering key in find_most_simi_subjs, the frequency of this object under this subject \n",
    "#    similarly, add the frequency key in get_simi_key\n",
    "#        in fact, this is the frequency prior\n",
    "\n",
    "# text/Flowers.txt\n",
    "#    grass -> leaf but should grass -> plant\n",
    "\n",
    "# 'text/Firmware.txt [resolved]\n",
    "#    robot can not be grounded to alien, because it's alien have robot, which is an oject\n",
    "#    -> ground to any subjects which have a \"have\" action, not just surrounding subjects\n",
    "\n",
    "# wild should be separate from man and boy? [resolved]\n",
    "#    generally if both are characters then in a layer group? surrounding cannot be in a group technically\n",
    "\n",
    "# chair should be with sit on?\n",
    "# but sofa should not be with lie on\n",
    "# what to do\n",
    "\n",
    "# in alien, lie is not bound to any keyword\n",
    "# because lie isn't in the collocations of alien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'A man and a boy are playing computer and sitting on the ground in the wild. A woman is waving under the tree. A girl is sitting on the chair. A man is lying on the sofa playing games. There are some trees and plants and flowers in the park. An alien is lying crying.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seemly that we don't need a second step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.instance import Node\n",
    "from tools.containers import Picture, Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function tools.image_process.LayerName.collapse_subj.<locals>.<lambda>()>,\n",
       "            {'background': {},\n",
       "             'accessory': {'have': {'leaf', 'plant'}},\n",
       "             'other': {'have': {'bulletin',\n",
       "               'camera',\n",
       "               'circle',\n",
       "               'cloud',\n",
       "               'drone',\n",
       "               'gamepad',\n",
       "               'plant',\n",
       "               'square',\n",
       "               'sun',\n",
       "               'tree',\n",
       "               'triangle',\n",
       "               'webpage',\n",
       "               'website',\n",
       "               'windmill'}},\n",
       "             'man': {'point_to': set(),\n",
       "              'stand': set(),\n",
       "              'sit': set(),\n",
       "              'hold': {'cube', 'fishing_rod', 'paper', 'phone'},\n",
       "              'fishing': set(),\n",
       "              'drink': {'beer'},\n",
       "              'raise': {'arm'},\n",
       "              'eat': {'chips'},\n",
       "              'touch': set(),\n",
       "              'play': {'computer'},\n",
       "              'think': set(),\n",
       "              'walk': set(),\n",
       "              'talk': set(),\n",
       "              'lie_on': {'stone'},\n",
       "              'collect': {'data'},\n",
       "              'step_on': {'stone'},\n",
       "              'exercise': set(),\n",
       "              'work': set(),\n",
       "              'watch': {'movie'},\n",
       "              'lie': set()},\n",
       "             'office': {'have': {'phone'}},\n",
       "             'wild': {'have': {'balloon',\n",
       "               'beach',\n",
       "               'boat',\n",
       "               'cloud',\n",
       "               'firework',\n",
       "               'fish',\n",
       "               'fish_tank',\n",
       "               'guideboard',\n",
       "               'hill',\n",
       "               'lake',\n",
       "               'leaf',\n",
       "               'plant',\n",
       "               'rock',\n",
       "               'sign',\n",
       "               'sun',\n",
       "               'sunflower',\n",
       "               'tree',\n",
       "               'water',\n",
       "               'wind'}},\n",
       "             'woman': {'sport': set(),\n",
       "              'run': set(),\n",
       "              'watch': set(),\n",
       "              'have': {'head_shot'},\n",
       "              'stand': set(),\n",
       "              'hold': {'flag', 'flower', 'pencil', 'phone'},\n",
       "              'play': {'computer', 'phone'},\n",
       "              'sit': set(),\n",
       "              'work': set(),\n",
       "              'write': set(),\n",
       "              'bend': set(),\n",
       "              'present': set(),\n",
       "              'back_on': set(),\n",
       "              'touch': set(),\n",
       "              'walk': set(),\n",
       "              'talk': set(),\n",
       "              'point_to': set(),\n",
       "              'show': set(),\n",
       "              'take': {'pictures'},\n",
       "              'lie': set(),\n",
       "              'sit_on': {'chair'},\n",
       "              'run_over': {'hurdle'},\n",
       "              'exercise': set()},\n",
       "             'chart': {'have': {'arrow', 'folder', 'list', 'money', 'paper'}},\n",
       "             'home': {'have': {'apple',\n",
       "               'bookcase',\n",
       "               'chips',\n",
       "               'house',\n",
       "               'sofa',\n",
       "               'sun',\n",
       "               'toy',\n",
       "               'tree',\n",
       "               'wall_clock'}},\n",
       "             'alien': {'stand': set(), 'have': {'robot', 'snowman'}},\n",
       "             'park': {'have': {'plant'}}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layerbase.layer_merge_.nested_entities_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{man1(subj): {sit(act): set(),\n",
       "  drink(act): {beer(obj)},\n",
       "  raise(act): {arm(obj)}},\n",
       " man2(subj): {sit(act): set(), raise(act): {arm(obj)}, eat(act): {chips(obj)}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### group layer exclusion check\n",
    "sorted(list(layerbase.layer_vocab_))[12].nested_entities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subj': {man(subj): 6}, 'act': {sit(act): 6, drink(act): 3, raise(act): 6, eat(act): 3}, 'obj': {beer(obj): 3, arm(obj): 6, chips(obj): 3}}\n",
      "{'subj': {man(subj): 6}, 'act': {sit(act): 6, drink(act): 3, raise(act): 6, eat(act): 3}, 'obj': {beer(obj): 3, arm(obj): 6, chips(obj): 3}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x96 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### absorption check\n",
    "from tools.containers import LayerName\n",
    "from tools.image_process import getLayerNames\n",
    "layer = LayerName(getLayerNames('images/Game_day.svg')[-1])#\n",
    "layer_c = LayerName()\n",
    "# layer_c.triples_\n",
    "# print(layer_c.entities_['subj'])\n",
    "layer_c.absorb(layer)\n",
    "layer_c.absorb(layer)\n",
    "layer_c.absorb(layer)\n",
    "layer_c.triples_\n",
    "# print(layer)\n",
    "# print(layer.entities_)\n",
    "# layer.collapse_subj()\n",
    "# print(layer.entities_)\n",
    "# layer.print_()\n",
    "# layer_c.plot()\n",
    "print(layer_c.entities_) # plot()\n",
    "print(layer_c._get_entities())\n",
    "layer_c.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2scene",
   "language": "python",
   "name": "text2scene"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
